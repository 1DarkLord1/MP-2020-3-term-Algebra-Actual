\documentclass[10pt,a4paper,oneside]{book}
\usepackage[a4paper,includeheadfoot,top=10mm,bottom=10mm,left=10mm,right=10mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{cmap} % Поддержка поиска русских слов в PDF (pdflatex)
\usepackage{comment}
%\usepackage{pdfsync} % синхронизация

\usepackage{amsmath,amsthm,amssymb,amscd,array}
\usepackage{latexsym}
\usepackage{stmaryrd} % Для знака нормальной подгруппы
\usepackage{misccorr} % российская полиграфия
\usepackage{indentfirst}% Красная строка в первом абзаце
\usepackage{ccaption} % Заголовки таблиц и рисунков
\usepackage{fancyhdr} % колонтитулы
\usepackage{hyperref} % гиперссылки

\usepackage{rotating} % Поворот текста
\usepackage{graphicx} % Вставка изображений
\usepackage{xcolor}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}
\graphicspath{ {./} } % относительно main.tex

\usepackage{arydshln} % штрихованные линии в массивах
\usepackage{mathtools} % выравнивание в матрицах
\usepackage{multirow} % слияние в столбце
\usepackage{multicol} % нумерация в нескольких колонках


\newtheorem{uprz}{\color{violet!100!black} Упражнение}
\newtheorem{predl}{\color{blue!50!black} Предложение}
\newtheorem{komment}{\color{green!50!blue} Комментарий}
\newtheorem{conj}{Гипотеза}
\newtheorem{notation}{\color{yellow!30!red} Обозначение}


\theoremstyle{definition}
\newtheorem{kit}{Кит}
\newtheorem*{rem}{\color{green!50!blue}Замечание}
\newtheorem{zad}{\color{violet!100!black}Задача}
\newtheorem*{defn}{\color{yellow!30!red} Определение}
\newtheorem*{fact}{Факт}
\newtheorem{thm}{\color{red!40!black}Теорема}
\newtheorem*{thmm}{\color{red!40!black} Теорема}
\newtheorem{lem}{\color{green!50!black}Лемма}
\newtheorem{cor}{\color{green!45!black}Следствие}
\newtheorem{utvr}{\color{blue!50!black}Утверждение}


\newcommand\tikznode[3][]%
   {\tikz[remember picture,baseline=(#2.base)]
      \node[minimum size=0pt,outer sep=0pt,#1](#2){#3};%
   }
\tikzset{>=stealth}


\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={red!80!black}
}
% цвета для ссылок


\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother


\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\proofname}{Доказательство}
\renewcommand{\mod}{\,\operatorname{mod}\,}
\renewcommand{\Re}{\operatorname{Re}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ovl}{\overline}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\K}{\operatorname{K_0}}
\newcommand{\witt}{\operatorname{W}}
\newcommand{\gw}{\operatorname{GW}}
\newcommand{\coh}{\operatorname{H}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\cl}{\operatorname{Cl}}
\newcommand{\Vol}{\operatorname{Vol}}
\newcommand\tgg{\mathop{\rm tg}\nolimits}
\newcommand\ccup{\mathop{\cup}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\chr}{\operatorname{char}}
\newcommand{\rk}{\operatorname{rk}}
\DeclareMathOperator{\Coker}{Coker}
\DeclareMathOperator{\Ker}{Ker}
\newcommand{\im}{\operatorname{Im}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\re}{\operatorname{Re}}
\newcommand{\tr}{\operatorname{Tr}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\orb}{\operatorname{\mathcal O}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\Out}{\operatorname{Out}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\SO}{\operatorname{SO}}
\renewcommand{\O}{\operatorname{O}}
\renewcommand{\U}{\operatorname{U}}
\newcommand{\Sym}{\operatorname{Sym}}
\newcommand{\Adj}{\operatorname{Adj}}
\newcommand{\Disc}{\operatorname{Disc}}
\newcommand{\cnt}{\operatorname{cont}}
\newcommand{\Frob}{\operatorname{Frob}}
\newcommand{\Iso}{\operatorname{Iso}}
\newcommand{\Isom}{\operatorname{Isom}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\di}{\mathop{\,\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\ndi}{\mathop{\not\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\nequiv}{\not \equiv}
\newcommand{\Nod}{\operatorname{\text{НОД}}}
\newcommand{\Nok}{\operatorname{\text{НОК}}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\Aff}{\operatorname{Aff}}
\newcommand{\AGL}{\operatorname{AGL}}
\newcommand{\PSL}{\operatorname{PSL}}
\newcommand{\Volume}{\operatorname{Volume}}

\def\llq{\textquotedblleft} 
\def\rrq{\textquotedblright} 
\def\exm{\noindent {\bf Примеры:}}


\def\Cb{\ovl{C}}
\def\ffi{\varphi}
\def\pa{\partial}
\def\V{\bf V}
\def\La{\Lambda}
\def\eps{\varepsilon}
\def\del{\delta}
\def\Del{\Delta}
\def\A{\EuScript{A}}
\def\lan{\left\langle }
\def\ran{\right\rangle}
\def\bar{\begin{array}}
\def\ear{\end{array}}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\thrm{\begin{thm}}
\def\ethrm{\end{thm}}
\def\dfn{\begin{defn}}
\def\edfn{\end{defn}}
\def\lm{\begin{lem}}
\def\elm{\end{lem}}
\def\zd{\begin{zad}}
\def\ezd{\end{zad}}
\def\prdl{\begin{predl}}
\def\eprdl{\end{predl}}
\def\crl{\begin{cor}}
\def\ecrl{\end{cor}}
\def\rm{\begin{rem}}
\def\erm{\end{rem}}
\def\fct{\begin{fact}}
\def\efct{\end{fact}}
\def\enm{\begin{enumerate}}
\def\eenm{\end{enumerate}}
\def\pmat{\begin{pmatrix}}
\def\epmat{\end{pmatrix}}
\def\utv{\begin{utvr}}
\def\eutv{\end{utvr}}
\def\upr{\begin{uprz}}
\def\eupr{\end{uprz}}
\def\nrml{\trianglelefteqslant}

\frenchspacing
\righthyphenmin=2
%\usepackage{floatflt}
\captiondelim{. }

\title{Современное программирование \\ 
Конспект по алгебре, 3 семестр}
\date{}


\begin{document}

\tableofcontents

\chapter{Линейная алгебра}

\section{Кватернионы}


Наша цель сейчас рассказать про геометрию трехмерного пространства используя при этом определённые алгебраические конструкции. А именно, ещё в XIX веке Уильям Роуэн Гамильтон стал искать аналогичную комплексным числам алгебраическую систему на трёхмерном пространстве.  
Однако, подходящий аналог удалось найти только в четырёхмерной ситуации.


Рассмотрим вещественное подпространство в алгебре матриц $M_2(\mb C)$ вида
$$\mb H = \left\{\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha} \epmat \right\}.$$
Базис этого пространства, как вещественного векторного пространства, состоит из матриц 
$$ 1=\pmat 1 & 0 \\ 0& 1 \epmat, i= \pmat i & 0 \\ 0& -i \epmat, j=\pmat 0& 1 \\ -1 & 0 \epmat, k=\pmat 0 & i \\ i & 0\epmat. $$ 
Покажем, что это вещественная подалгебра в $M_2(\mb C)$ и следовательно ассоциативное кольцо. 
Для этого достаточно показать, что произведение базисных снова лежит в $\mb H$. Имеем $$i^2=j^2=k^2=-1 \text{ и } ij=k=-ji,$$ откуда $$ik= iij=-j=jii=-ki \text{ и } jk=-jji=i=-kj.$$ Таким образом $\mb H$ образует ассоциативную алгебру размерности 4 над $\mb R$.
 
\dfn[Алгебра кватернионов] $\mb H$ называется алгеброй кватернионов. 
\edfn
Мы больше не будем думать (кроме одного нюанса) про кватернионы как про матрицы, а будем записывать их через $i,j,k$. Именно так обычно кватернионы и вводят -- как формальный суммы $a+bi+cj+dk$, для произведения которых выполнены тождества $i^2=j^2=k^2=-1$ и $ij=-ji=k$. Посмотрев на кватернионы как на матрицы мы сэкономили на доказательстве ассоциативности умножения.

\zd Алгебра кватернионов не снабжается структурой $\mb C$-алгебры.
\ezd






\dfn[Векторная и скалярная часть, сопряжённый кватернион] Пусть $x= a+bi+cj+dk$ кватернион. Определим вещественную или скалярную часть $\Re x=a$ и векторную часть $v= bi+cj+dk$ кватерниона. Сопряжённым кватернионом называется $\ovl{x}= a-bi-cj-dk= \Re x - \Im x =a-v$. 
\edfn



Посмотрим, как перемножаются кватернионы. Если оба кватерниона  $x$, $y$ разделить на скалярную и векторную части $x=a+v$, $y=b+u$ то $xy=ab+au+bv+ vu$. Нам осталось разобраться с умножением векторных частей. 

Рассмотрим произведение двух чисто мнимых кватернионов $uv=-\lan u,v\ran+[u,v]$. Его вещественная часть совпадает с минус скалярным произведением векторов. Про мнимую часть мы поговорим отдельно.

\dfn[Векторное произведение] Пусть $u,v \in \mb R^3$ два вектора. Тогда их векторным произведением называется вектор $[u,v]$.
\edfn

Если расписать в координатах $u=x_1i+x_2j+x_3k$ и  $v=y_1i+y_2j+y_3k$, то векторное произведение задаётся формулой

$$[u,v]= (x_2y_3-x_3y_2)i + (x_3y_1-x_1y_3)j + (x_1y_2- x_2y_1)k= \begin{vmatrix} i& j&k \\ x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \end{vmatrix} $$

\rm Операция $(u,v) \to [u,v]$ является билинейной и антисимметричной, то есть $[u,u]=0$ и, следовательно, $[u,v]=-[v,u]$.
\erm

Последнее замечание позволяет нам легко вычислить $x \ovl{x}= a^2+ \lan v,v \ran$. Это приводит нас к определению:

\dfn[Норма кватерниона] Определим норму кватерниона как $$\|x\|=\sqrt{x\ovl{x}}=\sqrt{ a^2+b^2+c^2+d^2}=\sqrt{\ovl{x}x}.$$
\edfn 


Норма кватерниона, как и модуль комплексного числа всегда положительны для ненулевых элементов. Это позволяет заметить, что

\dfn[Обратный кватернион] Если $0\neq x \in \mb H$, то $x^{-1}=\frac{\ovl{x}}{\|x\|^2}$. 
\edfn

Таким образом мы получили первый (и для нас единственный) пример некоммутативного кольца с делением. Такие кольца называются телами. Напоминаю, что алгебра для нас ассоциативна и с единицей. Неассоциативные алгебры представляют интерес. Например, можно взять $\mb R^3$, где в качестве умножения взято векторное произведение. Это пример неассоциативной алгебры или, точнее, алгебры Ли. В этом курсе мы не  обсуждаем неассоциативные алгебры в связи с тем, что им обычно находится применение либо внутри физических дисциплин, либо внутри самой математики и редко где ещё. 

Какие ещё свойства есть у отображения нормы? Если следовать параллели с комплексными числами, то стоит посмотреть, что происходит с нормой произведения. Для того, чтобы не обременяться вычислениями сделаем небольшой трюк и на секунду вспомним матричное представление кватернионов. Заметим, что на матричном языке, норма -- это $\|x\|=\sqrt{\det x}$, откуда получаем

\lm[Норма мультипликативна] $\|xy\|=\|x\|\|y\|$. В частности, $\|x^{-1}\|=\|x\|^{-1}$.
\proof Вспомним в последний раз, что кватернионы задаются матрицами из $M_2(\mb C)$. Пусть 
$$x=\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat.$$
Тогда $$\|x\|^2=|\alpha|^2+|\beta|^2=\det \pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat,$$
а определитель мультипликативен.
\endproof
\elm



Продолжим. Используя мультипликативность нормы легко доказать 
\lm Отображение $x \to \ovl{x}$ является антиизоморфизмом алгебр, то есть $\ovl{ab}=\ovl{b}\ovl{a}$.
\proof Линейной ясна. Пусть $x,y \neq 0$. Тогда $$\frac{\ovl{y}\,\ovl{x}}{\|y\|^2\|x\|^2}=y^{-1}x^{-1}=(xy)^{-1}=\frac{\ovl{xy}}{\|xy\|^2}.$$
\elm

На самом деле и здесь можно было воспользоваться матричным представлением. А именно, можно заметить, что операция сопряжения кватернионов совпадает на этом языке с транспонированием и сопряжением соответствующей комплексной матрицы. Вернёмся теперь к векторному произведению.





\lm[Свойства векторного произведения] Верны следующие свойства\\
1) Для любых $u,v \in \mb R^3$ верно $u\bot [u,v]$. Точнее $$u[u,v]= -\|u\|^2v+ \lan u,v\ran u$$
2) $\| [u,v]\|= \|u\|\|v\| \cdot |\sin \ffi |$, где $\ffi$ --  это угол между $u$ и $v$.
\elm
\proof Для того, чтобы посчитать скалярное произведение $\lan u, [u,v]\ran$ необходимо посчитать скалярную часть $u[u,v]$. 
$$u[u,v]= u (uv+ \lan u,v \ran)= u^2 v+ \lan u,v \ran u= -\|u\|^2 v+ \lan u,v \ran u$$
Последнее выражение, очевидно, чисто векторное.
Теперь
\begin{align*}
&\|[u,v]\|^2= -[u,v][u,v]= (uv + \lan u,v\ran)(vu + \lan u,v\ran)=\\
&=\|u\|^2\|v\|^2+ \lan u,v\ran^2 + \lan u,v\ran (uv+vu)=\|u\|^2\|v\|^2 - \lan u,v\ran^2= \|u\|^2\|v\|^2(1-\cos^2 \ffi)
\end{align*}
\endproof

\dfn Обозначим за $\mb H_{1}$ группу кватернионов, по норме равных единице.
\edfn

\thrm Отображение $\mb H_{1}\to \GL_3(\mb R)$ заданное по правилу $x\to (y \to xyx^{-1})$ корректно определено и даёт сюръективный  гомоморфизм из группы кватернионов единичной нормы в $\SO_3(\mb R)$. Ядро этого гомоморфизма состоит из $\{\pm 1\}$. Точнее, если единичный кватернион $x$  представим в виде $x=a+bv$, то соответствующее ему вращение есть вращение относительно  оси $\lan v \ran$ на угол $2\ffi$, где $\cos \ffi= a$, $\sin \ffi= b$ или тождественное преобразование в случае $v=\pm 1$.
\ethrm
\proof Рассмотрим преобразование $L_x \colon \mb H \to \mb H$ вида $y \to xyx^{-1}$. Прежде всего покажем, что мы получили ортогональное преобразование $\mb R^4$. Имеем
 $$\|xvx^{-1}\|=\|x\| \|v\| \|x^{-1}\| = \|v\|.$$
Теперь заметим, что преобразование $L_x$ сохраняет на месте вектор 1 и, следовательно, его ортогональное дополнение, то есть $\mb R^3$. Таким образом $L_x$ ограничивается на $\mb R^3$. Далее, очевидно, $L_xL_y= L_{xy}$. Осталось посчитать ядро гомоморфизма и явный вид отображения $L_x$. Заметим, что если $x=a+bv$, то $L_x$ оставляет $v$ на месте. Действительно, при $b\neq 0$ 
$$xbvx^{-1}=x(x-a)x^{-1}= x-a=bv.$$
Вычислим угол поворота. Для этого рассмотрим нормированный вектор  $u\bot v$ и $[u,v]$, которые образуют ортонормированный базис дополнения и посчитаем $xux^{-1}$ и $x[u,v]x^{-1}$. Из условия ортогональности следует, что $uv=[u,v]=-[v,u]=-vu$ и значит $v[u,v]=-[u,v]v=-uv^2=u$. Теперь
\begin{align*}
xux^{-1}&=(a+bv)u(a-bv)= (a+bv)(au-buv)=\\
&=a^2u -ab[u,v]+ab[v,u]- b^2vuv=a^2u-2ab[u,v]-b^2\|v\|^2u=\\ &=(a^2-b^2)u-2ab[u,v]=\cos2\ffi u+ \sin 2\ffi [u,v]
\\
\\
x[u,v]x^{-1}&=(a+bv)[u,v](a-bv)= (a[u,v]+bu)(a-bv)=\\
&=a^2[u,v]+abu-ab[u,v]v-b^2uv=\\
&=(a^2-b^2)[u,v]+2abu=\cos 2\ffi[u,v]+\sin 2\ffi u
\end{align*}

Осталось показать, что только $x=\pm 1$ лежит в ядре этого отображения. Это возможно только тогда, когда $2\ffi \equiv 0 \mod 2\pi$. Значит $\ffi=2 \pi$ или $\ffi= \pi$. Первое соответствует $x=1$. Втораое --  $a=\cos \ffi = -1$, а $b=\sin \ffi = 0$, то есть $x=-1$. 
\endproof


\zd
Покажите, что отображение $(x,y) \to (z \to xzy^{-1})$ задаёт сюръективный гомоморфизм из декартового квадрата группы единичных кватернионов в группу $\SO_4(\mb R)$ с ядром $\{(1,1),(-1,-1)\}$.
\ezd

Обсудим теперь для чего могут понадобится кватернионы. Каждый кватернион, как мы установили кодирует вращение трёхмерного пространства или, что тоже самое -- новую декартову систему координат в $\mb R^3$ с центром в нуле. Такой тип данных встречается в компьютерной графике, если вы хотите зафиксировать ракурс, в котором вы смотрите на 3d-сцену или положение какого-то конкретного объекта в такой сцене.

В такой задаче кватернионы сложно превзойти. Действительно, задание сцены при помощи кватернионов очень экономно -- 4 коэффициента с одним соотношением на них (3 коэффициента, если очень нужно) против 9 коэффициентов у ортогональной матрицы.

А что с эффективностью операций? Тут вопрос состоит в том, про какие операции идёт речь. Если вы хотите сказать, что тот или иной вектор, который был повёрнут на кватернион $q=a+v$ надо повернуть ещё на кватернион $p=b+u$, то вам надо всего лишь вычислить $pq=ab+au+bv+uv$. Такое произведение считается за 16 умножений и 12 сложений. Если брать произведение матриц $3\times 3$, то там получается 27 умножений и 18 сложений (можно обойтись и 23 умножениями, но сильно увеличив число сложений).

Можно конечно использовать углы Эйлера, но тогда придётся использовать для вычислений косинусы и синусы этих углов, которые сами даются не бесплатно.

Если вам даны два ракурса в виде кватернионов, то легко понять, на какой кватернион надо домножить, чтобы из первого получить второй.

Но что если вы хотите просто повернуть при помощи кватерниона какой-то вектор из $\mb R^3$. Тут ситуация несколько хуже. Для этого вам необходимо посчитать $qxq^{-1}$. Предположив, что $q$ нормирован можно заменить обращение на сопряжение. Тем не менее такой подход довольно дорог -- 32 умножения и 24 сложения. Конечно, такой способ не оптимален. Например, не обязательно считать скалярную часть -- она должна стать нулевой. На самом деле, если $q=a+v$, то 
$$qxq^{-1}=x+ 2[v, [v,x]+ ax]$$
что даёт 15 умножений и 15 сложений (если считать умножение на 2 сложением). Это конечно отличается от матриц, где необходимо 9 умножений и 6 сложений.

Впрочем, отличается не сильно. Кроме того у кватернионного представления есть плюс произведение нескольких кватернионов -- это кватернион несмотря на ошибки округления. Что не так для ортогональных матриц.

Наконец, представим себе задачу, что нам нужно плавно перейти от ракурса $q$ к ракурсу $p$. Желательно с "равномерной" скоростью. На языке кватернионов это становится понятно. Для этого заметим, что единичные кватернионы -- это всего лишь точки на трёхмерной сфере. Несложно понять, что их соединяет часть дуги сферы, точки на которой заданы как 
$$ \frac{\sin(t\theta)p + \sin ((1-t)\theta) q}{\sin \theta},$$
где $\theta$ -- угол между $p$ и $q$ (острый, не забываем, что представление кватернионами немного не однозначно).





\section{Задачи на максимизацию}

Теперь обратимся к вопросам, связанным с вещественными самосопряжёнными операторами.

Рассмотрим один из вопросов, связанных с такой конструкцией, а именно, рассмотрим задачу о нахождении нормы линейного оператора $L \colon U \to V$ между двумя евклидовыми пространствами. Для того, чтобы найти  норму необходимо найти $$\max_{x\neq 0}\sqrt{\frac{\lan Lx,Lx\ran}{\|x\|^2}}=\sqrt{\max_{x\neq 0}\frac{\lan L^*Lx,x\ran}{\|x\|^2}}=\sqrt{\max_{\|x\|=1} \lan L^*Lx,x\ran}.$$
Таким образом, нахождение нормы оператора свелось к задаче максимизации квадратичной формы на единичной сфере. Заметим, что максимум действительно достигается благодаря компактности сферы.

Оказывается, что довольно легко найти максимум или минимум квадратичной формы на сфере.



\thrm Пусть $V$ -- евклидово пространство, $A$ -- самосопряжённый оператор на $V$, а $q(x)=\lan x,Ax\ran$ -- соответствующая квадратичная форма. Тогда 
$$\max_{ x\in V } \frac{q(x)}{||x||^2}=\max_{\substack{ x\in V \\ ||x||=1}} q(x)=\lambda_1,$$
 где $\lambda_1$ - наибольшее собственное число оператора $A$ и достигается на собственном векторе $v_1$, соответствующему $\lambda_1$. Аналогично минимум равен минимальному собственному числу $A$. 
\proof
Пусть $v=\sum c_i e_i$, причём $1=||v||^2=\sum c_i^2$. Тогда $\lan Av,v\ran = \sum c^2_i \lambda_i $, что меньше $\lan A e_1,e_1\ran= \lambda_1= \sum \lambda_1 c_i^2$.
\endproof
\ethrm

Эта теорема, кроме, собственно, решения задачи, даёт геометрическую характеризацию первого собственного числа. Вопрос: можно ли аналогично охарактеризовать другие собственные числа? Ответ получается не таким простым, но, тем не менее, полезным.

\thrm[Куранта-Фишера] Пусть $q(x)=\lan x, Ax\ran$. Тогда $k$-ое по убыванию собственное число $\lambda_k$ для $A$ есть 
$$ \lambda_k=\max_{\dim L=k} \min_{\substack{ x\in L \\ ||x||=1}} q(x) = \min_{\dim L=n-k+1} \max_{\substack{ x\in L \\ ||x||=1}} q(x).$$
Причем максимум достигается на инвариантном подпространстве, содержащем собственные вектора для $\lambda_1,\dots,\lambda_k$.
\ethrm
\proof Пусть $U$ --- подпространство на котором достигается максимум, причём допустим, что максимум больше $\lambda_k$. Тогда рассмотрим подпространство $W=\lan v_k,\dots,v_n\ran$, где $v_i$ --- собственный вектор соответствующий $i$-ому по убыванию собственному числу. Заметим, что $U\cap W=\{0\}$, так как на $W$ форма принимает значения меньше или равные $\lambda_k$, а на $U$ -- строго большие. Однако $\dim W=n-k+1$. Приходим к противоречию с подсчётом размерности пересечения. 
\endproof




\crl Пусть $U$ некоторое подпространство, а $q(x)=x^{\top} Ax$. Пусть собственные числа $A$ --- это $\lambda_i$, а собственные числа оператора, соответствующего $q(x)|_U$ -- это $\mu_i$ упорядоченные по убыванию. Тогда 
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$  
\proof Заметим, что
 $$\mu_i=\max_{\substack{L\leq U\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x),$$
что очевидно меньше, чем 
$$\max_{\substack{L\leq V\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x)=\lambda_i.$$ Неравенство в другую сторону получается из второго описания в теореме Куранта-Фишера.
\endproof
\ecrl


Введём не совсем стандартное определение:
\dfn
Пусть $q$ -- квадратичная форма на евклидовом пространстве. Рассмотрим ортонормированный базис $u_i$ пространства $V$. Тогда положим 
$$\Tr q= \sum q(u_i).$$ Если в базисе $u_i$ форме $q(x)=x^{\top} Ax $ соответствует симметричная матрица $A$, то $\Tr q(x)=\Tr(A)$.
\edfn

\rm Определение не зависит от выбора ортонормированного базиса. Действительно, если замена координат ортогональна, то матрица $q$ в новой системе координат имеет вид $C^{\top}AC=C^{-1}AC$. Осталось заметить, что след последней матрицы очевидно равен следу $A$.
\erm

\rm Если форме $q$ соответствует самосопряжённый оператор $A$, то $\Tr q=\sum \lambda_i$, где $\lambda_i$ -- собственные числа $A$.
\erm




\crl Пусть $q(x)=x^{\top} Ax$, $U$ --- некоторое подпространство, $\dim U=k$. Пусть собственные числа $A$ --- это $\lambda_i$. Тогда $$\Tr q|_U\leq \sum_{i=1}^k \lambda_i= \Tr q|_{V_k},$$
где $V_k$ подпространство натянутое на первые $k$ собственных векторов $q$.
\proof Нам известны неравенства на собственные числа $\mu_i$, ограничения $q|_U$. А именно, $\mu_i\leq \lambda_i$. Но тогда и $$\sum_{i=1}^k \mu_i \leq \sum_{i=1}^k \lambda_i.$$
\endproof
\ecrl 






\section{Метод главных компонент}

Рассмотрим следующую задачу: имеется массив данных --- набор векторов $x_1,\dots,x_s \in \mb R^n$. Подразумевается, что точек очень много. Предположим, что при идеальных измерениях между координатами этих векторов есть линейные зависимости. Все отклонения от этой погрешности вызваны небольшими погрешностями. Задача состоит в том, чтобы восстановить линейную зависимость. 


Переформулируем задачу геометрически. Пусть наши вектора удовлетворяют линейным условиям $Ax_i=b$ для некоторой матрицы $A$ ранга $n-k$. Это значит, что они лежат в аффинном подпространстве $\Ker A + a_0$ размерности $k$. Если же нам даны вектора с погрешностями, то задачу таким образом можно поставить в виде: найти аффинное подпространство размерности $k$ наиболее близкое к данным точкам $x_1,\dots,x_s$. Понятие <<наиболее близкое>> требует конкретизации. Вообще говоря, тут есть выбор. Мы будем считать, что подходящее пространство $W=U+a_0$ должно давать минимум следующего выражения:
$$\sqrt{\sum_{i=1}^s \rho(x_i,W)^2} \to \min.$$
Совершенно ясно, что корень квадратный тут для красоты, и минимизировать нужно $\sum_{i=1}^s \rho(x_i,W)^2$.

Прежде всего установим, что какой бы <<минимайзер>> $W=U+a_0$ мы не нашли, в $W$ всегда будет лежать среднее $\frac{1}{s}\sum_{i=1}^s x_i$ и, следовательно, в качестве $a_0$ всегда можно взять среднее. Действительно, распишем условие, что сумма 
$$\sum_{i=1}^s \rho(x_i-a_0, U)^2=\sum \|pr_{U^{\bot}} (x_i-a_0)\|^2$$
минимальна.  Продифференцируем по координатам $a_0$. Получим $$\sum -2pr_{U^{\bot}} x_i + 2s \,pr_{U^{\bot}} a_0=0$$
 Это условие означает, что проекции $a_0$ и среднего $\frac{1}{s}\sum x_i$ на подпространство $U^{\bot}$ совпадают. То есть эти две величины отличаются на элемент $U$. Тогда среднее лежит в $W$. 

Итак, вычтя из всех $x_i$ их среднее можно считать $a_0=0$, а все точки $x_i$ удовлетворяют равенству $\sum_{i=1}^s x_i=0$. Замечу, что это равенство нигде в дальнейшем не будет использовано. Благодаря такой замене мы свели задачу к поиску подпространства $U$ размерности $k$, которое минимизирует 
$$\sum_{i=1}^s \rho(x_i, U)^2=\sum_{i=1}^s \|pr_{U^{\bot}} (x_i)\|^2.$$

Воспользуемся тем, что $\|x\|^2=\|pr_U x\|^2+\|pr_{U^{\bot}} x\|^2$ или в другом виде $\|x\|^2-\|pr_U x\|^2=\|pr_{U^{\bot}} x\|^2$. Получаем, что выражение
$$\sum_{i=1}^s \|pr_{U^{\bot}}(x_i)\|^2=\sum_{i=1}^s \|x_i\|^2-\|pr_U x_i\|^2$$
должно быть минимально. Сумма $\|x_i\|^2$ постоянна. Значит  необходимо и достаточно, чтобы выражение $\sum_i \|pr_U x_i\|^2$ было максимально.


Для того чтобы посчитать проекцию на $U$ в каждом слагаемом выберем в $U$ ортонормированный базис $u_1,\dots,u_k$. Перепишем
$$\sum_{i=1}^s \|pr_U x_i\|^2=\sum_{i=1}^s\sum_{j=1}^k \lan x_i,u_j\ran^2=\sum_{j=1}^k \sum_{i=1}^s \lan x_i,u_j\ran^2.$$

Внутренняя сумма теперь есть значение некоторой квадратичной формы на векторе $u_j$. Разберёмся, что это за форма. Рассмотрим матрицу $X$ размера $s\times n$, чьи строки это  вектора $x_i$ $i\in\ovl{1,s}$. Тогда вектор $d_j=Xu_j$ состоит из скалярных произведений  $\lan x_i, u_j\ran$. Значит 
$$\lan d_j,d_j\ran = (u_jX)^{\top}Xu_j = \sum_{i=1}^s \lan x_i,u_j\ran^2,$$ 
что совпадает со слагаемым нашей суммы. Рассмотрим симметричную матрицу $A=X^{\top}X$ и обозначим соответствующую ей форму за $q$. Тогда нам надо минимизировать выражение

$$\sum_{j=1}^k q(u_j).$$

Таким образом мы ищем максимум $\Tr q|_{U}$ по всем подпространствам $U$ размерности $k$, где форма $q$ соответствует матрице $X^{\top} X$. А его мы искать умеем. Сформулируем  ответ. 


\thrm  Пусть есть набор векторов $x_1,\dots,x_s \in V$. Определим симметричную, положительно полуопределённую матрицу $A$, как $A=X^{\top}X$, где строчки матрицы $X$ -- это вектора $x_i$. Тогда минимум по всем аффинным подпространствам $V$ размерности $k$ выражения $\sum_{i=1}^s \rho(x_i,W)^2$ достигается при $a_0=\frac{1}{s}\sum x_i$  и $U=\lan v_1,\dots,v_k\ran$, где $v_i$ --- собственные вектора $A$, причём соответствующие собственные числа упорядочены по убыванию. 
\ethrm

\rm Стоит немного сказать про вероятностную интерпретацию полученного ответа. Предположим, что точки $x_i\in \mb R^n$ сгенерированы каким-то случайным процессом и подчиняются некоторому общему распределению. Тогда вектор $a_0$ -- это просто оценка на математическое ожидание этого распределения.

После того как мы нашли $a_0$ мы центрируем набор $x_i$ и минимизируем $\sqrt{\sum \rho (x_i, U)^2}$. Представим себе, что $U=\{0\}$. Что значит эта сумма? Это то, что мы назвали бы оценкой дисперсией величины (конечно, надо ещё усреднить). 
А что если $U \neq \{0\}$? Тогда $\sqrt{\sum \rho (x_i, U)^2}$ -- это заготовка для оценки дисперсии проекций случайных величин на $U^\bot$. То есть мы ищем подпространство $U$, так что на ортогональное дополнение приходится минимально возможная дисперсия (а при проекции на само $U$ должен достигаться максимум дисперсии среди всех подпространств той же размерности).

Какой же смысле имеет матрица $X^\top X$, её собственные вектора и собственные числа. Матрица $X^\top X$ -- это с точностью до константы ($1/s-1$) эмпирическая матрица ковариации. Если мы хотим посчитать квадрат дисперсии в направлении $v$, то мы считаем $v\top X^\top X v$. Значит собственные вектора этой матрицы -- это такие вектора, для которых достигается экстремум квадрата дисперсии. А собственные числа -- это собственно квадраты дисперсии.
\erm

\rm Отдельно отметим, что в приложениях принято с самого начала центрировать данные и нормировать каждую компоненту $x_i$ так, чтобы дисперсия вдоль каждого направления была единичной.
\erm

При вычислении с помощью метода главных компонент часто используют несколько другие конструкции. Для того, чтобы в этом разобраться посмотрим на матрицу  $X^{\top}X$  и её собственные числа.  

\dfn Пусть $A$ -- линейное отображение между евклидовыми пространствами $U \to V$. Тогда сингулярными значениями $A$ называются корни из положительных собственных чисел оператора $A^*A$.
\edfn



Теперь обсудим важную конструкцию, проясняющую геометрический смысл сингулярных значений. Для этого мы перейдём к бескоординатной интерпретации произведения $X^\top X$.


\thrm[SVD разложение] Пусть $L$ -- линейное отображение $L\colon U \to V$ между евклидовыми пространствами. Тогда существуют такие ортонормированный базисы $U$ и $V$, что матрица $L$ имеет вид 
$$\Sigma=\pmat \sigma_1 &\dots& 0 & 0\\
 \vdots & \ddots &\vdots & \vdots\\
 0 & \dots & \sigma_r & 0\\
 0 &  \dots & 0 & 0 \epmat,$$
 где $r$ -- ранг $L$. Числа $\sigma_1, \dots, \sigma_r$ на диагонали обязаны быть равными сингулярным значениям $L$.
На языке матриц это означает, что для любой матрицы $A \in M_{m\times n}$ существуют ортогональные матрицы  $P$ -- размера $m$ и $Q$ -- размера $n$,  что
$$A= P \Sigma Q.$$
 
\proof Рассмотрим оператор $B = L^{*}L$. Тогда существуют ортонормированный базис $e_1,\dots,e_n$ в котором оператор $B$ диагонален, с неотрицательными числами на диагонали $d_1\geq\dots\geq d_n\geq 0$. Имеем  $d_i=\sigma_i^2$ для единственного положительного $\sigma_i$. 
Посмотрим на вектора $Le_i \in U$. Они ортогональны. Действительно
$$\lan Le_i, Le_j\ran = \lan L^{*}Le_i,e_j \ran = \lan d_i e_i,e_j\ran,$$
что равно нулю, если $i\neq j$. В случае $i=j$ получаем $\|Le_i\|^2=d_i$. Возьмём 
$$f_i=\frac{Le_i}{\sqrt{d_i}}$$
и дополним этот набор до ортонормированного базиса пространства $U$. Итого имеем $e_1,\dots,e_n$ ортонормированный базис $U$ и $f_1,\dots,f_m$ -- ортонормированный базис $V$.
Посмотрим матрицу $A$ в этих базисах. По определению $Le_i=\sqrt{d_i}f_i$. Это и даёт требуемый вид матрице оператора $L$.


Напоследок осталось решить вопрос, как выглядит матрица $Q$. В нашей конструкции матрица $Q$ есть матрица замены координат из стандартного базиса в базис из собственных векторов $e_i$ матрицы $A^{\top}A$. Если за $C$ обозначить матрицу из столбцов $e_i$, то $Q=C^{-1}$, но $C$ ортогональна и поэтому можно написать $Q=C^{T}$, то есть строки $Q$ -- собственные вектора $A^{\top}A$.
\endproof
\ethrm

\upr Получите аналогичное описание для $P$.
\eupr

\dfn Базисные вектора такой системы координат в $U$ называются левыми сингулярными векторами $A$, а базис в $V$ -- правыми.  
\edfn

Наличие SVD-разложения означает, что для всякого линейного отображения можно так выбрать декартову систему координат, что в этой системе координат это отображение будет выглядеть как растяжение вдоль каких-то осей.





SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $V_k$, то получится матрица ранга $k$ или меньше. Эта матрица будет ближайшей к исходной в смысле такой (и не только) нормы на пространстве матриц, называемой, нормой Фробениуса 
$$\|X\|_F=\sqrt{\sum_{i,j} x_{ij}^2}=\sqrt{\Tr X^{\top}X}.$$
Таким образом, нахождение проекций точек можно переформулировать, как нахождение ближайшей к $X$ матрице ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение.

Прежде чем использовать SVD-разложение заметим, что в такой формулировке задача может быть поставлена в бескоординатном виде: для данного линейного отображения $A$ между евклидовыми пространствами найти наиболее близкое к нему линейное отображение ранга $k$ относительно нормы $\|A\|=\sqrt{\Tr A^*A}$.

\thrm Пусть $A$ линейное отображение между евклидовыми пространствами. Пусть $\Sigma$ -- матрица с сингулярными значениями $A$ на диагонали. Тогда ближайшее к $A$ отображения ранга $k$ имеет матрицу $\Sigma^{(k)}$ в базисах из правых и левых сингулярных векторов. Здесь  на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальное -- нули.
\proof Перейдём в базис из правых и левых сингулярных векторов. В этих базисах матрица $A$ -- это $\Sigma$.
Значит теперь мы решаем задачу нахождения ближайшей матрицы к матрице $\Sigma$. Для этого нам надо взять проекции строк $\Sigma$ на подпространство порождённое первыми $k$ собственными векторами $\Sigma^\top \Sigma$. После этого из этих проекций надо составить матрицу. Но первые $k$ собственных векторов $\Sigma^\top \Sigma$ это просто первые $k$ координатных векторов. Значит составленная из проекций матрица -- это $\Sigma^{(k)}$.
\endproof
\ethrm 

\crl Пусть $A$ имеет сингулярное разложение $P\Sigma Q$. Тогда ближайшая к $A$ матрица ранга $k$ (в смысле нормы Фробениуса) имеет вид $A^{(k)}=P\Sigma^{(k)}Q$, где на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальные элементы матрицы -- нули.
\ecrl

\upr На самом деле матрица $A^{(k)}$ ближайшая к $A$ среди матриц ранга $k$ и в смысле обычной матричной $l_2$-нормы. 
\eupr


\end{document}








\end{document}
