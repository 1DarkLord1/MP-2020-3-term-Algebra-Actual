\documentclass[10pt,a4paper,oneside]{book}
\usepackage[a4paper,includeheadfoot,top=10mm,bottom=10mm,left=10mm,right=10mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{cmap} % Поддержка поиска русских слов в PDF (pdflatex)
\usepackage{comment}
%\usepackage{pdfsync} % синхронизация

\usepackage{amsmath,amsthm,amssymb,amscd,array}
\usepackage{latexsym}
\usepackage{stmaryrd} % Для знака нормальной подгруппы
\usepackage{misccorr} % российская полиграфия
\usepackage{indentfirst}% Красная строка в первом абзаце
\usepackage{ccaption} % Заголовки таблиц и рисунков
\usepackage{fancyhdr} % колонтитулы
\usepackage{hyperref} % гиперссылки

\usepackage{rotating} % Поворот текста
\usepackage{graphicx} % Вставка изображений
\usepackage{xcolor}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}
\graphicspath{ {./} } % относительно main.tex

\usepackage{arydshln} % штрихованные линии в массивах
\usepackage{mathtools} % выравнивание в матрицах
\usepackage{multirow} % слияние в столбце
\usepackage{multicol} % нумерация в нескольких колонках


\newtheorem{uprz}{\color{violet!100!black} Упражнение}
\newtheorem{predl}{\color{blue!50!black} Предложение}
\newtheorem{komment}{\color{green!50!blue} Комментарий}
\newtheorem{conj}{Гипотеза}
\newtheorem{notation}{\color{yellow!30!red} Обозначение}


\theoremstyle{definition}
\newtheorem{kit}{Кит}
\newtheorem*{rem}{\color{green!50!blue}Замечание}
\newtheorem{zad}{\color{violet!100!black}Задача}
\newtheorem*{defn}{\color{yellow!30!red} Определение}
\newtheorem*{fact}{Факт}
\newtheorem{thm}{\color{red!40!black}Теорема}
\newtheorem*{thmm}{\color{red!40!black} Теорема}
\newtheorem{lem}{\color{green!50!black}Лемма}
\newtheorem{cor}{\color{green!45!black}Следствие}
\newtheorem{utvr}{\color{blue!50!black}Утверждение}


\newcommand\tikznode[3][]%
   {\tikz[remember picture,baseline=(#2.base)]
      \node[minimum size=0pt,outer sep=0pt,#1](#2){#3};%
   }
\tikzset{>=stealth}


\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={red!80!black}
}
% цвета для ссылок


\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother


\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\proofname}{Доказательство}
\renewcommand{\mod}{\,\operatorname{mod}\,}
\renewcommand{\Re}{\operatorname{Re}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ovl}{\overline}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\K}{\operatorname{K_0}}
\newcommand{\witt}{\operatorname{W}}
\newcommand{\gw}{\operatorname{GW}}
\newcommand{\coh}{\operatorname{H}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\cl}{\operatorname{Cl}}
\newcommand{\Vol}{\operatorname{Vol}}
\newcommand\tgg{\mathop{\rm tg}\nolimits}
\newcommand\ccup{\mathop{\cup}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\chr}{\operatorname{char}}
\newcommand{\rk}{\operatorname{rk}}
\DeclareMathOperator{\Coker}{Coker}
\DeclareMathOperator{\Ker}{Ker}
\newcommand{\im}{\operatorname{Im}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\re}{\operatorname{Re}}
\newcommand{\tr}{\operatorname{Tr}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\orb}{\operatorname{\mathcal O}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\Out}{\operatorname{Out}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\SO}{\operatorname{SO}}
\renewcommand{\O}{\operatorname{O}}
\renewcommand{\U}{\operatorname{U}}
\newcommand{\Sym}{\operatorname{Sym}}
\newcommand{\Adj}{\operatorname{Adj}}
\newcommand{\Disc}{\operatorname{Disc}}
\newcommand{\cnt}{\operatorname{cont}}
\newcommand{\Frob}{\operatorname{Frob}}
\newcommand{\Iso}{\operatorname{Iso}}
\newcommand{\Isom}{\operatorname{Isom}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\di}{\mathop{\,\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\ndi}{\mathop{\not\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\nequiv}{\not \equiv}
\newcommand{\Nod}{\operatorname{\text{НОД}}}
\newcommand{\Nok}{\operatorname{\text{НОК}}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\Aff}{\operatorname{Aff}}
\newcommand{\AGL}{\operatorname{AGL}}
\newcommand{\PSL}{\operatorname{PSL}}
\newcommand{\Volume}{\operatorname{Volume}}

\def\llq{\textquotedblleft} 
\def\rrq{\textquotedblright} 
\def\exm{\noindent {\bf Примеры:}}


\def\Cb{\ovl{C}}
\def\ffi{\varphi}
\def\pa{\partial}
\def\V{\bf V}
\def\La{\Lambda}
\def\eps{\varepsilon}
\def\del{\delta}
\def\Del{\Delta}
\def\A{\EuScript{A}}
\def\lan{\left\langle }
\def\ran{\right\rangle}
\def\bar{\begin{array}}
\def\ear{\end{array}}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\thrm{\begin{thm}}
\def\ethrm{\end{thm}}
\def\dfn{\begin{defn}}
\def\edfn{\end{defn}}
\def\lm{\begin{lem}}
\def\elm{\end{lem}}
\def\zd{\begin{zad}}
\def\ezd{\end{zad}}
\def\prdl{\begin{predl}}
\def\eprdl{\end{predl}}
\def\crl{\begin{cor}}
\def\ecrl{\end{cor}}
\def\rm{\begin{rem}}
\def\erm{\end{rem}}
\def\fct{\begin{fact}}
\def\efct{\end{fact}}
\def\enm{\begin{enumerate}}
\def\eenm{\end{enumerate}}
\def\pmat{\begin{pmatrix}}
\def\epmat{\end{pmatrix}}
\def\utv{\begin{utvr}}
\def\eutv{\end{utvr}}
\def\upr{\begin{uprz}}
\def\eupr{\end{uprz}}
\def\nrml{\trianglelefteqslant}

\frenchspacing
\righthyphenmin=2
%\usepackage{floatflt}
\captiondelim{. }

\title{Современное программирование \\ 
Конспект по алгебре, 3 семестр}
\date{}


\begin{document}

\tableofcontents

\chapter{Линейная алгебра}

\section{Кватернионы}


Наша цель сейчас рассказать про геометрию трехмерного пространства используя при этом определённые алгебраические конструкции. А именно, ещё в XIX веке Уильям Роуэн Гамильтон стал искать аналогичную комплексным числам алгебраическую систему на трёхмерном пространстве.  
Однако, подходящий аналог удалось найти только в четырёхмерной ситуации.


Рассмотрим вещественное подпространство в алгебре матриц $M_2(\mb C)$ вида
$$\mb H = \left\{\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha} \epmat \right\}.$$
Базис этого пространства, как вещественного векторного пространства, состоит из матриц 
$$ 1=\pmat 1 & 0 \\ 0& 1 \epmat, i= \pmat i & 0 \\ 0& -i \epmat, j=\pmat 0& 1 \\ -1 & 0 \epmat, k=\pmat 0 & i \\ i & 0\epmat. $$ 
Покажем, что это вещественная подалгебра в $M_2(\mb C)$ и следовательно ассоциативное кольцо. 
Для этого достаточно показать, что произведение базисных снова лежит в $\mb H$. Имеем $$i^2=j^2=k^2=-1 \text{ и } ij=k=-ji,$$ откуда $$ik= iij=-j=jii=-ki \text{ и } jk=-jji=i=-kj.$$ Таким образом $\mb H$ образует ассоциативную алгебру размерности 4 над $\mb R$.
 
\dfn[Алгебра кватернионов] $\mb H$ называется алгеброй кватернионов. 
\edfn
Мы больше не будем думать (кроме одного нюанса) про кватернионы как про матрицы, а будем записывать их через $i,j,k$. Именно так обычно кватернионы и вводят -- как формальный суммы $a+bi+cj+dk$, для произведения которых выполнены тождества $i^2=j^2=k^2=-1$ и $ij=-ji=k$. Посмотрев на кватернионы как на матрицы мы сэкономили на доказательстве ассоциативности умножения.

\zd Алгебра кватернионов не снабжается структурой $\mb C$-алгебры.
\ezd






\dfn[Векторная и скалярная часть, сопряжённый кватернион] Пусть $x= a+bi+cj+dk$ кватернион. Определим вещественную или скалярную часть $\Re x=a$ и векторную часть $v= bi+cj+dk$ кватерниона. Сопряжённым кватернионом называется $\ovl{x}= a-bi-cj-dk= \Re x - \Im x =a-v$. 
\edfn



Посмотрим, как перемножаются кватернионы. Если оба кватерниона  $x$, $y$ разделить на скалярную и векторную части $x=a+v$, $y=b+u$ то $xy=ab+au+bv+ vu$. Нам осталось разобраться с умножением векторных частей. 

Рассмотрим произведение двух чисто мнимых кватернионов $uv=-\lan u,v\ran+[u,v]$. Его вещественная часть совпадает с минус скалярным произведением векторов. Про мнимую часть мы поговорим отдельно.

\dfn[Векторное произведение] Пусть $u,v \in \mb R^3$ два вектора. Тогда их векторным произведением называется вектор $[u,v]$.
\edfn

Если расписать в координатах $u=x_1i+x_2j+x_3k$ и  $v=y_1i+y_2j+y_3k$, то векторное произведение задаётся формулой

$$[u,v]= (x_2y_3-x_3y_2)i + (x_3y_1-x_1y_3)j + (x_1y_2- x_2y_1)k= \begin{vmatrix} i& j&k \\ x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \end{vmatrix} $$

\rm Операция $(u,v) \to [u,v]$ является билинейной и антисимметричной, то есть $[u,u]=0$ и, следовательно, $[u,v]=-[v,u]$.
\erm

Последнее замечание позволяет нам легко вычислить $x \ovl{x}= a^2+ \lan v,v \ran$. Это приводит нас к определению:

\dfn[Норма кватерниона] Определим норму кватерниона как $$\|x\|=\sqrt{x\ovl{x}}=\sqrt{ a^2+b^2+c^2+d^2}=\sqrt{\ovl{x}x}.$$
\edfn 


Норма кватерниона, как и модуль комплексного числа всегда положительны для ненулевых элементов. Это позволяет заметить, что

\dfn[Обратный кватернион] Если $0\neq x \in \mb H$, то $x^{-1}=\frac{\ovl{x}}{\|x\|^2}$. 
\edfn

Таким образом мы получили первый (и для нас единственный) пример некоммутативного кольца с делением. Такие кольца называются телами. Напоминаю, что алгебра для нас ассоциативна и с единицей. Неассоциативные алгебры представляют интерес. Например, можно взять $\mb R^3$, где в качестве умножения взято векторное произведение. Это пример неассоциативной алгебры или, точнее, алгебры Ли. В этом курсе мы не  обсуждаем неассоциативные алгебры в связи с тем, что им обычно находится применение либо внутри физических дисциплин, либо внутри самой математики и редко где ещё. 

Какие ещё свойства есть у отображения нормы? Если следовать параллели с комплексными числами, то стоит посмотреть, что происходит с нормой произведения. Для того, чтобы не обременяться вычислениями сделаем небольшой трюк и на секунду вспомним матричное представление кватернионов. Заметим, что на матричном языке, норма -- это $\|x\|=\sqrt{\det x}$, откуда получаем

\lm[Норма мультипликативна] $\|xy\|=\|x\|\|y\|$. В частности, $\|x^{-1}\|=\|x\|^{-1}$.
\proof Вспомним в последний раз, что кватернионы задаются матрицами из $M_2(\mb C)$. Пусть 
$$x=\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat.$$
Тогда $$\|x\|^2=|\alpha|^2+|\beta|^2=\det \pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat,$$
а определитель мультипликативен.
\endproof
\elm



Продолжим. Используя мультипликативность нормы легко доказать 
\lm Отображение $x \to \ovl{x}$ является антиизоморфизмом алгебр, то есть $\ovl{ab}=\ovl{b}\ovl{a}$.
\proof Линейной ясна. Пусть $x,y \neq 0$. Тогда $$\frac{\ovl{y}\,\ovl{x}}{\|y\|^2\|x\|^2}=y^{-1}x^{-1}=(xy)^{-1}=\frac{\ovl{xy}}{\|xy\|^2}.$$
\elm

На самом деле и здесь можно было воспользоваться матричным представлением. А именно, можно заметить, что операция сопряжения кватернионов совпадает на этом языке с транспонированием и сопряжением соответствующей комплексной матрицы. Вернёмся теперь к векторному произведению.





\lm[Свойства векторного произведения] Верны следующие свойства\\
1) Для любых $u,v \in \mb R^3$ верно $u\bot [u,v]$. Точнее $$u[u,v]= -\|u\|^2v+ \lan u,v\ran u$$
2) $\| [u,v]\|= \|u\|\|v\| \cdot |\sin \ffi |$, где $\ffi$ --  это угол между $u$ и $v$.
\elm
\proof Для того, чтобы посчитать скалярное произведение $\lan u, [u,v]\ran$ необходимо посчитать скалярную часть $u[u,v]$. 
$$u[u,v]= u (uv+ \lan u,v \ran)= u^2 v+ \lan u,v \ran u= -\|u\|^2 v+ \lan u,v \ran u$$
Последнее выражение, очевидно, чисто векторное.
Теперь
\begin{align*}
&\|[u,v]\|^2= -[u,v][u,v]= (uv + \lan u,v\ran)(vu + \lan u,v\ran)=\\
&=\|u\|^2\|v\|^2+ \lan u,v\ran^2 + \lan u,v\ran (uv+vu)=\|u\|^2\|v\|^2 - \lan u,v\ran^2= \|u\|^2\|v\|^2(1-\cos^2 \ffi)
\end{align*}
\endproof

\dfn Обозначим за $\mb H_{1}$ группу кватернионов, по норме равных единице.
\edfn

\thrm Отображение $\mb H_{1}\to \GL_3(\mb R)$ заданное по правилу $x\to (y \to xyx^{-1})$ корректно определено и даёт сюръективный  гомоморфизм из группы кватернионов единичной нормы в $\SO_3(\mb R)$. Ядро этого гомоморфизма состоит из $\{\pm 1\}$. Точнее, если единичный кватернион $x$  представим в виде $x=a+bv$, то соответствующее ему вращение есть вращение относительно  оси $\lan v \ran$ на угол $2\ffi$, где $\cos \ffi= a$, $\sin \ffi= b$ или тождественное преобразование в случае $v=\pm 1$.
\ethrm
\proof Рассмотрим преобразование $L_x \colon \mb H \to \mb H$ вида $y \to xyx^{-1}$. Прежде всего покажем, что мы получили ортогональное преобразование $\mb R^4$. Имеем
 $$\|xvx^{-1}\|=\|x\| \|v\| \|x^{-1}\| = \|v\|.$$
Теперь заметим, что преобразование $L_x$ сохраняет на месте вектор 1 и, следовательно, его ортогональное дополнение, то есть $\mb R^3$. Таким образом $L_x$ ограничивается на $\mb R^3$. Далее, очевидно, $L_xL_y= L_{xy}$. Осталось посчитать ядро гомоморфизма и явный вид отображения $L_x$. Заметим, что если $x=a+bv$, то $L_x$ оставляет $v$ на месте. Действительно, при $b\neq 0$ 
$$xbvx^{-1}=x(x-a)x^{-1}= x-a=bv.$$
Вычислим угол поворота. Для этого рассмотрим нормированный вектор  $u\bot v$ и $[u,v]$, которые образуют ортонормированный базис дополнения и посчитаем $xux^{-1}$ и $x[u,v]x^{-1}$. Из условия ортогональности следует, что $uv=[u,v]=-[v,u]=-vu$ и значит $v[u,v]=-[u,v]v=-uv^2=u$. Теперь
\begin{align*}
xux^{-1}&=(a+bv)u(a-bv)= (a+bv)(au-buv)=\\
&=a^2u -ab[u,v]+ab[v,u]- b^2vuv=a^2u-2ab[u,v]-b^2\|v\|^2u=\\ &=(a^2-b^2)u-2ab[u,v]=\cos2\ffi u+ \sin 2\ffi [u,v]
\\
\\
x[u,v]x^{-1}&=(a+bv)[u,v](a-bv)= (a[u,v]+bu)(a-bv)=\\
&=a^2[u,v]+abu-ab[u,v]v-b^2uv=\\
&=(a^2-b^2)[u,v]+2abu=\cos 2\ffi[u,v]+\sin 2\ffi u
\end{align*}

Осталось показать, что только $x=\pm 1$ лежит в ядре этого отображения. Это возможно только тогда, когда $2\ffi \equiv 0 \mod 2\pi$. Значит $\ffi=2 \pi$ или $\ffi= \pi$. Первое соответствует $x=1$. Втораое --  $a=\cos \ffi = -1$, а $b=\sin \ffi = 0$, то есть $x=-1$. 
\endproof


\zd
Покажите, что отображение $(x,y) \to (z \to xzy^{-1})$ задаёт сюръективный гомоморфизм из декартового квадрата группы единичных кватернионов в группу $\SO_4(\mb R)$ с ядром $\{(1,1),(-1,-1)\}$.
\ezd

Обсудим теперь для чего могут понадобится кватернионы. Каждый кватернион, как мы установили кодирует вращение трёхмерного пространства или, что тоже самое -- новую декартову систему координат в $\mb R^3$ с центром в нуле. Такой тип данных встречается в компьютерной графике, если вы хотите зафиксировать ракурс, в котором вы смотрите на 3d-сцену или положение какого-то конкретного объекта в такой сцене.

В такой задаче кватернионы сложно превзойти. Действительно, задание сцены при помощи кватернионов очень экономно -- 4 коэффициента с одним соотношением на них (3 коэффициента, если очень нужно) против 9 коэффициентов у ортогональной матрицы.

А что с эффективностью операций? Тут вопрос состоит в том, про какие операции идёт речь. Если вы хотите сказать, что тот или иной вектор, который был повёрнут на кватернион $q=a+v$ надо повернуть ещё на кватернион $p=b+u$, то вам надо всего лишь вычислить $pq=ab+au+bv+uv$. Такое произведение считается за 16 умножений и 12 сложений. Если брать произведение матриц $3\times 3$, то там получается 27 умножений и 18 сложений (можно обойтись и 23 умножениями, но сильно увеличив число сложений).

Можно конечно использовать углы Эйлера, но тогда придётся использовать для вычислений косинусы и синусы этих углов, которые сами даются не бесплатно.

Если вам даны два ракурса в виде кватернионов, то легко понять, на какой кватернион надо домножить, чтобы из первого получить второй.

Но что если вы хотите просто повернуть при помощи кватерниона какой-то вектор из $\mb R^3$. Тут ситуация несколько хуже. Для этого вам необходимо посчитать $qxq^{-1}$. Предположив, что $q$ нормирован можно заменить обращение на сопряжение. Тем не менее такой подход довольно дорог -- 32 умножения и 24 сложения. Конечно, такой способ не оптимален. Например, не обязательно считать скалярную часть -- она должна стать нулевой. На самом деле, если $q=a+v$, то 
$$qxq^{-1}=x+ 2[v, [v,x]+ ax]$$
что даёт 15 умножений и 15 сложений (если считать умножение на 2 сложением). Это конечно отличается от матриц, где необходимо 9 умножений и 6 сложений.

Впрочем, отличается не сильно. Кроме того у кватернионного представления есть плюс произведение нескольких кватернионов -- это кватернион несмотря на ошибки округления. Что не так для ортогональных матриц.

Наконец, представим себе задачу, что нам нужно плавно перейти от ракурса $q$ к ракурсу $p$. Желательно с "равномерной" скоростью. На языке кватернионов это становится понятно. Для этого заметим, что единичные кватернионы -- это всего лишь точки на трёхмерной сфере. Несложно понять, что их соединяет часть дуги сферы, точки на которой заданы как 
$$ \frac{\sin(t\theta)p + \sin ((1-t)\theta) q}{\sin \theta},$$
где $\theta$ -- угол между $p$ и $q$ (острый, не забываем, что представление кватернионами немного не однозначно).





\section{Задачи на максимизацию}

Теперь обратимся к вопросам, связанным с вещественными самосопряжёнными операторами.

Рассмотрим один из вопросов, связанных с такой конструкцией, а именно, рассмотрим задачу о нахождении нормы линейного оператора $L \colon U \to V$ между двумя евклидовыми пространствами. Для того, чтобы найти  норму необходимо найти $$\max_{x\neq 0}\sqrt{\frac{\lan Lx,Lx\ran}{\|x\|^2}}=\sqrt{\max_{x\neq 0}\frac{\lan L^*Lx,x\ran}{\|x\|^2}}=\sqrt{\max_{\|x\|=1} \lan L^*Lx,x\ran}.$$
Таким образом, нахождение нормы оператора свелось к задаче максимизации квадратичной формы на единичной сфере. Заметим, что максимум действительно достигается благодаря компактности сферы.

Оказывается, что довольно легко найти максимум или минимум квадратичной формы на сфере.



\thrm Пусть $V$ -- евклидово пространство, $A$ -- самосопряжённый оператор на $V$, а $q(x)=\lan x,Ax\ran$ -- соответствующая квадратичная форма. Тогда 
$$\max_{ x\in V } \frac{q(x)}{||x||^2}=\max_{\substack{ x\in V \\ ||x||=1}} q(x)=\lambda_1,$$
 где $\lambda_1$ - наибольшее собственное число оператора $A$ и достигается на собственном векторе $v_1$, соответствующему $\lambda_1$. Аналогично минимум равен минимальному собственному числу $A$. 
\proof
Пусть $v=\sum c_i e_i$, причём $1=||v||^2=\sum c_i^2$. Тогда $\lan Av,v\ran = \sum c^2_i \lambda_i $, что меньше $\lan A e_1,e_1\ran= \lambda_1= \sum \lambda_1 c_i^2$.
\endproof
\ethrm

Эта теорема, кроме, собственно, решения задачи, даёт геометрическую характеризацию первого собственного числа. Вопрос: можно ли аналогично охарактеризовать другие собственные числа? Ответ получается не таким простым, но, тем не менее, полезным.

\thrm[Куранта-Фишера] Пусть $q(x)=\lan x, Ax\ran$. Тогда $k$-ое по убыванию собственное число $\lambda_k$ для $A$ есть 
$$ \lambda_k=\max_{\dim L=k} \min_{\substack{ x\in L \\ ||x||=1}} q(x) = \min_{\dim L=n-k+1} \max_{\substack{ x\in L \\ ||x||=1}} q(x).$$
Причем максимум достигается на инвариантном подпространстве, содержащем собственные вектора для $\lambda_1,\dots,\lambda_k$.
\ethrm
\proof Пусть $U$ --- подпространство на котором достигается максимум, причём допустим, что максимум больше $\lambda_k$. Тогда рассмотрим подпространство $W=\lan v_k,\dots,v_n\ran$, где $v_i$ --- собственный вектор соответствующий $i$-ому по убыванию собственному числу. Заметим, что $U\cap W=\{0\}$, так как на $W$ форма принимает значения меньше или равные $\lambda_k$, а на $U$ -- строго большие. Однако $\dim W=n-k+1$. Приходим к противоречию с подсчётом размерности пересечения. 
\endproof




\crl Пусть $U$ некоторое подпространство, а $q(x)=x^{\top} Ax$. Пусть собственные числа $A$ --- это $\lambda_i$, а собственные числа оператора, соответствующего $q(x)|_U$ -- это $\mu_i$ упорядоченные по убыванию. Тогда 
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$  
\proof Заметим, что
 $$\mu_i=\max_{\substack{L\leq U\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x),$$
что очевидно меньше, чем 
$$\max_{\substack{L\leq V\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x)=\lambda_i.$$ Неравенство в другую сторону получается из второго описания в теореме Куранта-Фишера.
\endproof
\ecrl


Введём не совсем стандартное определение:
\dfn
Пусть $q$ -- квадратичная форма на евклидовом пространстве. Рассмотрим ортонормированный базис $u_i$ пространства $V$. Тогда положим 
$$\Tr q= \sum q(u_i).$$ Если в базисе $u_i$ форме $q(x)=x^{\top} Ax $ соответствует симметричная матрица $A$, то $\Tr q(x)=\Tr(A)$.
\edfn

\rm Определение не зависит от выбора ортонормированного базиса. Действительно, если замена координат ортогональна, то матрица $q$ в новой системе координат имеет вид $C^{\top}AC=C^{-1}AC$. Осталось заметить, что след последней матрицы очевидно равен следу $A$.
\erm

\rm Если форме $q$ соответствует самосопряжённый оператор $A$, то $\Tr q=\sum \lambda_i$, где $\lambda_i$ -- собственные числа $A$.
\erm




\crl Пусть $q(x)=x^{\top} Ax$, $U$ --- некоторое подпространство, $\dim U=k$. Пусть собственные числа $A$ --- это $\lambda_i$. Тогда $$\Tr q|_U\leq \sum_{i=1}^k \lambda_i= \Tr q|_{V_k},$$
где $V_k$ подпространство натянутое на первые $k$ собственных векторов $q$.
\proof Нам известны неравенства на собственные числа $\mu_i$, ограничения $q|_U$. А именно, $\mu_i\leq \lambda_i$. Но тогда и $$\sum_{i=1}^k \mu_i \leq \sum_{i=1}^k \lambda_i.$$
\endproof
\ecrl 






\section{Метод главных компонент}

Рассмотрим следующую задачу: имеется массив данных --- набор векторов $x_1,\dots,x_s \in \mb R^n$. Подразумевается, что точек очень много. Предположим, что при идеальных измерениях между координатами этих векторов есть линейные зависимости. Все отклонения от этой погрешности вызваны небольшими погрешностями. Задача состоит в том, чтобы восстановить линейную зависимость. 


Переформулируем задачу геометрически. Пусть наши вектора удовлетворяют линейным условиям $Ax_i=b$ для некоторой матрицы $A$ ранга $n-k$. Это значит, что они лежат в аффинном подпространстве $\Ker A + a_0$ размерности $k$. Если же нам даны вектора с погрешностями, то задачу таким образом можно поставить в виде: найти аффинное подпространство размерности $k$ наиболее близкое к данным точкам $x_1,\dots,x_s$. Понятие <<наиболее близкое>> требует конкретизации. Вообще говоря, тут есть выбор. Мы будем считать, что подходящее пространство $W=U+a_0$ должно давать минимум следующего выражения:
$$\sqrt{\sum_{i=1}^s \rho(x_i,W)^2} \to \min.$$
Совершенно ясно, что корень квадратный тут для красоты, и минимизировать нужно $\sum_{i=1}^s \rho(x_i,W)^2$.

Прежде всего установим, что какой бы <<минимайзер>> $W=U+a_0$ мы не нашли, в $W$ всегда будет лежать среднее $\frac{1}{s}\sum_{i=1}^s x_i$ и, следовательно, в качестве $a_0$ всегда можно взять среднее. Действительно, распишем условие, что сумма 
$$\sum_{i=1}^s \rho(x_i-a_0, U)^2=\sum \|pr_{U^{\bot}} (x_i-a_0)\|^2$$
минимальна.  Продифференцируем по координатам $a_0$. Получим $$\sum -2pr_{U^{\bot}} x_i + 2s \,pr_{U^{\bot}} a_0=0$$
 Это условие означает, что проекции $a_0$ и среднего $\frac{1}{s}\sum x_i$ на подпространство $U^{\bot}$ совпадают. То есть эти две величины отличаются на элемент $U$. Тогда среднее лежит в $W$. 

Итак, вычтя из всех $x_i$ их среднее можно считать $a_0=0$, а все точки $x_i$ удовлетворяют равенству $\sum_{i=1}^s x_i=0$. Замечу, что это равенство нигде в дальнейшем не будет использовано. Благодаря такой замене мы свели задачу к поиску подпространства $U$ размерности $k$, которое минимизирует 
$$\sum_{i=1}^s \rho(x_i, U)^2=\sum_{i=1}^s \|pr_{U^{\bot}} (x_i)\|^2.$$

Воспользуемся тем, что $\|x\|^2=\|pr_U x\|^2+\|pr_{U^{\bot}} x\|^2$ или в другом виде $\|x\|^2-\|pr_U x\|^2=\|pr_{U^{\bot}} x\|^2$. Получаем, что выражение
$$\sum_{i=1}^s \|pr_{U^{\bot}}(x_i)\|^2=\sum_{i=1}^s \|x_i\|^2-\|pr_U x_i\|^2$$
должно быть минимально. Сумма $\|x_i\|^2$ постоянна. Значит  необходимо и достаточно, чтобы выражение $\sum_i \|pr_U x_i\|^2$ было максимально.


Для того чтобы посчитать проекцию на $U$ в каждом слагаемом выберем в $U$ ортонормированный базис $u_1,\dots,u_k$. Перепишем
$$\sum_{i=1}^s \|pr_U x_i\|^2=\sum_{i=1}^s\sum_{j=1}^k \lan x_i,u_j\ran^2=\sum_{j=1}^k \sum_{i=1}^s \lan x_i,u_j\ran^2.$$

Внутренняя сумма теперь есть значение некоторой квадратичной формы на векторе $u_j$. Разберёмся, что это за форма. Рассмотрим матрицу $X$ размера $s\times n$, чьи строки это  вектора $x_i$ $i\in\ovl{1,s}$. Тогда вектор $d_j=Xu_j$ состоит из скалярных произведений  $\lan x_i, u_j\ran$. Значит 
$$\lan d_j,d_j\ran = (u_jX)^{\top}Xu_j = \sum_{i=1}^s \lan x_i,u_j\ran^2,$$ 
что совпадает со слагаемым нашей суммы. Рассмотрим симметричную матрицу $A=X^{\top}X$ и обозначим соответствующую ей форму за $q$. Тогда нам надо минимизировать выражение

$$\sum_{j=1}^k q(u_j).$$

Таким образом мы ищем максимум $\Tr q|_{U}$ по всем подпространствам $U$ размерности $k$, где форма $q$ соответствует матрице $X^{\top} X$. А его мы искать умеем. Сформулируем  ответ. 


\thrm  Пусть есть набор векторов $x_1,\dots,x_s \in V$. Определим симметричную, положительно полуопределённую матрицу $A$, как $A=X^{\top}X$, где строчки матрицы $X$ -- это вектора $x_i$. Тогда минимум по всем аффинным подпространствам $V$ размерности $k$ выражения $\sum_{i=1}^s \rho(x_i,W)^2$ достигается при $a_0=\frac{1}{s}\sum x_i$  и $U=\lan v_1,\dots,v_k\ran$, где $v_i$ --- собственные вектора $A$, причём соответствующие собственные числа упорядочены по убыванию. 
\ethrm

\rm Стоит немного сказать про вероятностную интерпретацию полученного ответа. Предположим, что точки $x_i\in \mb R^n$ сгенерированы каким-то случайным процессом и подчиняются некоторому общему распределению. Тогда вектор $a_0$ -- это просто оценка на математическое ожидание этого распределения.

После того как мы нашли $a_0$ мы центрируем набор $x_i$ и минимизируем $\sqrt{\sum \rho (x_i, U)^2}$. Представим себе, что $U=\{0\}$. Что значит эта сумма? Это то, что мы назвали бы оценкой дисперсией величины (конечно, надо ещё усреднить). 
А что если $U \neq \{0\}$? Тогда $\sqrt{\sum \rho (x_i, U)^2}$ -- это заготовка для оценки дисперсии проекций случайных величин на $U^\bot$. То есть мы ищем подпространство $U$, так что на ортогональное дополнение приходится минимально возможная дисперсия (а при проекции на само $U$ должен достигаться максимум дисперсии среди всех подпространств той же размерности).

Какой же смысле имеет матрица $X^\top X$, её собственные вектора и собственные числа. Матрица $X^\top X$ -- это с точностью до константы ($1/s-1$) эмпирическая матрица ковариации. Если мы хотим посчитать квадрат дисперсии в направлении $v$, то мы считаем $v\top X^\top X v$. Значит собственные вектора этой матрицы -- это такие вектора, для которых достигается экстремум квадрата дисперсии. А собственные числа -- это собственно квадраты дисперсии.
\erm

\rm Отдельно отметим, что в приложениях принято с самого начала центрировать данные и нормировать каждую компоненту $x_i$ так, чтобы дисперсия вдоль каждого направления была единичной.
\erm

При вычислении с помощью метода главных компонент часто используют несколько другие конструкции. Для того, чтобы в этом разобраться посмотрим на матрицу  $X^{\top}X$  и её собственные числа.  

\dfn Пусть $A$ -- линейное отображение между евклидовыми пространствами $U \to V$. Тогда сингулярными значениями $A$ называются корни из положительных собственных чисел оператора $A^*A$.
\edfn




\section{SVD-разложение}

SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $V_k$, то получится матрица ранга $k$ или меньше. Эта матрица будет ближайшей к исходной в смысле нормы на пространстве матриц, называемой, нормой Фробениуса 
$$\|X\|_F=\sqrt{\sum_{i,j} x_{ij}^2}=\sqrt{\Tr X^{\top}X}.$$
Таким образом, нахождение проекций точек можно переформулировать, как нахождение ближайшей к $X$ матрицы ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение.

Прежде чем использовать SVD-разложение заметим, что в такой формулировке задача может быть поставлена в бескоординатном виде: для данного линейного отображения $L$ между евклидовыми пространствами найти наиболее близкое к нему линейное отображение ранга $k$ относительно нормы $\|L\|=\sqrt{\Tr L^*L}$.

\thrm Пусть $L$ -- линейное отображение между евклидовыми пространствами. Пусть $\Sigma$ -- матрица с сингулярными значениями $L$ на диагонали. Тогда ближайшее к $L$ отображения ранга $k$ имеет матрицу $\Sigma^{(k)}$ в базисах из правых и левых сингулярных векторов. Здесь  на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальное -- нули.
\proof Перейдём в базис из правых и левых сингулярных векторов. В этих базисах матрица $A$ -- это $\Sigma$.
Значит теперь мы решаем задачу нахождения ближайшей матрицы к матрице $\Sigma$. Для этого нам надо взять проекции строк $\Sigma$ на подпространство порождённое первыми $k$ собственными векторами $\Sigma^\top \Sigma$. После этого из этих проекций надо составить матрицу. Но первые $k$ собственных векторов $\Sigma^\top \Sigma$ это просто первые $k$ координатных векторов. Значит составленная из проекций матрица -- это $\Sigma^{(k)}$.
\endproof
\ethrm 

\crl Пусть $A$ имеет сингулярное разложение $P\Sigma Q$. Тогда ближайшая к $A$ матрица ранга $k$ (в смысле нормы Фробениуса) имеет вид $A^{(k)}=P\Sigma^{(k)}Q$, где на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальные элементы матрицы -- нули.
\ecrl

\upr На самом деле матрица $A^{(k)}$ ближайшая к $A$ среди матриц ранга $k$ и в смысле обычной матричной $l_2$-нормы. 
\eupr



\section{Немного вычислительной линейной алгебры}


Перед нами встаёт вопрос: как аккуратно вычислить те объекты линейной алгебры, которые мы определили. Какие для этого есть методы и чем они отличаются. Совершенно понятно, что это огромный круг вопросов и ответить на них на все невозможно. Поэтому я постараюсь обрисовать основные примеры и основные подходы к ответам.

Рассмотрим прежде всего задачу о решении системы линейных уравнений $Ax=b$ с вещественными коэффициентами. 

Во всех реальных приложениях вектор $b$, а часто и матрица $A$ даны не точно, а с некоторой погрешностью. Кроме того, при вычислениях с плавающей точкой мы создаём ещё больше погрешностей. Посмотрим  пример
$$ \pmat 1 & 1 \\ 1 & 1.0001 \epmat x = \pmat 2\\ 2.0001 \epmat$$
У этой системы есть точное решение $x=(1,1)$. Предположим, однако, что из-за ошибок округления вектор $b$ стал равен 
$$b_{new}=\pmat 2 \\ 2 \epmat$$
Решение для $b_{new}$ равно $x=(2,0)^\top$. Видно, что малые изменения коэффициентов системы могут значительно изменить её решение.

Попробуем разобраться, что происходит с общей системой при малых возмущениях её коэффициентов. Для начала разберёмся с погрешностями в $b$.

Обозначим погрешность в значениях свободного члена за $\Delta b$. То есть на самом деле нам дана система $Ay=b+\Delta b$. Самое лучшее, что мы можем сделать -- это точно решить эту новую систему. Насколько решение $y$ этой приближённой системы может отличаться от решения исходной? Пусть $y=x+\Delta x$. Тогда вычитая одно уравнение из другого получаем
$$A \Delta x= \Delta b.$$
Нас будет интересовать прежде всего не абсолютная, а относительная погрешность $\frac{\|\Delta x\|}{\|x\|}$. Оценим её
$$\frac{\|\Delta x\|}{\|x\|}=\frac{\|A^-1 \Delta b \|}{\|x\|}=\frac{\|A^{-1} \Delta b \|}{\|b\|} \frac{\|b\|}{\|x\|} \leq \|A\| \|A^{-1}\| \frac{\|\Delta b\|}{\|b\|}.$$
Заметим, что здесь мы пользовались определением нормы матрицы при помощи нормы на исходном пространстве, но сам вид нормы на исходном пространстве нам не важен.

Получается, что чтобы мы не делали, просто из-за изначальных погрешностей в данных мы не можем рассчитывать  на погрешность меньшую $\kappa(A)\frac{\|\Delta b\|}{\|b\|}$
(чуть позже мы увидим, что эта оценка точная). 

\dfn Число $\kappa(A)$ называется числом обусловленности матрицы $A$.
\edfn

Оказывается, что число обусловленности отвечает и за влияние погрешностей в коэффициентах матрицы $A$.

Действительно, если посмотреть на решение возмущённой системы $(A+\Delta A)(x+\Delta x)=b$, то вычитая точную систему получаем
$$\Delta x= -A^{-1}\Delta A (x+\Delta x)$$
Переходя к нормам получаем
$$\frac{\|\Delta x\|}{\|x+\Delta x\|}\leq \|A^{-1}\| \|A\| \frac{\|\Delta A\|}{\|A\|}$$

Как найти $\kappa(A)$? Ответ зависит от выбранной нормы. Нам проще всего понять, как найти $\kappa(A)$ в случае евклидовой нормы на $\mb R^n$. В этой ситуации $$\kappa(A)=\frac{\sigma_1}{\sigma_n}.$$

Покажем теперь, что оценка погрешности при помощи $\kappa(A)$ точная (для $\kappa(A)$, вычисленного при помощи евклидовой нормы). Для этого перейдём в базисы из сингулярных векторов. Тогда можно считать, что $A=\Sigma$ -- диагональна. Возьмём вектора $x,b,\Delta x$ и $\Delta b$ следующим образом:
$$x=\pmat 1 \\ 0\\ \vdots \\ 0 \epmat,\, b=Ax=\pmat \sigma_1 \\ 0\\ \vdots \\ 0 \epmat, \, \Delta b = \pmat 0 \\ \vdots \\ 0 \\ \eps \epmat, \, \Delta x=  \pmat 0 \\  \vdots \\ 0 \\ \eps/\sigma_n\epmat.$$
Для таких векторов оценка достигается.

Можно ли заранее оценить $\kappa(A)$? Можно. Но это не так просто и мы это обсуждать не будем. Замечу только, что главная сложность состоит в оценке $\|A^{-1}\|$.

\dfn
Говорят, что система $Ax=b$ хорошо обусловлена, если $\kappa(A)$ -- мало.
\edfn

Однако мы пока ничего не сказали про особенности методов, могут ли они существенно добавить к ошибкам? Оказывается, что могут. Рассмотрим систему 
$$\pmat 0.0001 & 1 \\ 1 & 1 \epmat x= \pmat 1 \\ 2 \epmat $$
Допустим мы храним только три значащих цифры. Применяя напрямую метод Гаусса последнее уравнение преобразуется к виду $-9999 x_2=-9998 $. Округление его точного решения есть $x_2=0.9999$ (если округлить до 4-го знака), откуда получаем $x_1=1$ (что тоже будет верным до 4-го знака). Но если мы используем округление для коэффициентов матрицы на первом шаге, то предыдущее равенство принимает вид $10000x_2 = 10000$. Получаем, что $x_2'=1$ и $x_1'=0$. Что значительно отличается от точного решения.

\rm Отметьте, что $\kappa_2(A)$ в этом примере всего лишь $2.61...$.
\erm

\dfn Метод называется устойчивым, если ошибки в ходе вычисления имеют порядок, сравнимый с ошибками от возможных погрешностей в исходных данных.
\edfn

Метод Гаусса не является устойчивым методом решения систем линейных уравнений. Можно его доработать, на каждом шаге переставляя строки матрицы так, чтобы каждый раз выбирать в качестве главного элемента  наибольший элемент в столбце. Однако это всё равно решает не все проблемы (см. Wilkinson).

В качестве альтернативы можно посмотреть различные методы, которые находят $QR$ разложение. Однако и $QR$ разложение может давать побочные эффекты.





Однако, есть другой класс методов, позволяющих довольно легко найти приближённое решение. Речь идёт про  итерационные методы.
Пусть нам дана система линейных уравнений вида $x=Ax+b$, где $A$ -- квадратная матрица. Заметим, что любая система может быть представлена в таком виде. 

Предположим в дальнейшем, что собственные числа $A$ по модулю меньше $1$. Возьмём начальный вектор $x_0$ и построим последовательность
$$x_i=Ax_{i-1}+b.$$
Покажем, что при указанных условиях, эта последовательность стремится к решению системы. Прежде всего заметим, что матрица $E-A$ обратима, так как $0$ не является её собственным числом, и, следовательно, система имеет единственное решение. Пусть это решение --- $x'$. Обозначим $h_i=x_i-x'$. Для $h_i$ получается соотношение:
$$x'+h_i=Ax'+Ah_{i-1}+b.$$
Откуда получаем, что $h_i=Ah_{i-1}$. Но тогда $h_n=A^n h_0$ стремятся к нулю при $n\to \infty$. Значит $x_n\to x'$.


Можно показать, что этот метод устойчив, если имеет место сходимость. Кроме того, заметим, что каждый шаг итерации вычисляется за $O(n^2)$ или даже меньше, если в матрице $A$ много нулей.

Есть более интересные вариации на тему метода итераций. Например, метод Гаусса-Зейделя. Оказывается, что совершенно не обязательно смотреть на представление системы именно в виде $x=Ax+b$. Пусть матрица $A$ есть сумма $A=L+D+U$. Представим систему $Ax=b$ в виде 
$$(L+D)x=-Ux+b$$
Если мы определим последовательность $x_i$ по правилу $(L+D)x_{i+1}=-Ux_i+b$, то за сходимость такой последовательности будут отвечать собственные числа матрицы $-(L+D)^{-1}U$. 

Метод Гаусса-Зейделя чаще сходится и скорость сходимости для него обычно более высокая, чем для простого метода итераций. Кроме того, так как матрица $(L+D)$ нижнетреугольная, то решение системы с такой матрицей требует $O(n^2)$ операций.







\subsection{Нахождение собственных чисел и векторов}

Хотелось бы уметь оценивать размер собственных чисел матрицы $A$. Можно ли это сделать? Оказывается, что можно. 


Пусть $A$ -- вещественная или комплексная квадратная матрица. Рассмотрим строки $A$. Зададим элементы $r_i=\sum_{j\neq i} |a_{ij}|$. Рассмотрим множество замкнутых кругов радиуса $r_i$ с центром в $a_{ii}$ на комплексной плоскости. Эти круги называются кругами Гершгорина. 

\thrm Все собственные числа матрицы $A$ лежат в объединении кругов Гершгорина. 
\ethrm
\proof Пусть $\lambda$ -- собственное число $A$, а $v$ -- соответствующий собственный вектор. Тогда $Av=\lambda v$. Посмотрим на максимальный по модулю элемент $v$ -- $v_i$ и соответсвующую ему строчку. Покажем, что $\lambda$ лежит в круге Гершгорина с номером $i$. Имеем $\sum a_{ij}v_j =\lambda v_i$  и, значит, $\sum_{j\neq i} a_{ij}v_j= (\lambda - a_{ii})v_i $. Имеем неравенство $$|\lambda -a_{ii}||v_i| \leq |v_i|\sum_{j\neq i} |a_{ij}|$$
Сокращая на $v_i$ получаем требуемое. 
\endproof

\rm В частности, это рассуждение даёт критерий невырожденности матрицы -- матрица $A$ невырождена, если сумма модулей её внедиагональных элементов каждой строки меньше модуля диагонального элемента. Такие матрицы называются матрицами с диагональным преобладанием
\erm

\rm
Переходя от матрицы к транспонированной можно получить ещё одну оценку, которая получается из рассмотрения столбцов $A$.
\erm

А можно ли точно сказать, в каком из кругов находится собственное число? Оказывается, что иногда можно. Для того чтобы это показать нам нужна некоторая аналитическая подготовка.

\fct[Корни непрерывно зависят от коэффициентов] Пусть $f(x,t)$ -- комплексный многочлен от двух переменных со старшим коэффициентом $1$ при $x^n$, как многочлена над $\mb C[t]$. Пусть $\lambda_1,\dots,\lambda_n$ -- корни $f(x,0)$, а $\mu_1,\dots, \mu_n$ -- корни $f(x,1)$. Тогда cуществуют такие непрерывные на $[0,1]$ функции $\lambda_i(t)$, что $f(\lambda_i(t),t)=0$ и $\lambda_i(0)=\lambda_i$, а $\lambda_i(1)=\mu_{\sigma(i)}$ (для некоторой перестановки $\sigma \in S_n$).
\efct

\thrm Если $k$ кругов Гершгорина не пересекаются с остальными, то в них лежат $k$ собственных чисел с учётом кратности.
\ethrm

Как это применить к оценки возмущения собственных чисел? Предположим, что $A$ диагонализуема:
$$A=C\pmat \lambda_1 && \\ & \ddots & \\ && \lambda_n \epmat C^{-1}=CDC^{-1}.$$
Рассмотрим сумму $A+\Delta A$. Как оценить собственные числа этой матрицы, через собственный числа $A$?

Пусть $\| \Delta A\|=\eps$. Рассмотрим матрицу
$$C^{-1}(A+\Delta A) C= D+C^{-1}\Delta A C $$
Надо оценить её собственные числа. Для этого заметим, что позиции  $i,j$ матрицы $C^{-1}\Delta A C$ стоит элемент размера не более $\| C^{-1}\| \|C\| \eps$. По теореме о кругах Гершгорина собственные числа $A+\Delta A$ находятся в кругах радиуса $n \| C^{-1}\| \|C\| \eps $ c центрами в $\lambda_i$. Кроме того, для достаточно маленького $\eps$ пары таких кругов не пересекаются или вложены друг в друга. Отсюда

\crl Для достаточно маленького возмущения $\Delta A$ в круге радиуса меньше $n \| C^{-1}\| \|C\| \|\Delta A\|$ с центром в с.ч. $\lambda$ находится ровно $k$ собственных чисел $A+\Delta A$, где $k$ -- это кратность $\lambda$ у $A$.
\ecrl

Отсюда видно, что  роль, аналогичную числу обусловленности, в оценке на собственные числа играет число $\|C\|\|C^{-1}\|$. На самом деле для каждого собственного числа можно ввести своё собственное число обусловленности. В любом случае видно, что оценить это число заранее не находя матрицу $C$ не получится. Тем не менее что-то сделать можно. Для произвольной системы это число уже можно оценить, если мы приближённо вычислили $C$ и $C^{-1}$. Однако, есть матрицы для которых это число и так наилучшее возможное. Что это за матрицы?


Раньше, для того, чтобы найти спектр оператора мы считали характеристический многочлен. Однако, его вычисление довольно трудоёмко. Кроме того, после этого необходимо найти ещё и корни характеристического многочлена.


Однако, есть ряд других методов нахождения собственных чисел. С первой идеей мы уже знакомы -- это вариация метода итераций: рассмотрим случайный вектор $x$. Посмотрим на последовательность $A^kx/\|A^kx\|$ $k\to \infty$. Она стремится к максимальному собственному вектору для почти любого $x$. Легко оценить и собственное число.

Если предположить симметричность матрицы $A$. Заметим, что  для почти любого $x$ 
$$\frac {\lan A^{k+1}x, A^k x \ran}{\lan A^k x, A^k x \ran} \to \lambda, $$
где $\lambda$ -- максимальное собственное число.

Есть ещё множество методов для нахождения собственных чисел. 

Есть и метод нахождения всех одновременно собственных чисел: $QR$-алгоритм. Он заключается в следующем: вначале возьмём $A_0=A$. На $k$-ом шаге для матрицы $A_{k-1}$ строится $QR$-разложение $A_{k-1}=Q_kR_k$, а затем строится матрица $A_k=R_kQ_k$. Оказывается, что этот процесс в случае симметричной $A$ сойдётся к диагональной матрице, чьи диагональные элементы  и есть собственные числа $A$. 

Почему этот алгоритм вообще работает и как до него догадаться.

\lm Имеет место соотношение $A_k=Q_k^\top A_{k-1} Q_k$.
\elm

В частности, $A_k=Q_k^\top \dots Q_1^\top A Q_1 \dots Q_k$.

\lm Определим $Q^{(k)}=Q_1\dots Q_k$ и $R^{(k)}= R_k \dots R_1$. Тогда 
$$A^k=Q^{(k)}R^{(k)}.$$
\elm
\proof $$A^k=A A^{k-1}= Q^{(k)}A_k{Q^{(k)}}^\top Q^{(k-1)}R^{(k-1)} = Q^{(k)}A_k Q_k^\top R^{(k-1)}=Q^{(k)} R^{(k)}$$
\endproof

\crl Пусть $A$ -- симметричная матрица с различными собственными числами. Тогда столбцы $Q_k$ стремятся к собственным векторам $A$.
\ecrl
\proof
\endproof



\section{Графы}
С графом $G$ связаны несколько разных матриц. Как свойства этих матриц, их собственные числа отражаются в комбинаторных свойствах графа $G$ и может ли это помочь?
Приведём пример, как знание спектра графа даёт некоторые оценки на сложно вычисляемые величины.

\rm Граф $G$ является $k$-регулярным тогда и только тогда, когда вектор $(1,\dots,1)^\top$ является собственным вектором с собственным числом $k$ для $A(G)$.
\erm

\thrm Пусть $G$ -- $k$-регулярный граф. Тогда размер максимального независимого множества в графе оценивается как
$$\alpha(G)\leq n\frac{-\lambda_n}{k-\lambda_n}.$$
\ethrm
\proof Рассмотрим характеристический вектор $v$ для независимого множества размера $\alpha$. Имеем $v^{\top}Av=0$ и при этом $v^{\top}v=\alpha$. Так как граф $k$-регулярный, то у него есть нормированный собственный вектор $u_1=\frac{1}{\sqrt{n}}(1,\dots,1)$. Тогда $\lan v,u_1\ran = \frac{\alpha}{\sqrt{n}}$. Разложим вектор $v=c_1u_1 + \dots + c_n u_n$ по ортонормированной системе собственных векторов. Тогда из условия независимости получаем
$$0=v^{\top}Av=\sum c_i^2 \lambda_i= \lambda_1\frac{\alpha^2}{n}+ \sum_{i\geq 2} \lambda_i c_i^2\geq \lambda_1\frac{\alpha^2}{n}+ \lambda_n \sum_{i\geq 2} c_i^2.$$
Мы знаем, что $\sum c_i^2=\alpha$, откуда $\sum_{i\geq 2} c_i^2=\alpha - \frac{\alpha^2}{n}$. Итого 
$$0\geq \lambda_1\frac{\alpha^2}{n}+\lambda_n(\alpha- \frac{\alpha^2}{n})$$
Сокращая на $\alpha$ получаем 
$$(\lambda_1-\lambda_n)\frac{\alpha}{n}\leq -\lambda_n,$$
что эквивалентно нужному неравенству.
\endproof




\exm\\
1) Полный граф $K_n$ имеет спектр $n-1,-1,\dots,-1$. В полном соответствии с тем, что размер максимального независимого множества равен $1$.\\
2) Граф Петерсена имеет спектр 3, 1, 1, 1, 1, 1, -2, -2, -2, -2. Размер независимого множества в точности равен $4$.\\
3) Оценка точна в общем виде для графов Кнезера $K(n,r)$.


Из оценки на размер независимого множества можно вывести оценку на хроматическое число.

\zd Покажите, что $\alpha(G) \chi(G) \geq n$. Выведите отсюда, что для регулярного графа $\chi(G)\geq 1+ \frac{k}{-\lambda_n}$. Покажите, что для графа Петерсена оценка точная.
\ezd

\rm Удивительно, но ровно та же оценка верна и для нерегулярных графов тоже.
\erm

\zd Покажите, что $\chi(G) \leq [\lambda_1]+1$ для любого графа $G$. (Адаптируйте классическое доказательство с максимальной степенью).
\ezd


\subsection{Дополниьельно: ещё трюки и применения}

\thrm Рассмотрим граф $K_{10}$. Тогда его невозможно покрыть тремя копиями графа Петерсена.
\ethrm
\proof Пусть граф $K_{10}$ покрыт тремя графами Петерсена. Тогда его матрица смежности удовлетворяет соотношению 
$$A(K_{10})=P_1+P_2+P_3.$$
Для нашего удобства перепишем это в виде 
$$-A(K_{10})=-P_1-P_2-P_3.$$
Заметим, что вектор $(1,\dots,1)^\top$ -- собственный для всех трёх матриц. Значит мы может ограничить указанное равенство на его ортогональное дополнение $U$.

Мы находимся в 9-тимерном пространстве. Воспользуемся неравенствами на собственные числа суммы (которые мы не доказывали). Тогда $i$-ое собственное число  суммы трёх матриц $A,B,C$ может быть оценено как $\lambda_t +\mu_s+ \nu_r$, где $r+s+t=i+2$. У операторов $-P_i|_U$ собственные числа равны $$2,2,2,2,-1,-1,-1,-1,-1.$$
Возьмём  $r=s=5$ и $t=1$. Получим оценку на $9$-ое собственное число $-A(K_{10})|_U$. Но мы его знаем это $1$. С другой стороны получается, что оно должно быть меньше $0$. Противоречие! 
\endproof

Кроме оценок на хроматическое число и покрываемость при помощи спектров можно доказать негамильтоновость графа, оценивать его рёберное хроматическое число переходя к рёбреному графу и т.д.

\zd Пусть $B$ -- матрица инцидентности графа $G$. Покажите, что $B^\top B - 2 E$ есть матрица смежности для рёберного графа $G$, а $B B^\top$ есть $A(G)+D$, где $D$ -- это матрица со степенями вершин графа $G$ на диагонали. 
\ezd

\zd Покажите, что в рёберном графе графа Петерсена нет индуцированного цикла длины 10 и выведите отсюда, что граф Петерсена негамильтонов.
\ezd

Есть, однако, две темы, связанные с собственными числами графов, которые приводят к непосредственным применениям. К сожалению, как часто бывает, у нас нет шансов внимательно посмотреть результаты этих разделов, поэтому мы ограничимся лишь очерчиванием их границ.




\dfn Для $A$ -- подмножества $V(G)$ определим $\partial A$ как множество всех рёбер ведущих из $A$ в вершины не из $A$. Константа Чигера или константа расширения графа  $G$ это 
$$h(G) := \min \left\{ \left. \frac{| \partial A |}{| A |} \right|   A \subseteq V(G), 0 < | A | \leq \tfrac{1}{2} | V(G)| \right\} .$$
\edfn

Вычислить константу Чигера очень сложно. Однако её можно оценить.

\begin{thmm}[Неравенство Чигера] Пусть $G$ -- $d$-регулярный граф и $\lambda_2$ -- его второе по максимальности собственное число. Тогда
$$\tfrac{1}{2}(d - \lambda_2) \le h(G) \le \sqrt{2d(d - \lambda_2)}.$$
\end{thmm}



\dfn Семейство графов $G_n$ называется $(n,d,\alpha)$ (алгебраическим) экспандером, если $G_n$ -- $d$-регулярный граф на $n$ вершинах с $\lambda=\max(|\lambda_2|,|\lambda_n|)\leq \alpha d$.
\edfn

Где могут применяться такие графы:\\
1) Коды, исправляющие ошибки\\
2) Псевдослучайные генераторы\\
3) Дерандомизация

Несмотря на то, что большая часть регулярных графов степени $d$ обладает указанными свойствами  до некоторого времени не было ни одного явного примера семейства экспандеров. Первый пример семейства экспандеров дал Григорий Маргулис. \\


\exm\\
1) Рассмотрим $\mb Z/n \times \mb Z/n$ и проведём для пары $(x,y)$ ребро в $(x \pm 2y,y), (x \pm (2y+1),y), (x,y \pm 2x), (x,y \pm (2x+1))$. Получается граф-экспандер с $\lambda \leq 5\sqrt{2}$\\
2) Можно подметить, что рёбра экспандеров из предыдущего примера связаны с некоторыми обратимыми преобразованиями, то есть с какой-то группой. Это общее место для конструкции большинства экспандеров.



\dfn Определим Лапласиан графа как $L(G)$ как $D-A(G)$.
\edfn

\zd $L(G)=B B^\top$, где $B$ -- матрица инцидентности произвольным образом ориентированного графа $G$.
\ezd

\zd $L$ -- неотрицательно определена и  $\rk L= \rk B=n-c$, где $c$ -- это количество компонент связности графа $G$.
\ezd



Кроме собственно графов можно рассматривать взвешенные графы. Понятно, как для них определить матрицы $D$, $A(G)=W(G)$ -- матрица весов, $L(G)$. Кроме этих матриц нам понадобится ещё и нормализованная матрица Лапласа.


\dfn Рассмотрим матрицу $D^{-1/2} L(G) D^{-1/2}$. Это нормализованный Лапласиан.
\edfn

На основе нормализованного Лапласиана можно построить алгоритм фрагментации изображений. Как и любая естественная задача она не имеет чёткой формулировки. В 2000 году Jianbo Shi и  Jitendra Malik \href{https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf}{предложили} следующую версию формулировки задачи: по картинке строится взвешенный граф $G$  -- его множество вершин $V$ -- это пиксели, рёбра проводятся, если вершины близки друг к другу и вес зависит от того, насколько пиксели похожи друг на друга.

Теперь рассматриваются все подмножества пикселей $A\subseteq V$ и минимизируется величина разреза, который отрезает $A$ от остального графа.
$$\min_{A} \frac{assoc(A,V\setminus A)}{assoc(A,V)}  + \frac{assoc(A, V\setminus A)}{assoc(V\setminus A,V)}, $$
$$\text{ где } \,\, assoc(A,B)=\sum_{i\in A, j\in B} w_{ij}$$.


Вопрос стоит в выборе оптимального $A$. Сопоставим множеству $A$ вектор $y$, такой что $$y_i = \begin{cases}-b=-\frac{\sum_{v\in A} d_v}{\sum_{v \not\in A} d_v}, \text{ если } i\not\in  A \\
1, i\in A 
\end{cases}$$

Оказывается, что в этом случае задача сводится к минимизации по всем таким $y$ выражения 
$$\frac {y^\top L(G) y} {y^\top D y } \text{ при условии } y D \bf 1 = 0.$$ 

Решить такую дискретную задачу трудно, поэтому авторы переходят к непрерывной, отбрасывая ограничения на значения $y_i$. Минимум такого выражения достигается на $D^{-1/2}v$, где $v$ -- собственный вектор нормализованного Лапласиана для минимального положительного собственного числа.

Теперь, беря $V$ можно разделить точки по признаку положительности и отрицательности (например).

\rm Похожий метод для матрицы Лапласа приводит к оценке связности графа и позволяет приближённо найти минимальный вершинный разрез графа дающий несколько компонент связности. (см. вектор Фидлера).
\erm






\section{Двойственное пространство}

Начнём наше триумфальное возвращение из мира вещественных чисел в мир произвольных полей. В дальнейшем $K$ обозначает произвольное поле. 

\dfn Определим двойственное пространство к пространству $V$ как $V^*=\Hom (V, K)$.
\edfn

\dfn Пусть $e_1,\dots,e_n$ -- базис $V$. Определим $e^1,\dots,e^n$ как базис пространства $V^*$, заданный следующим соотношением: 
$$e^j(e_i)=\delta_{ij}.$$
Иными словами, $e^i$ -- это $i$-ая координатная функция в базисе $e$.
\edfn

\utv Пусть $V$ -- векторное пространство над полем $K$. Тогда имеет место естественный изоморфизм пространств $V\to {V^*}^*$.
\eutv
\proof Построим отображение по правилу $v \to (f \to f(v))$. Это отображение инъективно. Из равенства размерностей следует, что этого и достаточно.
\endproof

Если мы перешли от одного базиса к другому в пространстве $V$, то что произошло в $V^*$? 

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из старого базиса в новый. Тогда матрица перехода из базиса $e^1,\dots,e^n$ в базис $\hat{e}^1,\dots,\hat{e}^n$ есть ${C^{\top}}^{-1}$.
\proof Зафиксируем конвенцию: матрица $C$ это такая матрица, что для всякого вектора $v$ со старыми координатами $x$ и новыми координатами $y$ выполнено, что $y=Cx$. В частности, это выполнено для вектора $e_i$. Это означает, что вектор $e_i=\sum_{j=1}^nC_{ji}\hat{e}_j$. Собственно, это ещё одна характеризация матрицы $C$. Наша задача найти матрицу $D$, со свойством $e^i=\sum D_{ji}\hat{e}^j$. По определению $e^i(e_k)=\delta_{ik}$. Подставим вместо $e^i$ и $e_k$ выражения с матрицами $C$ и $D$. Имеем
$$\delta_{ik}=\sum_{j}C_{ji} \sum_{l}D_{lk}\hat{e}^j(\hat{e}_l)=\sum_{j,l}C_{ji}D_{lk}\cdot\delta_{jl}=\sum_{j}C_{ji}D_{jk}$$
Теперь это равенство можно проинтерпретировать при помощи матричного произведения как 
$$E_n=C^{\top}D,$$
что и доказывает требуемое.
\endproof
\ethrm

В особые отношения со своим двойственным пространством вступают евклидовы и унитарные пространства. 

\utv Пусть $V$ -- евклидово или унитарное пространство. Тогда любой линейный функционал имеет вид $\lan v, \_ \ran$ для единственного вектора $v$. 
\eutv
\proof Пусть $f$ -- линейный функцилнал. Рассмотрим $U=\Ker f^\bot$. Это одномерное пространство. Рассмотрим $v\in U$ отличный от нуля и подгоним при помощи костанты так, чтобы  $f(v)=\lan v,v \ran$. Этого достаточно.  
\endproof

\section{Немного про квантовую механику}

Вкратце напомню парадигму квантовой механики. Физическая система в квантовой механике задаётся при помощи некоторого унитарного пространства $V$. Каждая прямая этого унитарного пространства задаёт некоторое возможное физическое состояние системы. На каждой прямой есть вектор, по норме равный $1$. Итого, каждый вектор, равный по норме $1$ описывает некоторое физическое состояние. 

Квадрат модуля скалярного произведения двух таких нормированных векторов  $|\lan u,v \ran|^2$ интерпретируется как вероятность того, что  $u$ при измерении будет находится в состоянии $v$ (если вы можете померить, что что-то находится именно в состоянии $v$). Таким образом, каждый вектор $u\in V$ задаёт линейный функционал $\lan u, \_ \ran $ на $V$, который по состоянию $v$ выдаёт информацию о вероятности найти $v$ в состоянии $u$.

Каждой измеримой физической величине соответствует самосопряжённый оператор $T$ на $V$. Пусть $\psi_i$ -- это ортонормированный базис из собственных векторов $T$ с собственными числами $\lambda_i$. Тогда вероятность, что находясь в состоянии $v$ при измерении величины $T$ мы получим $\lambda_i$ есть вероятность найти $v$ в состоянии $\psi$, то есть $|\lan \psi_i, v \ran|^2$. Несложно заметить, что, как и должно быть, сумма по всем возможным $i$ этих вероятностей даёт $1$. Такая конструкция объясняет квантованность (дискретность) значений разных физических величин, которая возникает при измерении.

Типичным примером квантовой системы является нерелятивистская (без учёта теории относительности) модель одной частицы в $\mb R^3$. В этом случае роль унитарного пространства играет 
$$V= L_2(\mb R^3)=\left\{  f\colon \mb R^3 \to \mb C\, \Big| \, f - \text{ измерима и } \int_{\mb R^3} |f|^2<\infty \right\}.$$
Технически тут ещё понадобится отождествить функции отличающиеся друг от друга на множестве меры ноль. Функция $f\in V$ (нормированная, т.е. $\|f\|^2=1$)  имеет следующую интерпретацию: $\int_{B} |f|^2$ есть вероятность для частицы оказаться в области $B$. 

 

В квантовых вычислениях основным объектом является квантовый бит или кубит (qubit) -- это двумерная квантовая система. Первый её базисный вектор обозначается $\left|0\ran$, а второй $ \left|1\ran$. Предполагается, что это собственные вектора оператора энергии т.е. два эти состояния различаются при измерении энергии. Произвольное состояние (нормированное) есть смесь 
$$c_1 \left|0\ran+ c_2 \left| 1\ran, \text{ где } |c_1|^2+|c_2|^2=1.$$
 

Допустим, у нас есть две независимые системы с пространствами $V_1$ и $V_2$. Какое пространство соответствует  объединению двух таких систем? Обозначим это пространство за $V_1\otimes V_2$. Посмотрим как должны выглядеть состояния этого пространства. Понятно что для каждой пары состояний $u_1\in V_1$ и $u_2\in V_2$ должно существовать своё состояние объединённой системы. Обозначим его за $u_1 \otimes u_2$.

Пусть есть одна пара $(u_1,u_2)$  и вторая пара $(v_1,v_2)$. Какова вероятность того, что при измерении система $u_1\otimes u_2)$ будет в состоянии $v_1 \otimes v_2$? Логично, чтобы это было $|\lan u_1,v_1 \ran \lan u_2, v_2\ran|^2$.

Какому скалярному произведению на таких парах это соответствует? Должно быть $\lan u_1\otimes u_2, v_1\otimes v_2\ran = \lan u_1,v_1 \ran \lan u_2, v_2\ran$. То есть, скалярное произведение с состоянием $(u_1,u_2)$ есть не линейная функция от пары $(v_1, v_2)$, а билинейная!



Линейные отображения можно складывать между собой. Сумме таких линейных отображений на $V_1\otimes V_2$ должна соответствовать сумма билинейных форм. Заметим, что любую билинейную форму можно представить в виде суммы форм заданных при помощи пар $(u_1,u_2)$. Таким образом, линейные функционалы на $V_1\otimes V_2$ должны соответствовать билинейным формам на $V_1\times V_2$.





\section{Тензорное произведение}

Сейчас мы немного поговорили про пространство линейных функционалов, прошлом семестре мы подробно остановились на билинейных операциях. А квантовая механика дала нам необходимость в следующем определении:

\dfn Пусть дана пара пространств $U,V$ над полем $K$. Тогда их тензорным произведением называется пространство 
$U\otimes V$ вместе с билинейным отображением отображением
$$i \colon U \times  V \to U \otimes V,$$
удовлетворяющее условию что для любого билинейного отображения из $h\colon U \times V \to W$ существует единственное линейное отображение 
$$\hat{h}\colon U\otimes V \to W,$$
что 
$$\hat{h}\circ i=h.$$
Иными словами, отображение $i$ -- это <<универсальное>> билинейное отображение.
\edfn 


\lm Пусть $U\leq V$, а $\pi \colon V \to V/U$ -- это каноническая проекция на $V/U$. Пусть $L\colon V \to W$ -- линейное отображение. Тогда, для того, чтобы существовало $\hat{L}\colon V/U \to W$, что $L=\hat{L}\circ \pi$ необходимо и достаточно, чтобы $L(U)=\{0\}$.
\elm





\thrm Пусть $U,V$ -- пара векторных пространств. Тогда имеет место следующая конструкция тензорного произведения:
$$U \otimes V \cong K\lan U \times V \ran / Rel,$$
где $Rel$ -- это подпространство порождённое формальными суммами
$$(\lambda u_1+u_2, v) - \lambda (u_1, v) - ( u_2,v) \text{ и } (u,\lambda v_1+v_2) - \lambda (u,v_1) - (u,v_2).$$ 
\proof Для удобства обозначим пространство
$$T=K\lan U \times V \ran / Rel.$$ Будем обозначать образы элементов $(u,v)$ в $T$ как  $u\otimes v$. Отображение $$i \colon U\times V \to T$$
отображающее 
$$(u,v) \to u \otimes v$$
билинейно по самому определению соотношений из $Rel$. Пусть теперь дано пространство $W$ и билинейное отображение $$h \colon U \times V \to W.$$
Построим отображение $\hat{h}$ следующим образом: сначала определим $\hat{\hat{h}}\colon K\lan U \times V \ran \to W$, а затем покажем, что оно пропускается через $T$. По самому своему определению $K\lan U \times V\ran$ имеет базисом элементы $(u,v)$. Отображение $\hat{\hat{h}}$ достаточно задать на них. Положим $$\hat{\hat{h}}((u,v))=h(u,v).$$
Покажем, что оно однозначно пропускается через $T$. Как всегда единственность очевидна. Для того чтобы показать, что $\hat{\hat{h}}$ пропускается через $T$ необходимо показать, что все соотношения лежат в ядре $\hat{\hat{h}}$. Но это так, потому что $h$ билинейно! 
\endproof
\ethrm




\end{document}
