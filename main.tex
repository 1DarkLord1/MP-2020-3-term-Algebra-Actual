\documentclass[10pt,a4paper,oneside]{book}
\usepackage[a4paper,includeheadfoot,top=10mm,bottom=10mm,left=10mm,right=10mm]{geometry}
\usepackage{cmap} % Поддержка поиска русских слов в PDF (pdflatex)
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{fancyhdr} % колонтитулы
\usepackage{hyperref} % гиперссылки
\usepackage{comment} % многострочное комментирование 
%\usepackage{pdfsync} % синхронизация


\frenchspacing % Пробелы после конца предложения
\righthyphenmin=2 % минимальная длина фрагмента при переносе слова
\usepackage{misccorr} % российская полиграфия
\usepackage{indentfirst}% Красная строка в первом абзаце
\usepackage{ccaption} % Заголовки таблиц и рисунков
\captiondelim{. } % точки в подписи рисунка




\usepackage{rotating} % Поворот текста
\usepackage{graphicx} % Вставка изображений
\graphicspath{ {./} } % относительно main.tex
\usepackage{xcolor} % настройка цвета
\usepackage{pgf}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}

\usepackage{amsmath,amsthm,amssymb,amscd,array}
\usepackage{latexsym}
\usepackage{stmaryrd} % Для знака нормальной подгруппы
\usepackage{arydshln} % штрихованные линии в массивах
\usepackage{mathtools} % выравнивание в матрицах
\usepackage{multirow} % слияние в столбце
\usepackage{multicol} % нумерация в нескольких колонках


\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={red!80!black}
} % цвета для ссылок




\newtheorem{uprz}{\color{violet!100!black} Упражнение}
\newtheorem{predl}{\color{blue!50!black} Предложение}
\newtheorem{komment}{\color{green!50!blue} Комментарий}
\newtheorem{conj}{Гипотеза}
\newtheorem{notation}{\color{yellow!30!red} Обозначение}


\theoremstyle{definition}
\newtheorem{kit}{Кит}
\newtheorem*{rem}{\color{green!50!blue}Замечание}
\newtheorem{zad}{\color{violet!100!black}Задача}
\newtheorem*{defn}{\color{yellow!30!red} Определение}
\newtheorem*{fact}{Факт}
\newtheorem{thm}{\color{red!40!black}Теорема}
\newtheorem*{thmm}{\color{red!40!black} Теорема}
\newtheorem{lem}{\color{green!50!black}Лемма}
\newtheorem{cor}{\color{green!45!black}Следствие}
\newtheorem{utvr}{\color{blue!50!black}Утверждение}


\newcommand\tikznode[3][]%
   {\tikz[remember picture,baseline=(#2.base)]
      \node[minimum size=0pt,outer sep=0pt,#1](#2){#3};%
   }
\tikzset{>=stealth}


\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother


\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\proofname}{Доказательство}
\renewcommand{\mod}{\,\operatorname{mod}\,}
\renewcommand{\Re}{\operatorname{Re}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ovl}{\overline}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\K}{\operatorname{K_0}}
\newcommand{\witt}{\operatorname{W}}
\newcommand{\gw}{\operatorname{GW}}
\newcommand{\coh}{\operatorname{H}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\cl}{\operatorname{Cl}}
\newcommand{\Vol}{\operatorname{Vol}}
\newcommand\tgg{\mathop{\rm tg}\nolimits}
\newcommand\ccup{\mathop{\cup}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\chr}{\operatorname{char}}
\newcommand{\rk}{\operatorname{rk}}
\DeclareMathOperator{\Coker}{Coker}
\DeclareMathOperator{\Ker}{Ker}
\newcommand{\im}{\operatorname{Im}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\re}{\operatorname{Re}}
\newcommand{\tr}{\operatorname{Tr}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\orb}{\operatorname{\mathcal O}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\Out}{\operatorname{Out}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\SO}{\operatorname{SO}}
\renewcommand{\O}{\operatorname{O}}
\renewcommand{\U}{\operatorname{U}}
\newcommand{\Sym}{\operatorname{Sym}}
\newcommand{\Adj}{\operatorname{Adj}}
\newcommand{\Disc}{\operatorname{Disc}}
\newcommand{\cnt}{\operatorname{cont}}
\newcommand{\Frob}{\operatorname{Frob}}
\newcommand{\Iso}{\operatorname{Iso}}
\newcommand{\Isom}{\operatorname{Isom}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\di}{\mathop{\,\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\ndi}{\mathop{\not\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\nequiv}{\not \equiv}
\newcommand{\Nod}{\operatorname{\text{НОД}}}
\newcommand{\Nok}{\operatorname{\text{НОК}}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\Aff}{\operatorname{Aff}}
\newcommand{\AGL}{\operatorname{AGL}}
\newcommand{\PSL}{\operatorname{PSL}}
\newcommand{\Volume}{\operatorname{Volume}}


\def\llq{\textquotedblleft} 
\def\rrq{\textquotedblright} 
\def\exm{\noindent {\bf Примеры:}}
\def\Cb{\ovl{C}}
\def\ffi{\varphi}
\def\pa{\partial}
\def\V{\bf V}
\def\La{\Lambda}
\def\eps{\varepsilon}
\def\del{\delta}
\def\Del{\Delta}
\def\A{\EuScript{A}}
\def\lan{\left\langle }
\def\ran{\right\rangle}
\def\bar{\begin{array}}
\def\ear{\end{array}}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\thrm{\begin{thm}}
\def\ethrm{\end{thm}}
\def\dfn{\begin{defn}}
\def\edfn{\end{defn}}
\def\lm{\begin{lem}}
\def\elm{\end{lem}}
\def\zd{\begin{zad}}
\def\ezd{\end{zad}}
\def\prdl{\begin{predl}}
\def\eprdl{\end{predl}}
\def\crl{\begin{cor}}
\def\ecrl{\end{cor}}
\def\rm{\begin{rem}}
\def\erm{\end{rem}}
\def\fct{\begin{fact}}
\def\efct{\end{fact}}
\def\enm{\begin{enumerate}}
\def\eenm{\end{enumerate}}
\def\pmat{\begin{pmatrix}}
\def\epmat{\end{pmatrix}}
\def\utv{\begin{utvr}}
\def\eutv{\end{utvr}}
\def\upr{\begin{uprz}}
\def\eupr{\end{uprz}}
\def\nrml{\trianglelefteqslant}




\title{Современное программирование \\ 
Конспект по алгебре, 3 семестр}
\date{}


\begin{document}

\tableofcontents

\chapter{Линейная алгебра}

\section{Кватернионы}


Наша цель сейчас рассказать про геометрию трехмерного пространства используя при этом определённые алгебраические конструкции. А именно, ещё в XIX веке Уильям Роуэн Гамильтон стал искать аналогичную комплексным числам алгебраическую систему на трёхмерном пространстве.  
Однако, подходящий аналог удалось найти только в четырёхмерной ситуации.


Рассмотрим вещественное подпространство в алгебре матриц $M_2(\mb C)$ вида
$$\mb H = \left\{\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha} \epmat \right\}.$$
Базис этого пространства, как вещественного векторного пространства, состоит из матриц 
$$ 1=\pmat 1 & 0 \\ 0& 1 \epmat, i= \pmat i & 0 \\ 0& -i \epmat, j=\pmat 0& 1 \\ -1 & 0 \epmat, k=\pmat 0 & i \\ i & 0\epmat. $$ 
Покажем, что это вещественная подалгебра в $M_2(\mb C)$ и следовательно ассоциативное кольцо. 
Для этого достаточно показать, что произведение базисных снова лежит в $\mb H$. Имеем $$i^2=j^2=k^2=-1 \text{ и } ij=k=-ji,$$ откуда $$ik= iij=-j=jii=-ki \text{ и } jk=-jji=i=-kj.$$ Таким образом $\mb H$ образует ассоциативную алгебру размерности 4 над $\mb R$.
 
\dfn[Алгебра кватернионов] $\mb H$ называется алгеброй кватернионов. 
\edfn
Мы больше не будем думать (кроме одного нюанса) про кватернионы как про матрицы, а будем записывать их через $i,j,k$. Именно так обычно кватернионы и вводят -- как формальный суммы $a+bi+cj+dk$, для произведения которых выполнены тождества $i^2=j^2=k^2=-1$ и $ij=-ji=k$. Посмотрев на кватернионы как на матрицы мы сэкономили на доказательстве ассоциативности умножения.

\zd Алгебра кватернионов не снабжается структурой $\mb C$-алгебры.
\ezd






\dfn[Векторная и скалярная часть, сопряжённый кватернион] Пусть $x= a+bi+cj+dk$ кватернион. Определим вещественную или скалярную часть $\Re x=a$ и векторную часть $v= bi+cj+dk$ кватерниона. Сопряжённым кватернионом называется $\ovl{x}= a-bi-cj-dk= \Re x - \Im x =a-v$. 
\edfn



Посмотрим, как перемножаются кватернионы. Если оба кватерниона  $x$, $y$ разделить на скалярную и векторную части $x=a+v$, $y=b+u$ то $xy=ab+au+bv+ vu$. Нам осталось разобраться с умножением векторных частей. 

Рассмотрим произведение двух чисто мнимых кватернионов $uv=-\lan u,v\ran+[u,v]$. Его вещественная часть совпадает с минус скалярным произведением векторов. Про мнимую часть мы поговорим отдельно.

\dfn[Векторное произведение] Пусть $u,v \in \mb R^3$ два вектора. Тогда их векторным произведением называется вектор $[u,v]$.
\edfn

Если расписать в координатах $u=x_1i+x_2j+x_3k$ и  $v=y_1i+y_2j+y_3k$, то векторное произведение задаётся формулой

$$[u,v]= (x_2y_3-x_3y_2)i + (x_3y_1-x_1y_3)j + (x_1y_2- x_2y_1)k= \begin{vmatrix} i& j&k \\ x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \end{vmatrix} $$

\rm Операция $(u,v) \to [u,v]$ является билинейной и антисимметричной, то есть $[u,u]=0$ и, следовательно, $[u,v]=-[v,u]$.
\erm

Последнее замечание позволяет нам легко вычислить $x \ovl{x}= a^2+ \lan v,v \ran$. Это приводит нас к определению:

\dfn[Норма кватерниона] Определим норму кватерниона как $$\|x\|=\sqrt{x\ovl{x}}=\sqrt{ a^2+b^2+c^2+d^2}=\sqrt{\ovl{x}x}.$$
\edfn 


Норма кватерниона, как и модуль комплексного числа всегда положительны для ненулевых элементов. Это позволяет заметить, что

\dfn[Обратный кватернион] Если $0\neq x \in \mb H$, то $x^{-1}=\frac{\ovl{x}}{\|x\|^2}$. 
\edfn

Таким образом мы получили первый (и для нас единственный) пример некоммутативного кольца с делением. Такие кольца называются телами. Напоминаю, что алгебра для нас ассоциативна и с единицей. Неассоциативные алгебры представляют интерес. Например, можно взять $\mb R^3$, где в качестве умножения взято векторное произведение. Это пример неассоциативной алгебры или, точнее, алгебры Ли. В этом курсе мы не  обсуждаем неассоциативные алгебры в связи с тем, что им обычно находится применение либо внутри физических дисциплин, либо внутри самой математики и редко где ещё. 

Какие ещё свойства есть у отображения нормы? Если следовать параллели с комплексными числами, то стоит посмотреть, что происходит с нормой произведения. Для того, чтобы не обременяться вычислениями сделаем небольшой трюк и на секунду вспомним матричное представление кватернионов. Заметим, что на матричном языке, норма -- это $\|x\|=\sqrt{\det x}$, откуда получаем

\lm[Норма мультипликативна] $\|xy\|=\|x\|\|y\|$. В частности, $\|x^{-1}\|=\|x\|^{-1}$.
\proof Вспомним в последний раз, что кватернионы задаются матрицами из $M_2(\mb C)$. Пусть 
$$x=\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat.$$
Тогда $$\|x\|^2=|\alpha|^2+|\beta|^2=\det \pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat,$$
а определитель мультипликативен.
\endproof
\elm



Продолжим. Используя мультипликативность нормы легко доказать 
\lm Отображение $x \to \ovl{x}$ является антиизоморфизмом алгебр, то есть $\ovl{ab}=\ovl{b}\ovl{a}$.
\proof Линейной ясна. Пусть $x,y \neq 0$. Тогда $$\frac{\ovl{y}\,\ovl{x}}{\|y\|^2\|x\|^2}=y^{-1}x^{-1}=(xy)^{-1}=\frac{\ovl{xy}}{\|xy\|^2}.$$
\elm

На самом деле и здесь можно было воспользоваться матричным представлением. А именно, можно заметить, что операция сопряжения кватернионов совпадает на этом языке с транспонированием и сопряжением соответствующей комплексной матрицы. Вернёмся теперь к векторному произведению.





\lm[Свойства векторного произведения] Верны следующие свойства\\
1) Для любых $u,v \in \mb R^3$ верно $u\bot [u,v]$. Точнее $$u[u,v]= -\|u\|^2v+ \lan u,v\ran u$$
2) $\| [u,v]\|= \|u\|\|v\| \cdot |\sin \ffi |$, где $\ffi$ --  это угол между $u$ и $v$.
\elm
\proof Для того, чтобы посчитать скалярное произведение $\lan u, [u,v]\ran$ необходимо посчитать скалярную часть $u[u,v]$. 
$$u[u,v]= u (uv+ \lan u,v \ran)= u^2 v+ \lan u,v \ran u= -\|u\|^2 v+ \lan u,v \ran u$$
Последнее выражение, очевидно, чисто векторное.
Теперь
\begin{align*}
&\|[u,v]\|^2= -[u,v][u,v]= (uv + \lan u,v\ran)(vu + \lan u,v\ran)=\\
&=\|u\|^2\|v\|^2+ \lan u,v\ran^2 + \lan u,v\ran (uv+vu)=\|u\|^2\|v\|^2 - \lan u,v\ran^2= \|u\|^2\|v\|^2(1-\cos^2 \ffi)
\end{align*}
\endproof

\dfn Обозначим за $\mb H_{1}$ группу кватернионов, по норме равных единице.
\edfn

\thrm Отображение $\mb H_{1}\to \GL_3(\mb R)$ заданное по правилу $x\to (y \to xyx^{-1})$ корректно определено и даёт сюръективный  гомоморфизм из группы кватернионов единичной нормы в $\SO_3(\mb R)$. Ядро этого гомоморфизма состоит из $\{\pm 1\}$. Точнее, если единичный кватернион $x$  представим в виде $x=a+bv$, то соответствующее ему вращение есть вращение относительно  оси $\lan v \ran$ на угол $2\ffi$, где $\cos \ffi= a$, $\sin \ffi= b$ или тождественное преобразование в случае $v=\pm 1$.
\ethrm
\proof Рассмотрим преобразование $L_x \colon \mb H \to \mb H$ вида $y \to xyx^{-1}$. Прежде всего покажем, что мы получили ортогональное преобразование $\mb R^4$. Имеем
 $$\|xvx^{-1}\|=\|x\| \|v\| \|x^{-1}\| = \|v\|.$$
Теперь заметим, что преобразование $L_x$ сохраняет на месте вектор 1 и, следовательно, его ортогональное дополнение, то есть $\mb R^3$. Таким образом $L_x$ ограничивается на $\mb R^3$. Далее, очевидно, $L_xL_y= L_{xy}$. Осталось посчитать ядро гомоморфизма и явный вид отображения $L_x$. Заметим, что если $x=a+bv$, то $L_x$ оставляет $v$ на месте. Действительно, при $b\neq 0$ 
$$xbvx^{-1}=x(x-a)x^{-1}= x-a=bv.$$
Вычислим угол поворота. Для этого рассмотрим нормированный вектор  $u\bot v$ и $[u,v]$, которые образуют ортонормированный базис дополнения и посчитаем $xux^{-1}$ и $x[u,v]x^{-1}$. Из условия ортогональности следует, что $uv=[u,v]=-[v,u]=-vu$ и значит $v[u,v]=-[u,v]v=-uv^2=u$. Теперь
\begin{align*}
xux^{-1}&=(a+bv)u(a-bv)= (a+bv)(au-buv)=\\
&=a^2u -ab[u,v]+ab[v,u]- b^2vuv=a^2u-2ab[u,v]-b^2\|v\|^2u=\\ &=(a^2-b^2)u-2ab[u,v]=\cos2\ffi u+ \sin 2\ffi [u,v]
\\
\\
x[u,v]x^{-1}&=(a+bv)[u,v](a-bv)= (a[u,v]+bu)(a-bv)=\\
&=a^2[u,v]+abu-ab[u,v]v-b^2uv=\\
&=(a^2-b^2)[u,v]+2abu=\cos 2\ffi[u,v]+\sin 2\ffi u
\end{align*}

Осталось показать, что только $x=\pm 1$ лежит в ядре этого отображения. Это возможно только тогда, когда $2\ffi \equiv 0 \mod 2\pi$. Значит $\ffi=2 \pi$ или $\ffi= \pi$. Первое соответствует $x=1$. Втораое --  $a=\cos \ffi = -1$, а $b=\sin \ffi = 0$, то есть $x=-1$. 
\endproof


\zd
Покажите, что отображение $(x,y) \to (z \to xzy^{-1})$ задаёт сюръективный гомоморфизм из декартового квадрата группы единичных кватернионов в группу $\SO_4(\mb R)$ с ядром $\{(1,1),(-1,-1)\}$.
\ezd

Обсудим теперь для чего могут понадобится кватернионы. Каждый кватернион, как мы установили кодирует вращение трёхмерного пространства или, что тоже самое -- новую декартову систему координат в $\mb R^3$ с центром в нуле. Такой тип данных встречается в компьютерной графике, если вы хотите зафиксировать ракурс, в котором вы смотрите на 3d-сцену или положение какого-то конкретного объекта в такой сцене.

В такой задаче кватернионы сложно превзойти. Действительно, задание сцены при помощи кватернионов очень экономно -- 4 коэффициента с одним соотношением на них (3 коэффициента, если очень нужно) против 9 коэффициентов у ортогональной матрицы.

А что с эффективностью операций? Тут вопрос состоит в том, про какие операции идёт речь. Если вы хотите сказать, что тот или иной вектор, который был повёрнут на кватернион $q=a+v$ надо повернуть ещё на кватернион $p=b+u$, то вам надо всего лишь вычислить $pq=ab+au+bv+uv$. Такое произведение считается за 16 умножений и 12 сложений. Если брать произведение матриц $3\times 3$, то там получается 27 умножений и 18 сложений (можно обойтись и 23 умножениями, но сильно увеличив число сложений).

Можно конечно использовать углы Эйлера, но тогда придётся использовать для вычислений косинусы и синусы этих углов, которые сами даются не бесплатно.

Если вам даны два ракурса в виде кватернионов, то легко понять, на какой кватернион надо домножить, чтобы из первого получить второй.

Но что если вы хотите просто повернуть при помощи кватерниона какой-то вектор из $\mb R^3$. Тут ситуация несколько хуже. Для этого вам необходимо посчитать $qxq^{-1}$. Предположив, что $q$ нормирован можно заменить обращение на сопряжение. Тем не менее такой подход довольно дорог -- 32 умножения и 24 сложения. Конечно, такой способ не оптимален. Например, не обязательно считать скалярную часть -- она должна стать нулевой. На самом деле, если $q=a+v$, то 
$$qxq^{-1}=x+ 2[v, [v,x]+ ax]$$
что даёт 15 умножений и 15 сложений (если считать умножение на 2 сложением). Это конечно отличается от матриц, где необходимо 9 умножений и 6 сложений.

Впрочем, отличается не сильно. Кроме того у кватернионного представления есть плюс произведение нескольких кватернионов -- это кватернион несмотря на ошибки округления. Что не так для ортогональных матриц.

Наконец, представим себе задачу, что нам нужно плавно перейти от ракурса $q$ к ракурсу $p$. Желательно с "равномерной" скоростью. На языке кватернионов это становится понятно. Для этого заметим, что единичные кватернионы -- это всего лишь точки на трёхмерной сфере. Несложно понять, что их соединяет часть дуги сферы, точки на которой заданы как 
$$ \frac{\sin(t\theta)p + \sin ((1-t)\theta) q}{\sin \theta},$$
где $\theta$ -- угол между $p$ и $q$ (острый, не забываем, что представление кватернионами немного не однозначно).





\section{Задачи на максимизацию}

Теперь обратимся к вопросам, связанным с вещественными самосопряжёнными операторами.

Рассмотрим один из вопросов, связанных с такой конструкцией, а именно, рассмотрим задачу о нахождении нормы линейного оператора $L \colon U \to V$ между двумя евклидовыми пространствами. Для того, чтобы найти  норму необходимо найти $$\max_{x\neq 0}\sqrt{\frac{\lan Lx,Lx\ran}{\|x\|^2}}=\sqrt{\max_{x\neq 0}\frac{\lan L^*Lx,x\ran}{\|x\|^2}}=\sqrt{\max_{\|x\|=1} \lan L^*Lx,x\ran}.$$
Таким образом, нахождение нормы оператора свелось к задаче максимизации квадратичной формы на единичной сфере. Заметим, что максимум действительно достигается благодаря компактности сферы.

Оказывается, что довольно легко найти максимум или минимум квадратичной формы на сфере.



\thrm Пусть $V$ -- евклидово пространство, $A$ -- самосопряжённый оператор на $V$, а $q(x)=\lan x,Ax\ran$ -- соответствующая квадратичная форма. Тогда 
$$\max_{ x\in V } \frac{q(x)}{||x||^2}=\max_{\substack{ x\in V \\ ||x||=1}} q(x)=\lambda_1,$$
 где $\lambda_1$ - наибольшее собственное число оператора $A$ и достигается на собственном векторе $v_1$, соответствующему $\lambda_1$. Аналогично минимум равен минимальному собственному числу $A$. 
\proof
Пусть $v=\sum c_i e_i$, причём $1=||v||^2=\sum c_i^2$. Тогда $\lan Av,v\ran = \sum c^2_i \lambda_i $, что меньше $\lan A e_1,e_1\ran= \lambda_1= \sum \lambda_1 c_i^2$.
\endproof
\ethrm

Эта теорема, кроме, собственно, решения задачи, даёт геометрическую характеризацию первого собственного числа. Вопрос: можно ли аналогично охарактеризовать другие собственные числа? Ответ получается не таким простым, но, тем не менее, полезным.

\thrm[Куранта-Фишера] Пусть $q(x)=\lan x, Ax\ran$. Тогда $k$-ое по убыванию собственное число $\lambda_k$ для $A$ есть 
$$ \lambda_k=\max_{\dim L=k} \min_{\substack{ x\in L \\ ||x||=1}} q(x) = \min_{\dim L=n-k+1} \max_{\substack{ x\in L \\ ||x||=1}} q(x).$$
Причем максимум достигается на инвариантном подпространстве, содержащем собственные вектора для $\lambda_1,\dots,\lambda_k$.
\ethrm
\proof Пусть $U$ --- подпространство на котором достигается максимум, причём допустим, что максимум больше $\lambda_k$. Тогда рассмотрим подпространство $W=\lan v_k,\dots,v_n\ran$, где $v_i$ --- собственный вектор соответствующий $i$-ому по убыванию собственному числу. Заметим, что $U\cap W=\{0\}$, так как на $W$ форма принимает значения меньше или равные $\lambda_k$, а на $U$ -- строго большие. Однако $\dim W=n-k+1$. Приходим к противоречию с подсчётом размерности пересечения. 
\endproof




\crl Пусть $U$ некоторое подпространство, а $q(x)=x^{\top} Ax$. Пусть собственные числа $A$ --- это $\lambda_i$, а собственные числа оператора, соответствующего $q(x)|_U$ -- это $\mu_i$ упорядоченные по убыванию. Тогда 
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$  
\proof Заметим, что
 $$\mu_i=\max_{\substack{L\leq U\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x),$$
что очевидно меньше, чем 
$$\max_{\substack{L\leq V\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x)=\lambda_i.$$ Неравенство в другую сторону получается из второго описания в теореме Куранта-Фишера.
\endproof
\ecrl


Введём не совсем стандартное определение:
\dfn
Пусть $q$ -- квадратичная форма на евклидовом пространстве. Рассмотрим ортонормированный базис $u_i$ пространства $V$. Тогда положим 
$$\Tr q= \sum q(u_i).$$ Если в базисе $u_i$ форме $q(x)=x^{\top} Ax $ соответствует симметричная матрица $A$, то $\Tr q(x)=\Tr(A)$.
\edfn

\rm Определение не зависит от выбора ортонормированного базиса. Действительно, если замена координат ортогональна, то матрица $q$ в новой системе координат имеет вид $C^{\top}AC=C^{-1}AC$. Осталось заметить, что след последней матрицы очевидно равен следу $A$.
\erm

\rm Если форме $q$ соответствует самосопряжённый оператор $A$, то $\Tr q=\sum \lambda_i$, где $\lambda_i$ -- собственные числа $A$.
\erm




\crl Пусть $q(x)=x^{\top} Ax$, $U$ --- некоторое подпространство, $\dim U=k$. Пусть собственные числа $A$ --- это $\lambda_i$. Тогда $$\Tr q|_U\leq \sum_{i=1}^k \lambda_i= \Tr q|_{V_k},$$
где $V_k$ подпространство натянутое на первые $k$ собственных векторов $q$.
\proof Нам известны неравенства на собственные числа $\mu_i$, ограничения $q|_U$. А именно, $\mu_i\leq \lambda_i$. Но тогда и $$\sum_{i=1}^k \mu_i \leq \sum_{i=1}^k \lambda_i.$$
\endproof
\ecrl 






\section{Метод главных компонент}

Рассмотрим следующую задачу: имеется массив данных --- набор векторов $x_1,\dots,x_s \in \mb R^n$. Подразумевается, что точек очень много. Предположим, что при идеальных измерениях между координатами этих векторов есть линейные зависимости. Все отклонения от этой погрешности вызваны небольшими погрешностями. Задача состоит в том, чтобы восстановить линейную зависимость. 


Переформулируем задачу геометрически. Пусть наши вектора удовлетворяют линейным условиям $Ax_i=b$ для некоторой матрицы $A$ ранга $n-k$. Это значит, что они лежат в аффинном подпространстве $\Ker A + a_0$ размерности $k$. Если же нам даны вектора с погрешностями, то задачу таким образом можно поставить в виде: найти аффинное подпространство размерности $k$ наиболее близкое к данным точкам $x_1,\dots,x_s$. Понятие <<наиболее близкое>> требует конкретизации. Вообще говоря, тут есть выбор. Мы будем считать, что подходящее пространство $W=U+a_0$ должно давать минимум следующего выражения:
$$\sqrt{\sum_{i=1}^s \rho(x_i,W)^2} \to \min.$$
Совершенно ясно, что корень квадратный тут для красоты, и минимизировать нужно $\sum_{i=1}^s \rho(x_i,W)^2$.

Прежде всего установим, что какой бы <<минимайзер>> $W=U+a_0$ мы не нашли, в $W$ всегда будет лежать среднее $\frac{1}{s}\sum_{i=1}^s x_i$ и, следовательно, в качестве $a_0$ всегда можно взять среднее. Действительно, распишем условие, что сумма 
$$\sum_{i=1}^s \rho(x_i-a_0, U)^2=\sum \|pr_{U^{\bot}} (x_i-a_0)\|^2$$
минимальна.  Продифференцируем по координатам $a_0$. Получим $$\sum -2pr_{U^{\bot}} x_i + 2s \,pr_{U^{\bot}} a_0=0$$
 Это условие означает, что проекции $a_0$ и среднего $\frac{1}{s}\sum x_i$ на подпространство $U^{\bot}$ совпадают. То есть эти две величины отличаются на элемент $U$. Тогда среднее лежит в $W$. 

Итак, вычтя из всех $x_i$ их среднее можно считать $a_0=0$, а все точки $x_i$ удовлетворяют равенству $\sum_{i=1}^s x_i=0$. Замечу, что это равенство нигде в дальнейшем не будет использовано. Благодаря такой замене мы свели задачу к поиску подпространства $U$ размерности $k$, которое минимизирует 
$$\sum_{i=1}^s \rho(x_i, U)^2=\sum_{i=1}^s \|pr_{U^{\bot}} (x_i)\|^2.$$

Воспользуемся тем, что $\|x\|^2=\|pr_U x\|^2+\|pr_{U^{\bot}} x\|^2$ или в другом виде $\|x\|^2-\|pr_U x\|^2=\|pr_{U^{\bot}} x\|^2$. Получаем, что выражение
$$\sum_{i=1}^s \|pr_{U^{\bot}}(x_i)\|^2=\sum_{i=1}^s \|x_i\|^2-\|pr_U x_i\|^2$$
должно быть минимально. Сумма $\|x_i\|^2$ постоянна. Значит  необходимо и достаточно, чтобы выражение $\sum_i \|pr_U x_i\|^2$ было максимально.


Для того чтобы посчитать проекцию на $U$ в каждом слагаемом выберем в $U$ ортонормированный базис $u_1,\dots,u_k$. Перепишем
$$\sum_{i=1}^s \|pr_U x_i\|^2=\sum_{i=1}^s\sum_{j=1}^k \lan x_i,u_j\ran^2=\sum_{j=1}^k \sum_{i=1}^s \lan x_i,u_j\ran^2.$$

Внутренняя сумма теперь есть значение некоторой квадратичной формы на векторе $u_j$. Разберёмся, что это за форма. Рассмотрим матрицу $X$ размера $s\times n$, чьи строки это  вектора $x_i$ $i\in\ovl{1,s}$. Тогда вектор $d_j=Xu_j$ состоит из скалярных произведений  $\lan x_i, u_j\ran$. Значит 
$$\lan d_j,d_j\ran = (Xu_j)^{\top}Xu_j = \sum_{i=1}^s \lan x_i,u_j\ran^2,$$ 
что совпадает со слагаемым нашей суммы. Рассмотрим симметричную матрицу $A=X^{\top}X$ и обозначим соответствующую ей форму за $q$. Тогда нам надо максимизировать выражение

$$\sum_{j=1}^k q(u_j).$$

Таким образом мы ищем максимум $\Tr q|_{U}$ по всем подпространствам $U$ размерности $k$, где форма $q$ соответствует матрице $X^{\top} X$. А его мы искать умеем. Сформулируем  ответ. 


\thrm  Пусть есть набор векторов $x_1,\dots,x_s \in V$. Определим симметричную, положительно полуопределённую матрицу $A$, как $A=X^{\top}X$, где строчки матрицы $X$ -- это вектора $x_i$. Тогда минимум по всем аффинным подпространствам $V$ размерности $k$ выражения $\sum_{i=1}^s \rho(x_i,W)^2$ достигается при $a_0=\frac{1}{s}\sum x_i$  и $U=\lan v_1,\dots,v_k\ran$, где $v_i$ --- собственные вектора $A$, причём соответствующие собственные числа упорядочены по убыванию. 
\ethrm

\rm Стоит немного сказать про вероятностную интерпретацию полученного ответа. Предположим, что точки $x_i\in \mb R^n$ сгенерированы каким-то случайным процессом и подчиняются некоторому общему распределению. Тогда вектор $a_0$ -- это просто оценка на математическое ожидание этого распределения.

После того как мы нашли $a_0$ мы центрируем набор $x_i$ и минимизируем $\sqrt{\sum \rho (x_i, U)^2}$. Представим себе, что $U=\{0\}$. Что значит эта сумма? Это то, что мы назвали бы оценкой дисперсией величины (конечно, надо ещё усреднить). 
А что если $U \neq \{0\}$? Тогда $\sqrt{\sum \rho (x_i, U)^2}$ -- это заготовка для оценки дисперсии проекций случайных величин на $U^\bot$. То есть мы ищем подпространство $U$, так что на ортогональное дополнение приходится минимально возможная дисперсия (а при проекции на само $U$ должен достигаться максимум дисперсии среди всех подпространств той же размерности).

Какой же смысле имеет матрица $X^\top X$, её собственные вектора и собственные числа. Матрица $X^\top X$ -- это с точностью до константы ($1/s-1$) эмпирическая матрица ковариации. Если мы хотим посчитать квадрат дисперсии в направлении $v$, то мы считаем $v\top X^\top X v$. Значит собственные вектора этой матрицы -- это такие вектора, для которых достигается экстремум квадрата дисперсии. А собственные числа -- это собственно квадраты дисперсии.
\erm

\rm Отдельно отметим, что в приложениях принято с самого начала центрировать данные и нормировать каждую компоненту $x_i$ так, чтобы дисперсия вдоль каждого направления была единичной.
\erm



\section{SVD-разложение}

При вычислении с помощью метода главных компонент часто используют несколько другие конструкции. Для того, чтобы в этом разобраться посмотрим на матрицу  $X^{\top}X$  и её собственные числа.  

\dfn Пусть $A$ -- линейное отображение между евклидовыми пространствами $U \to V$. Тогда сингулярными значениями $A$ называются корни из положительных собственных чисел оператора $A^*A$.
\edfn



Теперь обсудим важную конструкцию, проясняющую геометрический смысл сингулярных значений.


\thrm[SVD разложение] Пусть $L$ -- линейное отображение $L\colon U \to V$ между евклидовыми пространствами. Тогда существуют такие ортонормированный базисы $U$ и $V$, что матрица $L$ имеет вид 
$$\Sigma=\pmat \sigma_1 &\dots& 0 & 0\\
 \vdots & \ddots &\vdots & \vdots\\
 0 & \dots & \sigma_r & 0\\
 0 &  \dots & 0 & 0 \epmat,$$
 где $r$ -- ранг $L$. Числа $\sigma_1, \dots, \sigma_r$ на диагонали обязаны быть равными сингулярным значениям $L$.
На языке матриц это означает, что для любой матрицы $A \in M_{m\times n}$ существуют ортогональные матрицы  $P$ -- размера $m$ и $Q$ -- размера $n$,  что
$$A= P \Sigma Q.$$
 
\proof Рассмотрим оператор $B = L^{*}L$. Тогда существуют ортонормированный базис $e_1,\dots,e_n$ в котором оператор $B$ диагонален, с неотрицательными числами на диагонали $d_1\geq\dots\geq d_n\geq 0$. Имеем  $d_i=\sigma_i^2$ для единственного положительного $\sigma_i$. 
Посмотрим на вектора $Le_i \in U$. Они ортогональны. Действительно
$$\lan Le_i, Le_j\ran = \lan L^{*}Le_i,e_j \ran = \lan d_i e_i,e_j\ran,$$
что равно нулю, если $i\neq j$. В случае $i=j$ получаем $\|e_i\|^2=d_i$. Возьмём 
$$f_i=\frac{Le_i}{\sqrt{d_i}}$$
и дополним этот набор до ортонормированного базиса пространства $U$. Итого имеем $e_1,\dots,e_n$ ортонормированный базис $U$ и $f_1,\dots,f_m$ -- ортонормированный базис $V$.
Посмотрим матрицу $A$ в этих базисах. По определению $Le_i=\sqrt{d_i}f_i$. Это и даёт требуемый вид матрице оператора $L$.


Напоследок осталось решить вопрос, как выглядит матрица $Q$. В нашей конструкции матрица $Q$ есть матрица замены координат из стандартного базиса в базис из собственных векторов $e_i$ матрицы $A^{\top}A$. Если за $C$ обозначить матрицу из столбцов $e_i$, то $Q=C^{-1}$, но $C$ ортогональна и поэтому можно написать $Q=C^{T}$, то есть строки $Q$ -- собственные вектора $A^{\top}A$.
\endproof
\ethrm

\upr Получите аналогичное описание для $P$.
\eupr

\dfn Базисные вектора такой системы координат в $U$ называются левыми сингулярными векторами $A$, а базис в $V$ -- правыми.  
\edfn

Наличие SVD-разложения означает, что для всякого линейного отображения можно так выбрать декартову систему координат, что в этой системе координат это отображение будет выглядеть как растяжение вдоль каких-то осей.





SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $V_k$, то получится матрица ранга $k$ или меньше.

Если составить из этих проекцией матрицу $X^{(k)}$, то $\rho(\{x_i\}, V_k)^2= \|X-X^{(k)}\|_F^2 $, где 
$$\|X\|_F=\sqrt{\sum_{i,j} x_{ij}^2}=\sqrt{\Tr X^{\top}X}$$
есть нормы на пространстве матриц. Эта норма, называется нормой Фробениуса.

Оказывается, что так построенная матрица $X^{(k)}$, является ближайшей матрицей ранга $\leq k$ к матрице $X$
Действительно, рассмотрим матрицу $Y^{(k)}$, которая приближает $X$ не хуже $X^{(k)}$. То есть 
$$\|X -Y^{(k)}\|_F^2 \leq \| X-X^{(k)}\|_F^2= \sum \rho(x_i, V_k)^2.$$
Возьмём $L=\Im Y$ -- подпространство размерности $k$. Тогда матрица ${Y^{(k)}}'$ чьи столбцы есть проекции столбцов $X$ на $L$ ближе к $X$, чем $Y^{(k)}$ и равенство достигается, только если $Y^{(k)}={Y^{(k)}}'$. В этой ситуации оптимальная матрица как раз и приходит из пространства $L$ что нам бы и хотелось. Покажем это равенство.

Заметим по построению $\| X - {Y^{(k)}}'\|_F = \sum \rho (x_i, L)^2$, что обязано быть больше или равно чем $\sum \rho(x_i, V_k)^2$. Значит имеет место равенство. Но тогда и матрицы $Y^{(k)}$ и ${Y^{(k)}}'$ одинаково хорошо приближают $X$. Значит $Y^{(k)}={Y^{(k)}}'$ приходят из оптимального подпространства в методе главных компонент. Почему мы не говорим, что эти матрицы не обязательно равны $X^{(k)}$? Потому что оптимум не единственен (если есть кратные собственные числа у $X^\top X$).


Таким образом, нахождение проекций точек на оптимальное, с точки зрения метода главных компонент подпространство можно переформулировать, как нахождение ближайшей к $X$ матрицы ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение.

Прежде чем использовать SVD-разложение заметим, что в такой формулировке задача может быть поставлена в бескоординатном виде: для данного линейного отображения $L$ между евклидовыми пространствами найти наиболее близкое к нему линейное отображение ранга $k$ относительно нормы $\|L\|=\sqrt{\Tr L^*L}$.

\thrm Пусть $L$ -- линейное отображение между евклидовыми пространствами. Пусть $\Sigma$ -- матрица с сингулярными значениями $L$ на диагонали. Тогда ближайшее к $L$ отображения ранга $k$ имеет матрицу $\Sigma^{(k)}$ в базисах из правых и левых сингулярных векторов. Здесь  на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальное -- нули.
\proof Перейдём в базис из правых и левых сингулярных векторов. В этих базисах матрица $A$ -- это $\Sigma$.
Значит теперь мы решаем задачу нахождения ближайшей матрицы к матрице $\Sigma$. Для этого нам надо взять проекции строк $\Sigma$ на подпространство порождённое первыми $k$ собственными векторами $\Sigma^\top \Sigma$. После этого из этих проекций надо составить матрицу. Но первые $k$ собственных векторов $\Sigma^\top \Sigma$ это просто первые $k$ координатных векторов. Значит составленная из проекций матрица -- это $\Sigma^{(k)}$.
\endproof
\ethrm 

\crl Пусть $A$ имеет сингулярное разложение $P\Sigma Q$. Тогда ближайшая к $A$ матрица ранга $k$ (в смысле нормы Фробениуса) имеет вид $A^{(k)}=P\Sigma^{(k)}Q$, где на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальные элементы матрицы -- нули.
\ecrl

\upr На самом деле матрица $A^{(k)}$ ближайшая к $A$ среди матриц ранга $k$ и в смысле обычной матричной $l_2$-нормы. 
\eupr



\section{Немного вычислительной линейной алгебры}


Перед нами встаёт вопрос: как аккуратно вычислить те объекты линейной алгебры, которые мы определили. Какие для этого есть методы и чем они отличаются. Совершенно понятно, что это огромный круг вопросов и ответить на них на все невозможно. Поэтому я постараюсь обрисовать основные примеры и основные подходы к ответам.

Рассмотрим прежде всего задачу о решении системы линейных уравнений $Ax=b$ с вещественными коэффициентами. 

Во всех реальных приложениях вектор $b$, а часто и матрица $A$ даны не точно, а с некоторой погрешностью. Кроме того, при вычислениях с плавающей точкой мы создаём ещё больше погрешностей. Посмотрим  пример
$$ \pmat 1 & 1 \\ 1 & 1.0001 \epmat x = \pmat 2\\ 2.0001 \epmat$$
У этой системы есть точное решение $x=(1,1)$. Предположим, однако, что из-за ошибок округления вектор $b$ стал равен 
$$b_{new}=\pmat 2 \\ 2 \epmat$$
Решение для $b_{new}$ равно $x=(2,0)^\top$. Видно, что малые изменения коэффициентов системы могут значительно изменить её решение.

Попробуем разобраться, что происходит с общей системой при малых возмущениях её коэффициентов. Для начала разберёмся с погрешностями в $b$.

Обозначим погрешность в значениях свободного члена за $\Delta b$. То есть на самом деле нам дана система $Ay=b+\Delta b$. Самое лучшее, что мы можем сделать -- это точно решить эту новую систему. Насколько решение $y$ этой приближённой системы может отличаться от решения исходной? Пусть $y=x+\Delta x$. Тогда вычитая одно уравнение из другого получаем
$$A \Delta x= \Delta b.$$
Нас будет интересовать прежде всего не абсолютная, а относительная погрешность $\frac{\|\Delta x\|}{\|x\|}$. Оценим её
$$\frac{\|\Delta x\|}{\|x\|}=\frac{\|A^{-1} \Delta b \|}{\|x\|}=\frac{\|A^{-1} \Delta b \|}{\|b\|} \frac{\|b\|}{\|x\|} \leq \|A\| \|A^{-1}\| \frac{\|\Delta b\|}{\|b\|}.$$
Заметим, что здесь мы пользовались определением нормы матрицы при помощи нормы на исходном пространстве, но сам вид нормы на исходном пространстве нам не важен.

Получается, что чтобы мы не делали, просто из-за изначальных погрешностей в данных мы не можем рассчитывать  на погрешность меньшую $\kappa(A)\frac{\|\Delta b\|}{\|b\|}$
(чуть позже мы увидим, что эта оценка точная). 

\dfn Число $\kappa(A)$ называется числом обусловленности матрицы $A$.
\edfn

Оказывается, что число обусловленности отвечает и за влияние погрешностей в коэффициентах матрицы $A$.

Действительно, если посмотреть на решение возмущённой системы $(A+\Delta A)(x+\Delta x)=b$, то вычитая точную систему получаем
$$\Delta x= -A^{-1}\Delta A (x+\Delta x)$$
Переходя к нормам получаем
$$\frac{\|\Delta x\|}{\|x+\Delta x\|}\leq \|A^{-1}\| \|A\| \frac{\|\Delta A\|}{\|A\|}$$

Как найти $\kappa(A)$? Ответ зависит от выбранной нормы. Нам проще всего понять, как найти $\kappa(A)$ в случае евклидовой нормы на $\mb R^n$. В этой ситуации $$\kappa_2(A)=\frac{\sigma_1}{\sigma_n}.$$
Двойка снизу тут в честь $l_2$-нормы.
Покажем теперь, что оценка погрешности при помощи $\kappa_2(A)$ точная. Для этого перейдём в базисы из сингулярных векторов. Тогда можно считать, что $A=\Sigma$ -- диагональна. Возьмём вектора $x,b,\Delta x$ и $\Delta b$ следующим образом:
$$x=\pmat 1 \\ 0\\ \vdots \\ 0 \epmat,\, b=Ax=\pmat \sigma_1 \\ 0\\ \vdots \\ 0 \epmat, \, \Delta b = \pmat 0 \\ \vdots \\ 0 \\ \eps \epmat, \, \Delta x=  \pmat 0 \\  \vdots \\ 0 \\ \eps/\sigma_n\epmat.$$
Для таких векторов оценка достигается.

Можно ли заранее оценить $\kappa(A)$? Можно. Но это не так просто и мы это обсуждать не будем. Замечу только, что главная сложность состоит в оценке $\|A^{-1}\|$.

\dfn
Говорят, что система $Ax=b$ хорошо обусловлена, если $\kappa(A)$ -- мало.
\edfn

Однако мы пока ничего не сказали про особенности методов, могут ли они существенно добавить к ошибкам? Оказывается, что могут. Рассмотрим систему 
$$\pmat 0.0001 & 1 \\ 1 & 1 \epmat x= \pmat 1 \\ 2 \epmat $$
Допустим мы храним только три значащих цифры. Применяя напрямую метод Гаусса последнее уравнение преобразуется к виду $-9999 x_2=-9998 $. Округление его точного решения есть $x_2=0.9999$ (если округлить до 4-го знака), откуда получаем $x_1=1$ (что тоже будет верным до 4-го знака). Но если мы используем округление для коэффициентов матрицы на первом шаге, то предыдущее равенство принимает вид $10000x_2 = 10000$. Получаем, что $x_2'=1$ и $x_1'=0$. Что значительно отличается от точного решения.

\rm Отметьте, что $\kappa_2(A)$ в этом примере всего лишь $2.61...$.
\erm

\dfn Метод называется устойчивым, если ошибки в ходе вычисления имеют порядок, сравнимый с ошибками от возможных погрешностей в исходных данных.
\edfn

Метод Гаусса не является устойчивым методом решения систем линейных уравнений. Можно его доработать, на каждом шаге переставляя строки матрицы так, чтобы каждый раз выбирать в качестве главного элемента  наибольший элемент в столбце. Однако это всё равно решает не все проблемы (см. Wilkinson).

В качестве альтернативы можно посмотреть различные методы, которые находят $QR$ разложение. Однако и $QR$ разложение может давать побочные эффекты.





Есть другой класс методов, позволяющих довольно легко найти приближённое решение. Речь идёт про  итерационные методы. Разберём самый простой пример. Пусть нам дана система линейных уравнений вида $x=Ax+b$, где $A$ -- квадратная матрица. Заметим, что любая система может быть представлена в таком виде. 

Предположим в дальнейшем, что собственные числа $A$ по модулю меньше $1$. Возьмём начальный вектор $x_0$ и построим последовательность
$$x_i=Ax_{i-1}+b.$$
Покажем, что при указанных условиях, эта последовательность стремится к решению системы. Прежде всего заметим, что матрица $E-A$ обратима, так как $0$ не является её собственным числом, и, следовательно, система имеет единственное решение. Пусть это решение --- $x'$. Обозначим $h_i=x_i-x'$. Для $h_i$ получается соотношение:
$$x'+h_i=Ax'+Ah_{i-1}+b.$$
Откуда получаем, что $h_i=Ah_{i-1}$. Но тогда $h_n=A^n h_0$ стремятся к нулю при $n\to \infty$. Значит $x_n\to x'$.


Можно показать, что этот метод устойчив, если имеет место сходимость. Кроме того, заметим, что каждый шаг итерации вычисляется за $O(n^2)$ или даже меньше, если в матрице $A$ много нулей.

Есть более интересные вариации на тему метода итераций. Например, метод Гаусса-Зейделя. Оказывается, что совершенно не обязательно смотреть на представление системы именно в виде $x=Ax+b$. Пусть матрица $A$ есть сумма $A=L+D+U$ (нижнетреугольная -- диагональная -- верхнетреугольная). Представим систему $Ax=b$ в виде 
$$(L+D)x=-Ux+b$$
Если мы определим последовательность $x_i$ по правилу $(L+D)x_{i+1}=-Ux_i+b$, то за сходимость такой последовательности будут отвечать собственные числа матрицы $-(L+D)^{-1}U$. 

Метод Гаусса-Зейделя чаще сходится и скорость сходимости для него обычно более высокая, чем для простого метода итераций (но не всегда). Кроме того, так как матрица $(L+D)$ нижнетреугольная, то решение системы с такой матрицей требует $O(n^2)$ операций. То есть в итоге, каждый шаг стоит $O(n^2)$ операций как и в методе простых итераций.







\subsection{Нахождение собственных чисел и собственных векторов}

Хотелось бы уметь оценивать размер собственных чисел матрицы $A$. Можно ли это сделать? Оказывается, что можно. 


Пусть $A$ -- вещественная или комплексная квадратная матрица. Рассмотрим строки $A$. Зададим элементы $r_i=\sum_{j\neq i} |a_{ij}|$. Рассмотрим множество замкнутых кругов радиуса $r_i$ с центром в $a_{ii}$ на комплексной плоскости. Эти круги называются кругами Гершгорина. 

\thrm Все собственные числа матрицы $A$ лежат в объединении кругов Гершгорина. 
\ethrm
\proof Пусть $\lambda$ -- собственное число $A$, а $v$ -- соответствующий собственный вектор. Тогда $Av=\lambda v$. Посмотрим на максимальный по модулю элемент $v$ -- $v_i$ и соответсвующую ему строчку. Покажем, что $\lambda$ лежит в круге Гершгорина с номером $i$. Имеем $\sum a_{ij}v_j =\lambda v_i$  и, значит, $\sum_{j\neq i} a_{ij}v_j= (\lambda - a_{ii})v_i $. Имеем неравенство $$|\lambda -a_{ii}||v_i| \leq |v_i|\sum_{j\neq i} |a_{ij}|$$
Сокращая на $v_i$ получаем требуемое. 
\endproof

\rm В частности, это рассуждение даёт критерий невырожденности матрицы -- матрица $A$ невырождена, если сумма модулей её внедиагональных элементов каждой строки меньше модуля диагонального элемента. Такие матрицы называются матрицами с диагональным преобладанием
\erm

\rm
Переходя от матрицы к транспонированной можно получить ещё одну оценку, которая получается из рассмотрения столбцов $A$.
\erm

А можно ли точно сказать, в каком из кругов находится собственное число? Оказывается, что иногда можно. Для того чтобы это показать нам нужна некоторая аналитическая подготовка.

\fct[Корни непрерывно зависят от коэффициентов] Пусть $f(x,t)$ -- комплексный многочлен от двух переменных со старшим коэффициентом $1$ при $x^n$, как многочлена над $\mb C[t]$. Пусть $\lambda_1,\dots,\lambda_n$ -- корни $f(x,0)$, а $\mu_1,\dots, \mu_n$ -- корни $f(x,1)$. Тогда cуществуют такие непрерывные на $[0,1]$ функции $\lambda_i(t)$, что $f(\lambda_i(t),t)=0$ и $\lambda_i(0)=\lambda_i$, а $\lambda_i(1)=\mu_{\sigma(i)}$ (для некоторой перестановки $\sigma \in S_n$).
\efct

\thrm Если $k$ кругов Гершгорина матрицы $A\in M_n(\mb C)$ не пересекаются с остальными, то в них лежат $k$ собственных чисел с учётом кратности.
\ethrm
\proof Рассмотрим матрицу $A_t= D+ tB$, где $D$ -- это матрица из диагональных элементов матрицы $A$, а $B$ -- из внедиагональных. Применим факт для характеристического многочлена матрицы $A_t$. Пусть  $\lambda_1(t),\dots,\lambda_k(t)$ -- непрерывные функции, соответствующие выбранным кругам Гершгорина $B_1,\dots,B_k$. Покажем, что каждая из них не вышла за пределы объединения этих кругов в момент $t=1$. 

Пусть скажем $\lambda_1$ при $t=1$ оказался в другом круге. Круги Гершгорина замкнуты. Пусть 
$$r=\rho(B,\cup_{i=1}^k B_i),$$
где $B$ -- это объединение оставшихся кругов. Рассмотрим функцию $f(t)=\rho(\lambda_1(t),B)$,  Это непрерывная по $t$ функция при этом $f(1)=0$. Значит есть такое $t$, что $0<f(t)<r$. С одной стороны, эта точка должна лежать в кругах Гершгорина, но с другой, из определения $r$ следует, что она должна лежать вне всех кругов.  
\endproof

Как это применить к оценки возмущения собственных чисел? Предположим, что $A$ диагонализуема:
$$A=C\pmat \lambda_1 && \\ & \ddots & \\ && \lambda_n \epmat C^{-1}=CDC^{-1}.$$
Рассмотрим сумму $A+\Delta A$. Как оценить собственные числа этой матрицы, через собственный числа $A$?

Пусть $\| \Delta A\|=\eps$. Рассмотрим матрицу
$$C^{-1}(A+\Delta A) C= D+C^{-1}\Delta A C $$
Надо оценить её собственные числа. Для этого заметим, что позиции  $i,j$ матрицы $C^{-1}\Delta A C$ стоит элемент размера не более $\| C^{-1}\| \|C\| \eps$. По теореме о кругах Гершгорина собственные числа $A+\Delta A$ находятся в кругах радиуса $n \| C^{-1}\| \|C\| \eps $ c центрами в $\lambda_i$. Кроме того, для достаточно маленького $\eps$ пары таких кругов не пересекаются или вложены друг в друга. Отсюда

\crl Для достаточно маленького возмущения $\Delta A$ в круге радиуса  $n \| C^{-1}\| \|C\| \|\Delta A\|$ с центром в с.ч.  $\lambda$ матрицы $A$ находится ровно $k$ собственных чисел $A+\Delta A$, где $k$ -- это кратность $\lambda$ у $A$.
\ecrl

Отсюда видно, что  роль, аналогичную числу обусловленности, в оценке на собственные числа играет число $\|C\|\|C^{-1}\|$. На самом деле для каждого собственного числа можно ввести своё число обусловленности. В любом случае, видно, что оценить это число заранее не находя матрицу $C$ не получится. Тем не менее что-то сделать можно. Для произвольной системы это число уже можно оценить, если мы приближённо вычислили $C$ и $C^{-1}$. Однако, есть матрицы для которых это число и так наилучшее возможное. Что это за матрицы?


Раньше, для того, чтобы найти спектр оператора мы считали характеристический многочлен. Однако, его вычисление довольно трудоёмко. Кроме того, после этого необходимо найти ещё и корни характеристического многочлена.


Однако, есть ряд других методов нахождения собственных чисел. С первой идеей мы уже знакомы -- это вариация метода итераций: рассмотрим случайный вектор $x$. Посмотрим на последовательность $A^kx/\|A^kx\|$ $k\to \infty$. Она стремится к максимальному собственному вектору для почти любого $x$. Легко оценить и собственное число.

Если предположить симметричность матрицы $A$. Заметим, что  для почти любого $x$ 
$$\frac {\lan A^{k+1}x, A^k x \ran}{\lan A^k x, A^k x \ran} \to \lambda, $$
где $\lambda$ -- максимальное собственное число.

Есть ещё множество методов для нахождения собственных чисел. 

Есть и метод нахождения всех одновременно собственных чисел: $QR$-алгоритм. Он заключается в следующем: вначале возьмём $A_0=A$. На $k$-ом шаге для матрицы $A_{k-1}$ строится $QR$-разложение $A_{k-1}=Q_kR_k$, а затем строится матрица $A_k=R_kQ_k$. Оказывается, что этот процесс в случае симметричной $A$ сойдётся к диагональной матрице, чьи диагональные элементы  и есть собственные числа $A$. 

Почему этот алгоритм вообще работает и как до него догадаться.

\lm Имеет место соотношение $A_k=Q_k^\top A_{k-1} Q_k$.
\elm

Определим $Q^{(k)}=Q_1\dots Q_k$ и $R^{(k)}= R_k \dots R_1$. В частности, из предыдущей леммы получаем, что $$A_k=Q_k^\top \dots Q_1^\top A Q_1 \dots Q_k= {Q^{(k)}}^\top A Q^{(k)}.$$

\lm  Имеет место соотношение. 
$$A^k=Q^{(k)}R^{(k)}.$$
\elm
\proof Докажем по индукции. 
$$A^k=A A^{k-1}= Q^{(k)}A_k{Q^{(k)}}^\top Q^{(k-1)}R^{(k-1)} = Q^{(k)}A_k Q_k^\top R^{(k-1)}=Q^{(k)} R^{(k)}$$
\endproof
Для того, чтобы просто сформулировать и доказать результат о сходимости QR-алгоритма мы ограничимся положительно определёнными симметричными матрицами.

\thrm Пусть $A$ -- симметричная матрица с различными положительными собственными числами. Тогда столбцы $Q^{(k)}$ стремятся к собственным векторам $A$, а матрицы $A_k$ -- к диагональной матрице, где на диагонали расположены собственные числа $A$.
\ethrm
\proof  Пусть $u_1,\dots,u_n \in \mb R^n$ -- ортонормированный базис из собственных векторов $A$ упорядоченный по убыванию собственных чисел.
Разложим стандартный базисный вектор $e_j$ по базису $u_i$
$$e_j = \sum_i c_{ij} u_i $$
Применим матрицу $A^k$. 
$$A^k e_j = \sum_i c_{ij} \lambda_i^k u_i.$$
Предположим, что главные миноры матрицы $C$ не равны нулю.
Как мы уже доказали, матрица $Q^{(k)}$ -- есть результат ортогонализации (с точностью до $\pm 1$) столбцов матрицы $A^k$, то есть векторов $A^k e_j$. Обозначим столбцы $Q^{(k)}$ за $q^{(k)}_j$ Посмотрим на первый столбец $A^k$. Исходя из нашего предположения, $c_{11}$ не ноль и
$$q^{(k)}_1=\frac1{\|A^ke_1\|} A^k e_1 = \pm u_1 + o(1),$$
при $k\to \infty$. Меняя при необходимости направление $u_1$ на противоположенное избавимся от $\pm$ в формуле. Посмотрим, что происходит при ортогонализации второго столбца. Вычислим скалярное произведение:
$$\frac{\lan A^k e_2, A^k e_1 \ran}{\lan A^k e_1, A^k e_1 \ran} = \frac{c_{12}}{c_{11}}+ O\left( \lambda_2^{2k}/\lambda_1^{2k}\right)
=\frac{c_{12}}{c_{11}}+ o \left(\left(\lambda_2/\lambda_1 \right)^k\right)  $$
Вычитая получаем
$$ A^k e_2 - \left(\frac{c_{12}}{c_{11}}+ o \left(\left(\lambda_2/\lambda_1\right)^k\right)\right) A^k e_1= o(\lambda_2^k)u_1+ c_{22}\lambda_2^ku_2 - \frac{c_{12}}{c_{11}}c_{21}\lambda_2^k u_2 +o(\lambda_2^k).$$
Коэффициент $c_{22}-\frac{c_{12}}{c_{11}}c_{21}$ по условию не равен $0$. После нормировки этот вектор стремится к $\pm u_2$. Продолжая так далее, получим, что столбцы $Q^{(k)}$ сходятся к собственным векторам $A$ в порядке убывания собственных чисел. Тогда в $i,j$ элементе матрицы $A_k={Q^{(k)}}^\top A Q^{(k)}$  стоит
$$\lan q^{(k)}_i, Aq^{(k)}_j\ran \to \lan u_i, \lambda u_j \ran = \lambda_i \delta_{ij}.$$
Значит, матрица $A_k$ действительно сходится к нужной диагональной матрице.
\endproof



\section{Спектры графов}
С графом $G$ связаны несколько разных матриц. Как свойства этих матриц, их собственные числа отражаются в комбинаторных свойствах графа $G$ и может ли это помочь?
Приведём пример, как знание спектра графа даёт некоторые оценки на сложно вычисляемые величины.

\rm Граф $G$ является $k$-регулярным тогда и только тогда, когда вектор $(1,\dots,1)^\top$ является собственным вектором с собственным числом $k$ для $A(G)$.
\erm

\thrm Пусть $G$ -- $k$-регулярный граф. Тогда размер максимального независимого множества в графе оценивается как
$$\alpha(G)\leq n\frac{-\lambda_n}{k-\lambda_n}.$$
\ethrm
\proof Рассмотрим характеристический вектор $v$ для независимого множества размера $\alpha$. Имеем $v^{\top}Av=0$ и при этом $v^{\top}v=\alpha$. Так как граф $k$-регулярный, то у него есть нормированный собственный вектор $u_1=\frac{1}{\sqrt{n}}(1,\dots,1)$. Тогда $\lan v,u_1\ran = \frac{\alpha}{\sqrt{n}}$. Разложим вектор $v=c_1u_1 + \dots + c_n u_n$ по ортонормированной системе собственных векторов. Тогда из условия независимости получаем
$$0=v^{\top}Av=\sum c_i^2 \lambda_i= \lambda_1\frac{\alpha^2}{n}+ \sum_{i\geq 2} \lambda_i c_i^2\geq \lambda_1\frac{\alpha^2}{n}+ \lambda_n \sum_{i\geq 2} c_i^2.$$
Мы знаем, что $\sum c_i^2=\alpha$, откуда $\sum_{i\geq 2} c_i^2=\alpha - \frac{\alpha^2}{n}$. Итого 
$$0\geq \lambda_1\frac{\alpha^2}{n}+\lambda_n(\alpha- \frac{\alpha^2}{n})$$
Сокращая на $\alpha$ получаем 
$$(\lambda_1-\lambda_n)\frac{\alpha}{n}\leq -\lambda_n,$$
что эквивалентно нужному неравенству.
\endproof




\exm\\
1) Полный граф $K_n$ имеет спектр $n-1,-1,\dots,-1$. В полном соответствии с тем, что размер максимального независимого множества равен $1$.\\
2) Граф Петерсена имеет спектр 3, 1, 1, 1, 1, 1, -2, -2, -2, -2. Размер независимого множества в точности равен $4$.\\
3) Оценка точна в общем виде для графов Кнезера $K(n,r)$.


Из оценки на размер независимого множества можно вывести оценку на хроматическое число.

\zd Покажите, что $\alpha(G) \chi(G) \geq n$. Выведите отсюда, что для регулярного графа $\chi(G)\geq 1+ \frac{\lambda_1}{-\lambda_n}$. Покажите, что для графа Петерсена оценка точная.
\ezd

\rm Удивительно, но ровно та же оценка верна и для нерегулярных графов тоже.
\erm

\zd Покажите, что $\chi(G) \leq [\lambda_1]+1$ для любого графа $G$. (Адаптируйте классическое доказательство с максимальной степенью).
\ezd


\subsection{Дополнительно: ещё трюки и применения}

\thrm Рассмотрим граф $K_{10}$. Тогда его невозможно покрыть тремя копиями графа Петерсена.
\ethrm
\proof Пусть граф $K_{10}$ покрыт тремя графами Петерсена. Тогда его матрица смежности удовлетворяет соотношению 
$$A(K_{10})=P_1+P_2+P_3.$$
Для нашего удобства перепишем это в виде 
$$-A(K_{10})=-P_1-P_2-P_3.$$
Заметим, что вектор $(1,\dots,1)^\top$ -- собственный для всех трёх матриц. Значит мы может ограничить указанное равенство на его ортогональное дополнение $U$.

Мы находимся в 9-тимерном пространстве. Воспользуемся неравенствами на собственные числа суммы (которые мы не доказывали). Тогда $i$-ое собственное число  суммы трёх матриц $A,B,C$ может быть оценено как $\lambda_t +\mu_s+ \nu_r$, где $r+s+t=i+2$. У операторов $-P_i|_U$ собственные числа равны $$2,2,2,2,-1,-1,-1,-1,-1.$$
Возьмём  $r=s=5$ и $t=1$. Получим оценку на $9$-ое собственное число $-A(K_{10})|_U$. Но мы его знаем: это $1$. С другой стороны получается, что оно должно быть меньше $0$. Противоречие! 
\endproof

Кроме оценок на хроматическое число и покрываемость при помощи спектров можно доказать негамильтоновость графа, оценивать его рёберное хроматическое число переходя к рёберному графу и т.д.

\zd Пусть $B$ -- матрица инцидентности графа $G$. Покажите, что $B^\top B - 2 E$ есть матрица смежности для рёберного графа $G$, а $B B^\top$ есть $A(G)+D$, где $D$ -- это матрица со степенями вершин графа $G$ на диагонали. 
\ezd

\zd Покажите, что в рёберном графе графа Петерсена нет индуцированного цикла длины 10 и выведите отсюда, что граф Петерсена негамильтонов.
\ezd

Есть, однако, две темы, связанные с собственными числами графов, которые приводят к непосредственным применениям. К сожалению, как часто бывает, у нас нет шансов внимательно посмотреть результаты этих разделов, поэтому мы ограничимся лишь очерчиванием их границ.




\dfn Для $A$ -- подмножества $V(G)$ определим $\partial A$ как множество всех рёбер ведущих из $A$ в вершины не из $A$. Константа Чигера или константа расширения графа  $G$ это 
$$h(G) := \min \left\{ \left. \frac{| \partial A |}{| A |} \right|   A \subseteq V(G), 0 < | A | \leq \tfrac{1}{2} | V(G)| \right\} .$$
\edfn

Вычислить константу Чигера очень сложно. Однако её можно оценить.

\begin{thmm}[Неравенство Чигера] Пусть $G$ -- $d$-регулярный граф и $\lambda_2$ -- его второе по максимальности собственное число. Тогда
$$\tfrac{1}{2}(d - \lambda_2) \le h(G) \le \sqrt{2d(d - \lambda_2)}.$$
\end{thmm}



\dfn Семейство графов $G_n$ называется $(n,d,\alpha)$ (алгебраическим) экспандером, если $G_n$ -- $d$-регулярный граф на $n$ вершинах с $\lambda=\max(|\lambda_2|,|\lambda_n|)\leq \alpha d$.
\edfn

Где могут применяться такие графы:\\
1) Коды, исправляющие ошибки\\
2) Псевдослучайные генераторы\\
3) Дерандомизация

Несмотря на то, что большая часть регулярных графов степени $d$ обладает указанными свойствами  до некоторого времени не было ни одного явного примера семейства экспандеров. Первый пример семейства экспандеров дал Григорий Маргулис. \\


\exm\\
1) Рассмотрим $\mb Z/n \times \mb Z/n$ и проведём для пары $(x,y)$ ребро в $(x \pm 2y,y), (x \pm (2y+1),y), (x,y \pm 2x), (x,y \pm (2x+1))$. Получается граф-экспандер с $\lambda \leq 5\sqrt{2}$\\
2) Можно подметить, что рёбра экспандеров из предыдущего примера связаны с некоторыми обратимыми преобразованиями, то есть с какой-то группой. Это общее место для конструкции большинства экспандеров.



\dfn Определим Лапласиан графа как $L(G)$ как $D-A(G)$.
\edfn

\zd $L(G)=B B^\top$, где $B$ -- матрица инцидентности произвольным образом ориентированного графа $G$.
\ezd

\zd $L$ -- неотрицательно определена и  $\rk L= \rk B=n-c$, где $c$ -- это количество компонент связности графа $G$.
\ezd



Кроме собственно графов можно рассматривать взвешенные графы. Понятно, как для них определить матрицы $D$, $A(G)=W(G)$ -- матрица весов, $L(G)$. Кроме этих матриц нам понадобится ещё и нормализованная матрица Лапласа.


\dfn Рассмотрим матрицу $D^{-1/2} L(G) D^{-1/2}$. Это нормализованный Лапласиан.
\edfn

На основе нормализованного Лапласиана можно построить алгоритм фрагментации изображений. Как и любая естественная задача она не имеет чёткой формулировки. В 2000 году Jianbo Shi и  Jitendra Malik \href{https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf}{предложили} следующую версию формулировки задачи: по картинке строится взвешенный граф $G$  -- его множество вершин $V$ -- это пиксели, рёбра проводятся, если вершины близки друг к другу и вес зависит от того, насколько пиксели похожи друг на друга.

Теперь рассматриваются все подмножества пикселей $A\subseteq V$ и минимизируется величина разреза, который отрезает $A$ от остального графа.
$$\min_{A} \frac{assoc(A,V\setminus A)}{assoc(A,V)}  + \frac{assoc(A, V\setminus A)}{assoc(V\setminus A,V)}, $$
$$\text{ где } \,\, assoc(A,B)=\sum_{i\in A, j\in B} w_{ij}$$.


Вопрос стоит в выборе оптимального $A$. Сопоставим множеству $A$ вектор $y$, такой что $$y_i = \begin{cases}-b=-\frac{\sum_{v\in A} d_v}{\sum_{v \not\in A} d_v}, \text{ если } i\not\in  A \\
1, i\in A 
\end{cases}$$

Оказывается, что в этом случае задача сводится к минимизации по всем таким $y$ выражения 
$$\frac {y^\top L(G) y} {y^\top D y } \text{ при этом выполнено } y D \bf 1 = 0.$$ 

Здесь $\bf 1$ -- это столбец из единиц. Решить эту дискретную задачу (допустимых $y$-ков конечное число) трудно, поэтому авторы переходят к непрерывной, отбрасывая ограничения на значения $y_i$. Минимум такого выражения достигается на $D^{-1/2}v$, где $v$ -- собственный вектор нормализованного Лапласиана для минимального положительного собственного числа ($D^{-1/2}\bf 1$ -- это собственный вектор для собственного числа $0$).

Теперь, беря $v$ можно разделить точки по признаку положительности и отрицательности (например).

\rm Похожий метод для матрицы Лапласа приводит к оценке связности графа и позволяет приближённо найти минимальный вершинный разрез графа дающий несколько компонент связности. (см. вектор Фидлера).
\erm






\section{Двойственное пространство}

Начнём наше триумфальное возвращение из мира вещественных чисел в мир произвольных полей. В дальнейшем $K$ обозначает произвольное поле. 

\dfn Определим двойственное пространство к пространству $V$ как $V^*=\Hom (V, K)$.
\edfn

\dfn Пусть $e_1,\dots,e_n$ -- базис $V$. Определим $e^1,\dots,e^n$ как базис пространства $V^*$, заданный следующим соотношением: 
$$e^j(e_i)=\delta_{ij}.$$
Иными словами, $e^i$ -- это $i$-ая координатная функция в базисе $e$.
\edfn

\utv Пусть $V$ -- векторное пространство над полем $K$. Тогда имеет место естественный изоморфизм пространств $V\to {V^*}^*$.
\eutv
\proof Построим отображение по правилу $v \to (f \to f(v))$. Это отображение инъективно. Из равенства размерностей следует, что этого и достаточно.
\endproof

Если мы перешли от одного базиса к другому в пространстве $V$, то что произошло в $V^*$? 

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из старого базиса в новый. Тогда матрица перехода из базиса $e^1,\dots,e^n$ в базис $\hat{e}^1,\dots,\hat{e}^n$ есть ${C^{\top}}^{-1}$.
\proof Вспомним нашу конвенцию: матрица перехода $C$ это, столбцы которой составлены из координат нового базиса в старом базисе. Если у нас есть вектор $v$, то это условие означает, что его новый столбец координат $y$ и старый столбец координат связаны уравнением $x=Cy$. Переписывая получаем соотношение 
$$C^{-1}x=y.$$
Но это есть соотношение между координатными функциями в новом базисе и в старом базисе, то есть, между векторами двойственных базисов. Почти то, что нужно. Осталось из этого соотношения найти матрицу перехода. Для этого запишем $$y_i=\sum_j (C^{-1})_{ij}x_j$$
Значит, элементы $(C^{-1})_{ij}$ должны стоять в $i$-ом столбце матрицы перехода из $\hat{e}$ в $\hat{e}^*$. Это значит, что эта матрица равна ${C^{-1}}^{\top}$.
\endproof
\ethrm

В особые отношения со своим двойственным пространством вступают евклидовы и унитарные пространства. 

\utv Пусть $V$ -- евклидово или унитарное пространство. Тогда любой линейный функционал имеет вид $\lan v, \_ \ran$ для единственного вектора $v$. 
\eutv
\proof Пусть $f$ -- линейный функционал. Рассмотрим $U=\Ker f^\bot$. Это одномерное пространство. Рассмотрим $v\in U$ отличный от нуля и подгоним при помощи константы так, чтобы  $f(v)=\lan v,v \ran$. Этого достаточно благодаря линейности каждого выражения.  
\endproof

\section{Немного про квантовую механику}

Вкратце напомню парадигму квантовой механики. Физическая система в квантовой механике задаётся при помощи некоторого унитарного пространства $V$. Каждая прямая этого унитарного пространства задаёт некоторое возможное физическое состояние системы. На каждой прямой есть вектор, по норме равный $1$. Итого, каждый вектор, равный по норме $1$ описывает некоторое физическое состояние. 

Квадрат модуля скалярного произведения двух таких нормированных векторов  $|\lan u,v \ran|^2$ интерпретируется как вероятность того, что  $v$ при измерении будет находится в состоянии $u$ (если вы можете померить, что что-то находится именно в состоянии $u$). Таким образом, каждый вектор $u\in V$ задаёт линейный функционал $\lan u, \_ \ran $ на $V$, который по состоянию $v$ выдаёт информацию о вероятности найти $v$ в состоянии $u$.

Каждой измеримой физической величине соответствует самосопряжённый оператор $T$ на $V$. Пусть $\psi_i$ -- это ортонормированный базис из собственных векторов $T$ с собственными числами $\lambda_i$. Тогда вероятность, что находясь в состоянии $v$ при измерении величины $T$ мы получим $\lambda_i$ есть вероятность найти $v$ в состоянии $\psi$, то есть $|\lan \psi_i, v \ran|^2$. Несложно заметить, что, как и должно быть, сумма по всем возможным $i$ этих вероятностей даёт $1$. Такая конструкция объясняет квантованность (дискретность) значений разных физических величин, которая возникает при измерении.

Типичным примером квантовой системы является нерелятивистская (без учёта теории относительности) модель одной частицы в $\mb R^3$. В этом случае роль унитарного пространства играет 
$$V= L_2(\mb R^3)=\left\{  f\colon \mb R^3 \to \mb C\, \Big| \, f - \text{ измерима и } \int_{\mb R^3} |f|^2<\infty \right\}.$$
Технически тут ещё понадобится отождествить функции отличающиеся друг от друга на множестве меры ноль, но я это опускаю. Скалярное произведение таких функций задаётся привычной формулой 
$$\lan f,g \ran = \int_{\mb R^3} \ovl{f}g\, dx.$$
Состояние квантовой системы, соответствующее нормированной функции $f\in V$ (т.е. $\|f\|^2=1$)  имеет следующую интерпретацию: $\int_{B} |f|^2$ есть вероятность для частицы оказаться в области $B$. 

 

В квантовых вычислениях основным объектом является квантовый бит или кубит (qubit) -- это двумерная квантовая система. Первый её базисный вектор обозначается $\left|0\ran$, а второй $ \left|1\ran$. Предполагается, что это собственные вектора оператора энергии т.е. два эти состояния различаются при измерении энергии. Произвольное состояние (нормированное) есть смесь 
$$c_1 \left|0\ran+ c_2 \left| 1\ran, \text{ где } |c_1|^2+|c_2|^2=1.$$
 

Допустим, у нас есть две независимые системы с пространствами $V_1$ и $V_2$. Какое пространство соответствует  объединению двух таких систем? Обозначим это пространство за $V_1\otimes V_2$. Посмотрим как должны выглядеть состояния этого пространства. Понятно что для каждой пары состояний $u_1\in V_1$ и $u_2\in V_2$ должно существовать своё состояние объединённой системы. Обозначим его за $u_1 \otimes u_2$. Другими словами должно быть задано отображение $i\colon V_1\times V_2\to V_1 \otimes V_2$.

Пусть есть одна пара $(u_1,u_2)$ и вторая пара $(v_1,v_2)$. Какова вероятность того, что при измерении система в состоянии $u_1\otimes u_2$ перейдёт  в состояние $v_1 \otimes v_2$? Логично, чтобы это было $|\lan u_1,v_1 \ran \lan u_2, v_2\ran|^2$.

Какому скалярному произведению на таких парах это соответствует? Должно быть $\lan u_1\otimes u_2, v_1\otimes v_2\ran = \lan u_1,v_1 \ran \lan u_2, v_2\ran$. То есть, скалярное произведение с состоянием $u_1\otimes u_2$ есть не линейная функция от пары $(v_1, v_2)$, а билинейная!



Линейные отображения можно складывать между собой. Сумме линейных отображений, заданных при помощи $u_1\otimes u_2$ на $V_1\otimes V_2$ должна соответствовать сумма билинейных форм на $V_1\times V_2$. Заметим, что любую билинейную форму можно представить в виде суммы форм заданных при помощи пар $(u_1,u_2)$. Таким образом, логично считать, что все линейные функционалы на $V_1\otimes V_2$ должны соответствовать билинейным формам на $V_1\times V_2$.





\section{Тензорное произведение}

Сейчас мы немного поговорили про пространство линейных функционалов, прошлом семестре мы подробно остановились на билинейных операциях. А квантовая механика дала нам необходимость в следующем определении:

\dfn Пусть дана пара пространств $U,V$ над полем $K$. Тогда их тензорным произведением называется пространство 
$U\otimes V$ вместе с билинейным отображением отображением
$$i \colon U \times  V \to U \otimes V,$$
удовлетворяющее условию что для любого билинейного отображения из $h\colon U \times V \to W$ существует единственное линейное отображение 
$$\hat{h}\colon U\otimes V \to W,$$
что 
$$\hat{h}\circ i=h.$$
Иными словами, отображение $i$ -- это <<универсальное>> билинейное отображение.
\edfn 


\lm Пусть $U\leq V$, а $\pi \colon V \to V/U$ -- это каноническая проекция на $V/U$. Пусть $L\colon V \to W$ -- линейное отображение. Тогда, для того, чтобы существовало $\hat{L}\colon V/U \to W$, что $L=\hat{L}\circ \pi$ необходимо и достаточно, чтобы $L(U)=\{0\}$.
\elm





\thrm Пусть $U,V$ -- пара векторных пространств. Тогда имеет место следующая конструкция тензорного произведения:
$$U \otimes V \cong K\lan U \times V \ran / Rel,$$
где $Rel$ -- это подпространство порождённое формальными суммами
$$(\lambda u_1+u_2, v) - \lambda (u_1, v) - ( u_2,v) \text{ и } (u,\lambda v_1+v_2) - \lambda (u,v_1) - (u,v_2).$$ 
\proof Для удобства обозначим пространство
$$T=K\lan U \times V \ran / Rel.$$ Будем обозначать образы элементов $(u,v)$ в $T$ как  $u\otimes v$. Отображение $$i \colon U\times V \to T$$
заданное правилом  $(u,v) \to u \otimes v$
билинейно по самому определению соотношений из $Rel$. Пусть теперь дано пространство $W$ и билинейное отображение $$h \colon U \times V \to W.$$
Построим отображение $\hat{h}$ следующим образом: сначала определим $\hat{\hat{h}}\colon K\lan U \times V \ran \to W$, а затем покажем, что оно пропускается через $T$. По самому своему определению $K\lan U \times V\ran$ имеет базисом элементы $(u,v)$. Отображение $\hat{\hat{h}}$ достаточно задать на них. Положим $$\hat{\hat{h}}((u,v))=h(u,v).$$
Покажем, что оно однозначно пропускается через $T$. Как всегда единственность очевидна. Для того чтобы показать, что $\hat{\hat{h}}$ пропускается через $T$ необходимо показать, что все соотношения лежат в ядре $\hat{\hat{h}}$. Но это так, потому что $h$ билинейно! 
\endproof
\ethrm




Мы показали, что тензорное произведение существует. Неплохо бы понять, что тензорное произведение однозначно определено с точностью до изоморфизма. 


\utv Если тензорное произведение существует, то оно единственно с точностью до изоморфизма. Конкретно, пусть $U,V$ -- векторные пространства и есть два пространства $T_1,T_2$ и $i_1 \colon U\times V \to T_1$ и $i_2\colon U\times V \to T_2$, играющие роль тензорного произведения, то существует единственный изоморфизм $h\colon T_1\to T_2$, что $h\circ i_1=i_2$.
\proof Это типичное <<категорное>> доказательство. Пусть есть два пространства $T_1$ и $T_2$ вместе с отображениями $i_1 \colon U\times V \to T_1$ и $i_2 \colon  U\times V \to T_2$, для которых выполнены аксиомы тензорного произведения. Заметим, что так как $i_2$ полилинейное, то есть такое $\hat{i_2} \colon T_1 \to T_2$, что $$\hat{i_2}\circ i_1= i_2.$$ 
Аналогично существует $\hat{i_1} \colon T_2 \to T_1$, что
$$\hat{i_1}\circ i_2= i_1.$$ 
Покажем, что $\hat{i_1} $ и $\hat{i_2}$ взаимно обратны. Действительно, имеем
$$\hat{i_1}\circ \hat{i_2}\circ i_1= \hat{i_1}\circ i_2= i_1,$$
что означает, что отображение $\hat{i_1}\circ \hat{i_2}$ есть то самое единственное отображение $T_1\to T_1$, которое гарантируется благодаря полилинейности $i_1$ и того, что само $T_1$ -- тензорное произведение. Но есть другой кандидат на эту роль -- это $\id_{T_1}$. По единственности 
$$\hat{i_1}\circ \hat{i_2}=\id_{T_1}.$$
Аналогично проверяется равенство для второй композиции.
\endproof
\eutv





Теперь необходимо посчитать что-то про тензорное произведение. Например, научиться считать размерность тензорного произведения и находить его базис.
\thrm Пусть $e_1,\dots,e_n$ базис $U$, а $f_1,\dots,f_m$ -- базис $V$. Тогда $e_i\otimes f_j$ базис $U \otimes V$. В частности, 
$$\dim U \otimes V= \dim U \cdot \dim V.$$ 
\proof Прежде всего заметим, что набор $e_i \otimes f_j$ является порождающей системой для тензорного произведения. Значит оно на самом деле конечномерно. Далее, по определению тензорного произведения,
$$\Hom(U;V, K) \simeq \Hom(U\otimes  V,K).$$
Размерность последнего пространства совпадает с размерностью $U\otimes V$. С другой стороны, полилинейное отображение $h \in \Hom(U,V, K)$ однозначно задаётся $\dim U \cdot \dim V$  параметрами $h(e_i,f_j)$. Комбинируя эти два факта получаем, что размерность $\dim U \otimes V$ есть $\dim U \cdot \dim V$. Отсюда, любая порождающая система такого размера есть базис. В частности, набор $e_i \otimes f_j$.
\endproof
\ethrm


\rm Пространство состояний квантовой системы из $n$ кубит имеет размерность $2^n$. Это ключевой факт, который проясняет потенциальную эффективность квантовых компьютеров: вместо $n$ бит мы имеем дело с $2^n$ чисел -- координатами вектора из $2^n$-мерного пространства.
\erm


Определим теперь тензорное произведение линейных отображений.

\dfn Пусть дана пара линейных отображений $L \colon U_1 \to V_1$ и $S\colon U_2 \to V_2 $. Определим отображение $$L\otimes S \colon U_1\otimes U_2 \to V_1\otimes V_2$$ по  правилу $$(L \otimes S) (u_1\otimes  u_2) = L(u_1)\otimes S(u_2).$$
Отображение с таким свойством единственно.
\edfn

\rm Указанное отображение существует и единственно. Действительно,  отображение $L\otimes S$ должно соответствовать билинейному отображению $L\times S$ заданному правилом $L\times S ((u_1,u_2)) = L(u_1)\otimes S(u_2)$. 
\erm

\rm Пусть заданы наборы линейных отображений $L_1, L_2$ и $S_1, S_2$, так что определены композиции $L_i\circ S_i$. Тогда
$$(L_1\otimes S_1) \circ (L_2 \otimes S_2)=(L_1\circ L_2)\otimes (S_1\circ S_2).$$
\erm 

А как устроена матрица тензорного произведения линейных отображений?


\lm Пусть $L_1 \colon V_1 \to U_1$, а $L_2 \colon V_2 \to U_2$. Пусть $e_1,\dots, e_{n_1}$ базис $V_1$,  $e_1',\dots, e_{n_2}'$ базис $V_2$,  и $f_1,\dots, f_{m_1}$ -- базис $U_1$, а $f_1',\dots, f_{m_2}'$ -- базис $U_2$. 
Упорядочим базисы тензорных произведений -- удобно это сделать, например, в лексикографическом порядке (номер первой координаты важнее).
Тогда матрица  $L_1\otimes L_2$  разобьётся на $n_1m_1$ блоков в каждом из которых будет стоять $ A_{ij} B$, где $i,j$ -- номер блока, а $A$ и $B$ матрицы $L_1$ и $L_2$ соответственно.
\proof Действительно, отображение $A\otimes B$ отправляет $e_i\otimes e'_j$ в 
$$Ae_i \otimes Be'_j = \sum_{k,l} A_{ki}B_{lj} \, f_k\otimes f'_l.$$
\endproof
\elm

\dfn Такая матрица называется кронекеровым или тензорным произведением матриц $A$ и $B$ и обозначается как $A\otimes B$.
\edfn

А что если стартовать с операторов, а не с линейных отображений?

\rm Если есть операторы $A\colon V \to V$ и $B \colon W \to W$. Тогда задан оператор $A\otimes B$ на $V\otimes W$.
\erm

\lm Собственные числа оператора $A\otimes B$  -- это попарные произведения собственных чисел для $A$ и $B$. 
\proof Перейдём к матрицам и, если нужно, будем считать, что мы находимся над алгебраически замкнутым полем. Рассмотрим жорданов базис для $A$ и $B$ -- $v_1,\dots,v_n$ и $u_1,\dots, u_m$. Тогда, рассмотрев базис $v_i\otimes u_j$, заметим, что под диагональю будут стоять нули, а на диагонали -- попарные произведения собственных чисел $A$ и $B$.
\endproof
\elm


 

\dfn[Произведение графов] Пусть $G$ и $H$ -- два графа (возможно ориентированных). Тогда их категорным произведением называется граф чьи вершины есть пары вершин $G$ и $H$ и ребро между парами $(u_1,v_1)$ и $(u_2,v_2)$ проводится только если есть рёбра $u_1 \to u_2$ и $v_1 \to v_2$.

Декартовым произведением графов $G$ и $H$ называется граф на тех же вершинах с ребром между парами если $u_1=u_2$ и есть ребро $v_1\to v_2$ или, симметрично, $v_1=v_2$ и есть ребро $u_1 \to u_2$. Разумеется для неориентированных графов эта конструкция снова выдаёт неориентированный граф.
\edfn


\crl Спектр категорного произведения графов состоит из всех возможных попарных произведений собственных чисел графов.
\proof Заметим, что матрица смежности категорного произведения графов -- это тензорное произведение матриц смежности исходных графов.
\endproof
\ecrl



\zd Чему равен спектр декартового произведения графов?
\ezd

\zd Чему равен спектр графа-решётки
\ezd

А можно ли как-то охарактеризовать тензорное произведение нескольких пространств?

\rm Если рассмотреть тензорное произведение $V_1\otimes \dots \otimes V_n$, то линейные отображения из него в $W$ будут соответствовать полилинейным отображениям $\Hom(V_1;\dots ;V_k, W)$.
\erm

С понятием тензорного произведения связан ряд канонических отождествлений между разными на первый взгляд пространствами в духе изоморфизма $V \simeq V^{**}$.

\thrm Имеют место следующие естественные изоморфизмы: 
$$(U \otimes V) \otimes W \simeq U \otimes V \otimes W \simeq U \otimes (V \otimes W)$$
$$ U \otimes V \simeq V \otimes U $$
$$ \Hom (U,V) \simeq  V \otimes U^*$$
$$(U \otimes V)^{*} \simeq U^{*}\otimes V^{*} $$
$$\Hom (U\otimes V,  W) \simeq \Hom (U, \Hom (V,W))$$
\proof Наиболее интересная часть этой теоремы состоит в бескоординатном построении этих отображений.

Построим отображение $V\otimes U^{*} \to \Hom (U,V)$ по правилу $$v\otimes f \to (u \to f(u)v).$$
Это соответствие полилинейно по $v,f$ поэтому задаёт корректное линейное отображение. Полезно посмотреть, как оно действует на базисных векторах. Пусть $e_i$ базис $V$, $f_j$ -- базис $U$, а $f^j$ базис $U^{*}$. Тогда $e_i\otimes f^j$ соответствует линейное отображение с матрицей 
$$ e_{ij}= \bordermatrix{
 & &j&& \cr
 &0&\cdots&\cdots&0\cr
 &\vdots&\ddots && \vdots\cr
i&\vdots& 1 & \ddots& \vdots\cr
 &0&\cdots& \cdots&0
}$$
Такие линейные отображения образуют базис $\Hom(U,V)$. Значит указанное отображение -- изоморфизм.

\endproof 
\ethrm


\subsection{Тензоры на пространстве и их классическое определение}

Понятие тензора появилось в классической механике для ответа на вопрос, как меняются те или иные величины при замене координат. Для того, чтобы обсудить этот аспект тензоров дадим определение:

\dfn Тензором валентности $(p,q)$ на пространстве $V$ называется элемент пространства ${V^{*}}^{\otimes p} \otimes V^{\otimes q}$. Так же будем говорить, что такие элементы -- это $p$ раз ковариантные и $q$ раз контравариантные тензоры. Тензорами валентности $(0,0)$ называются элементы поля $K$ -- скаляры.
\edfn

Теперь я утверждаю, что более менее все встречавшиеся нам структуры на векторном пространстве $V$ являются тензорами.



\exm\\
1) Вектор $v\in V$ является 1 раз контравариантным тензором.\\
2) Элемент двойственного пространства $f \in V^{*}$ является 1 раз ковариантным тензором. Вообще ковариантными называют тензоры, которые соответствуют полилинейным формам на пространстве $V$. Это историческая традиция. Точнее:\\
3) Так как пространство ${V^{*}}^{\otimes p} \simeq \left(V^{\otimes p}\right)^*\simeq \Hom(V;\dots;V,K)$, то тензор валентности $(p,0)$ соответствует полилинейному отображению $V\times\dots \times V \to K$.\\
4) В частности, тензор валентности $(2,0)$ -- это билинейная форма.\\
5) Линейный оператор -- это элемент $\Hom(V,V)\simeq V^{*}\otimes V$, то есть тензор валентности $(1,1)$.\\
6) Структура алгебры на $V$ (без требования ассоциативности) задаётся билинейным отображением $V \times V \to V$, то есть линейным отображением $V\otimes V \to V$ или же элементом $V^{*}\otimes V^* \otimes V$, то есть тензором типа $(2,1)$.\\

Как записать тензор в координатах? Выберем базис $e_1,\dots,e_n$ пространства $V$ и возьмём в двойственном пространстве двойственный базис $e^1,\dots,e^n$. Теперь построим базис тензорного произведения ${V^{*}}^{\otimes p}\otimes V^{\otimes q}$. Он имеет вид $e^{j_1}\otimes\dots\otimes e^{j_p}\otimes e_{i_1}\otimes \dots \otimes e_{i_q}$. Тогда произвольный тензор $T$ валентности $(p,q)$ имеет вид 
$$ T= \sum_{\substack{i_1,\dots,i_q \in \ovl{1,n}\\ j_1,\dots,j_p \in \ovl{1,n}} } \,T_{j_1,\dots,j_p}^{i_1,\dots,i_q}\,\, e^{j_1}\otimes\dots\otimes e^{j_p}\otimes e_{i_1}\otimes \dots \otimes e_{i_q}.$$
Элементы $T_{j_1,\dots,j_p}^{i_1,\dots,i_q}$ называются координатами тензора $T$. Как меняются координаты тензора при замене базиса? 

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из нового базиса в старый, а $D={C^{\top}}^{-1}$. Тогда координаты тензора $T$ в базисе $\hat{e}$ выражаются через старые координаты следующим образом:
$$\hat{T}_{j_1,\dots,j_p}^{i_1,\dots,i_q}=\sum_{\substack{i'_1,\dots,i'_q \in \ovl{1,n}\\ j'_1,\dots,j'_p \in \ovl{1,n}}} \,\,
\prod_{t\in \ovl{1,p}} D_{j_t,j'_t} \prod_{s\in \ovl{1,q}} C_{i_s,i'_s}  \,\,T_{j'_1,\dots,j'_p}^{i'_1,\dots,i'_q}.$$
\ethrm

Важность тензоров в теоретической физике обуславливается тем, что практически все физические объекты -- это тензоры. Точнее: с точки зрения теории относительности пространство-время это некоторое четырёхмерное многообразие $M$ (в двумерной ситуации подошла бы обычная сфера или тор). С каждой точкой $x$ этого многообразия связано касательное пространство в этой точке -- некоторое четырёхмерное пространство $T_x$. Представим себе, что в каждой точке пространства задана плотность вещества (на самом деле не так, но допустим) -- это даёт вам функцию $f \colon M \to \mb R$ -- скаляр в каждой точке, то есть тензор типа $(0,0)$. 

Направление движения материи можно задать взяв в каждой точке касательный вектор, то есть тензор валентности $(0,1)$ на $T_x$. Дальше, у каждого такого вектора можно считать его <<длину>> и углы между векторами. Для этого надо задать для каждой точки $x$ билинейную форму на касательном пространстве, то есть элемент $T_x^{(2,0)}$. И т.д. Чаще всего такие объекты называют тензорными полями, если хочется подчеркнуть, что в разных точках это тензор вообще говоря на разных пространствах.

Важно, что уравнения в физике не должны зависеть от выбора координат. Можно, конечно, писать какие-то уравнения при помощи координат тензоров и каждый раз проверять, что выбрав новые координаты уравнение будет того же вида. Однако, чем сложнее наука тем сложнее становятся проверки. Становится важно работать с тензорами не рассматривая их координаты. Поэтому на тензорах вводят стандартные операции, которые заведомо не зависят от выбора координат.


\section{Ранг тензора}

Нам хорошо знакомо понятие ранга билинейной формы и ранга линейного отображения. Но и билинейная форма и линейное отображение тесно связаны с понятием тензорного произведения. Можно ли обобщить понятие ранга на произвольные тензоры? Оказывается, что да. 

\dfn Пусть $U_1,\dots, U_n$ пространства и дан $f\in U_1 \otimes \dots \otimes U_n$. Рангом тензора $f$ называется наименьшее такое $r$, что $f=f_1+\dots+f_r$, где $f_i$ -- элементарный тензор.
\edfn

\zd Это определение совпадает с определением ранга для билинейных форм (как элементов $U^*\otimes V^*$) и для линейных отображений (как элементов $U^* \otimes V$).
\ezd

Наибольшую популярность понятие ранга тензора приобрело после работ Штрассена по умножению матриц. А именно, рассмотрим конкретный тензор. Пусть $V$ -- это пространство квадратных матриц размера $k$. Тогда матричное умножение задаёт на $V$ тензор валентности $(2,1)$, то есть элемент $M\in V^*\otimes V^* \otimes V$. Представим себе, что ранг этого тензора равен $r$.

Это означает, что мы можем выбрать такие элементы $f_1,\dots,f_r, g_1, \dots, g_r \in V^*$, и $v_1,\dots,v_r \in V$, что
$$M= \sum f_i\otimes g_i \otimes v_i.$$
Что это означает с точки зрения произведения матриц? Две матрицы -- это два элемента $A,B \in V$. Тогда 
$$AB=\sum f_i(A)g_i(B)v_i$$
$f_i$ и $g_i$ -- это линейные выражения от коэффициентов матриц и значит  $f_i(A)$ и $g_i(B)$ могут быть вычислены при помощи сложения чисел и умножения чисел на фиксированные скаляры. Далее, мы вычисляем произведения $P_i=f_i(A)g_i(B)$. Для этого нужны полноценные операции произведения. После этого, умножаем $P_i$ на $v_i$. Так как коэффициенты $v_i$ постоянны, то для этой операции тоже не нужно честное умножение. Итого, реально используется $r$ настоящих умножений. 

На самом деле, верно и обратное: если если мы найдём формулу для умножения матриц, которая использует только $r$ честных умножений, то можно будет построить такое разложение тензора матричного умножения. 

Пример такого умножения построил Штрассен для матриц $2\times 2$. Пусть 
$$A=\pmat A_{1,1} & A_{1,2} \\ A_{2,1} & A_{2,2} \epmat \text{ а } B=\pmat B_{1,1} & B_{1,2} \\ B_{2,1} & B_{2,2} \epmat.$$
Определим вслед за Штрассеном
$$
\begin{aligned}
&f_1=A_{1,1}+ A_{2,2} &\quad & g_1= B_{1,1}+B_{2,2} &\quad & P_1=f_1\cdot g_1\\
&f_2=A_{2,1}+A_{2,2} &\quad & g_2=B_{1,1}&\quad & P_2=f_2\cdot g_2\\
&f_3=A_{1,1}&\quad & g_3=B_{1,2}-B_{2,2}  &\quad & P_3=f_3\cdot g_3\\
&f_4=A_{2,2}&\quad & g_4=B_{2,1}-B_{1,1} &\quad & P_4=f_4\cdot g_4\\
&f_5=A_{1,1}+A_{1,2}&\quad & g_5=B_{2,2} &\quad & P_5=f_5\cdot g_5\\
&f_6=A_{2,1}-A_{1,1}&\quad & g_6=B_{1,1}+B_{1,2} &\quad & P_6=f_6\cdot g_6\\
&f_7=A_{1,2}-A_{2,2}&\quad & g_7=B_{2,1}+B_{2,2} &\quad & P_7=f_7\cdot g_7.
\end{aligned}
$$


$$
v_1= \pmat 1 & 0 \\ 0 & 1 \epmat,  \quad
v_2=\pmat 0& 0\\0 & -1\epmat,  \quad
v_3=\pmat 0 & 1 \\ 0 & -1 \epmat,  \quad
v_4=\pmat 1& 0\\1 & 0 \epmat,  
$$
$$
v_5=\pmat -1& 1\\ 0& 0\epmat,  \quad
v_6=\pmat 0&0 \\0 & 1\epmat,  \quad
v_7=\pmat 1& 0\\ 0& 0\epmat,  \quad
$$
Элементы $f_i,g_i,v_i$ задают разложение тензора матричного умножения. Если $C=AB$, то для элементов $C$ справедливы формулы
$$ 
\begin{aligned}
&C_{1,1}=P_1+P_4-P_5+P_7\\
&C_{1,2}=P_3+P_5\\
&C_{2,1}=P_2+P_4\\
&C_{2,2}=P_1-P_2+P_3+P_6
\end{aligned}
$$

Элементы $f_i,g_i,v_i$ показывают, что ранг тензорного произведения матриц $2\times 2$ не больше $7$. На самом деле ранг умножения матриц $2\times 2$ действительно равен $7$.

Но как это поможет для умножения матриц больших размеров? Как обычно предположим, что $n=2^k$. Тогда матрицы $A$ и $B$ можно разбить на подматрицы размера $2^{k-1}$ которые мы обозначим как $A_{i,j}, B_{i,j}$ как и раньше. Заметим, что теперь мы снова перемножаем матрицы $2\times 2$, элементы которых на этот раз не числа, а матрицы. 

Если мы посмотрим процедуру умножения матриц, которая произошла из разложения тензора матричного умножения , то увидим, что в этой процедуре мы не использовали коммутативности умножения элементов поля. Действительно, элементы $A$ всегда находились слева в формуле по сравнению с элементами $B$, а элементы $A$ как и элементы $B$ между собой не перемножались.

Значит указанная формула справедлива и для умножения блочных матриц. Мы можем применить указанный способ рекуррентно. Тогда необходимое  число умножений в данном алгоритме будет удовлетворять рекурренте:
$$T(n)=7\cdot T(n/2).$$
То есть всего мы потратим $7^{k}=n^{\log_2 7}$ умножений.

\zd Оцените количество сложений в алгоритме как $6\cdot 7^k$.
\ezd

На текущий момент при помощи оценки рангов (правда не совсем тензора матричного умножения) получены оценки $O(n^{2.373})$ операций для умножений матриц $n\times n$.

Почему мы не знаем окончательного ответа на то, какой ранг у умножения матриц размера $k\times k$? Потому что ранг тензора тяжело считать. Прежде всего тут стоит отметить, что ранг тензора, зависит от того поля, над которым мы этот ранг считаем. 

Например, ранг умножения комплексных чисел над $\mb R$ равен $3$ (а не $4$, как вы могли бы подумать). Но ранг тензора умножения комплексных чисел над $\mb C$ равен $2$ (коэффициенты тензора вещественные, но мы их  рассматриваем как элементы из $\mb C$).  

Так вот. Вычисление ранга тензора над $\mb C$ и над $\mb R$ является NP-трудной задачей. А вычисление ранга тензора над $\mb Z$, то есть когда мы хотим, чтобы все коэффициенты были целочисленными, и вовсе алгоритмически не разрешим. Это есть следствие негативного решения 10 проблемы Гильберта. Вопрос алгоритмической неразрешимости над $\mb Q$ остаётся открытым (как и аналог 10 проблемы Гильберта).




\section{Внешняя и симметрическая алгебры}

В этом разделе мы будем рассматривать векторное пространство $V$ над полем характеристики $0$. Есть общая теория не только для полей, но и для произвольных колец, однако, даже базовые конструкции в этой теории сложнее и некоторые её утверждения просто неверны над полем ненулевой характеристики.

\dfn Определим пространство $\Lambda^k V$ как подпространство $V^{\otimes k}$. Это подпространство выделяется следующими условиями -- для любой перестановки из $\sigma \in S_k$ и любого тензора $a\in \Lambda^k V$ верно, что $\sigma(a)=\sgn(\sigma)a$. Под $\sigma(a)$ подразумевается действие перестановки $\sigma$ на тензор $a$ перестановкой его компонент. Аналогично определяется подпространство $\Sym^k V \leq V^{\otimes^k}$, чьи элементы удовлетворяют свойству: $\sigma(a)=a$.
\edfn

\lm Имеет место проектор $Alt \colon V^{\otimes k} \to \Lambda^k V$ (называется альтернированием) 
$$a \to Alt(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn (\sigma) \sigma(a).$$
Аналогично отображение  симметризации
$$ a \to S(a)= \frac{1}{k!} \sum_{\sigma \in S_k} \sigma(a)$$
есть проектор на подпространство $\Sym^k V$.
\proof Докажем только первую часть. Прежде всего заметим, что $Alt$ принимает значение в подпространстве кососимметричных тензоров. Действительно 
$$\tau(Alt(a))=\frac{1}{k!}\sum_{\sigma \in S_k}\sgn(\sigma) \tau(\sigma(a))= \sgn{\tau} \frac{1}{k!}\sum_{\tau\sigma \in S_k} \sgn(\tau\sigma) \tau(\sigma(a))=\sgn(\tau) Alt(a).$$
Далее, покажем, что для любого кососимметричного тензора $a$ верно, что $Alt(a)=a$. Это покажет, что $Alt$ -- есть проектор и в его образе лежат все элементы из $\Lambda^k(V)$, то есть ровно то, что осталось показать. Итак пусть $a\in \Lambda^k(V)$. Тогда 
$$Alt(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn (\sigma) \sigma(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn^2(\sigma) a=a.$$
\endproof
\elm

\dfn Пусть $e_1,\dots, e_k$ набор элементов из $V$. Определим элементы $e_1\wedge \dots \wedge e_k \in \Lambda^k V$ как образы при проекции $e_1\otimes \dots \otimes e_k$.
\edfn

\thrm Пусть $e_1,\dots, e_n$ базис пространства $V$. Тогда элементы $e_{i_1}\wedge \dots \wedge e_{i_k}$, где $i_1<i_2< \dots < i_k$ образуют базис пространства $\Lambda^k V$. Элемент $e_{i_1}\wedge \dots \wedge e_{i_k}$, где $i_1<i_2< \dots < i_k$  обозначим за $e_{\Gamma}$, где $\Gamma =\{i_1,\dots,i_k\}\subseteq \ovl{1,n}$ подмножество размера $k$. Это взаимооднозначное соответствие. В частности, размерность $\dim \Lambda^k V = C^k_n$. 
\proof Прежде всего заметим, что это действительно порождающая система. Для этого вспомним, что $e_{i_1,\dots,i_k}$ по всем возможным наборам $i_1,\dots,i_k$ порождают тензорное произведение $V^{\otimes k}$. Но раз они порождают тензорное произведение, то они порождают и его образ при проекции $Alt$ на пространство кососимметричных тензоров. Далее заметим, что $$Alt(e_{i_1,\dots i_k})= \sgn(\sigma) Alt(e_{i_{\sigma(1)},\dots i_{\sigma(k)}}).$$
В частности, если в наборе есть два повторяющихся индекса, то элемент проектируется в 0. Далее, это же соотношение даёт, что,  с точностью до знака $Alt$ от тензора для набора $i_1,\dots,i_k$ совпадает с проекцией для упорядочевания этого набора. Таким образом, из набора образующих можно исключить неупорядоченные наборы и наборы с повторениями, что и требовалось.

\noindent Покажем линейную независимость. Пусть  
$$\sum_{\Gamma} \alpha_{\Gamma} e_{\Gamma}=0.$$
Тогда расписывая эту сумму через $e_{i_1}\otimes \dots \otimes e_{i_k}$ -- базисные элементы $V^{\otimes k}$ получаем
$$\frac{1}{k!}\sum_{\Gamma} \alpha_{\Gamma} \sum_{\sigma \in S_{\Gamma}} \sgn(\sigma) e_{\sigma(i_1),\dots,\sigma(i_k)} =0 $$
Заметим, что все слагаемые соответствуют разным базисным элементам. Тогда, все коэффициенты равны нулю. В частности, коэффициенты при $ e_{i_1, \dots, i_k}$, которые равны $\frac{1}{k!}\alpha_{\Gamma}$. 

\endproof
\ethrm

\thrm Аналогично, пусть $e_1,\dots, e_n$ базис пространства $V$. Тогда элементы образы тензоров $e_{i_1}\otimes \dots \otimes e_{i_k}$, где $i_1\leq \dots \leq i_k$ образуют базис пространства $\Sym^k V$.
\ethrm

\dfn Определим $k$-ую внешнюю степень линейного отображения $L\colon V \to W$ -- отображение $\Lambda^{k} L  \colon \Lambda^k V \to \Lambda^k W$ заданное на тензорах по правилу $v_1\wedge \dots \wedge v_k \to L v_1 \wedge \dots \wedge L v_k$. 
\edfn

Для того, чтобы показать корректность такого определения покажем следующую теорему:

\thrm Рассмотрим отображение $g=Alt \circ i \colon V^{\times k} \to \Lambda^k(V)$. Тогда для любого полилинейного кососимметричного $h \colon V^{\times k} \to U$ существует единственное отображение $\hat{h} \colon \Lambda^k(V) \to U$, что $\hat{h} \circ g = h$.
\proof Зафиксируем подходящее $h \colon V^{\times k} \to U$. Так как это полилинейное отображение, то есть линейное $\hat{\hat{h}}\colon V^{\otimes k} \to U$, что $\hat{\hat{h}} \circ i =h$. Покажем, что ограничение $\hat{\hat{h}}$ на $\Lambda^k(V)$ есть искомое отображение. Действительно $$\hat{h}(v_1\wedge\dots\wedge v_k)=\hat{\hat{h}}(v_1\wedge\dots\wedge v_k)=\hat{\hat{h}} \left(\frac{1}{k!} \sum_{\sigma \in S_k} \sgn(\sigma) v_{\sigma(1)}\otimes \dots \otimes v_{\sigma(k)} \right) =$$
$$=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn(\sigma) h(v_{\sigma(1)},\dots,v_{\sigma(k)})=\frac{1}{k!}k!h(v_1,\dots,v_k)=h(v_1,\dots,v_k).$$

Осталось показать единственность. Для этого заметим, что из условия  $\hat{h}$ однозначно задано на базисе $e_{\Gamma}$.
\endproof
\ethrm


\fct Полезно смотреть не на пространства $\Lambda^k (V)$ и $\Sym^k V$, а на пространства $\Lambda^k(V^*)$ и $\Sym^k(V^*)$, потому что они допускают привычную и наглядную интерпретацию --- их элементы это полилинейные функции со специальными свойствами.
\efct


\exm \\
1) Элемент $\Lambda^2(V^*)$ --- это просто кососимметрическая билинейная форма.\\
2) А элемент $\Sym^2 V^*$ -- это симметрическая билинейная форма или просто квадратичная форма.\\
3) Элемент $\Lambda^{\dim V} V^*$ -- это просто форма объёма на $V$.\\

Мы хотим ввести умножение на кососимметричных тензорах. Есть два подхода: первый из них -- потребовать, чтобы пара $v_1 \wedge \dots \wedge v_p , u_1\wedge \dots \wedge u_q$ переходила в $v_1 \wedge \dots \wedge v_p \wedge u_1\wedge \dots \wedge u_q$. С таким подходом возникает вопрос о корректности. С другой стороны, такое умножение удобно вычислять. Второй подход -- связать это умножение с формальным умножением тензоров. Проблема возникает в следующем -- если $T_1$ и $T_2$ -- кососимметрические тензоры, то $T_1\otimes T_2$, вообще говоря, не кососимметричный. Мы доведём до конца второй путь.

\thrm[Внешняя алгебра] Рассмотрим пространство $\Lambda(V)=\oplus_{k=0}^{\dim V} \Lambda^k(V)$ и введём на нём структуру ассоциативной алгебры по правилу $ f\wedge g= Alt(f\otimes g)$. Если $f\in \Lambda^p(V)$, а $g \in \Lambda^q(V)$, то $f\wedge g=(-1)^{pq}g \wedge f$. Такое свойство называется градуированной коммутативностью. Более того, пара $v_1 \wedge \dots \wedge v_p , u_1\wedge \dots \wedge u_q$ переходит при этом умножении в $v_1 \wedge \dots \wedge v_p \wedge u_1\wedge \dots \wedge u_q$. Такое умножение называется внешним произведением тензоров.
\proof Для этого удобно проверить тождество $Alt(Alt(T_1)\otimes T_2)= Alt(T_1\otimes T_2)= Alt(T_1 \otimes Alt(T_2))$, которое говорит, что внутри альтернирования можно свободно альтернировать сомножители не боясь ничего поменять. Действительно
$$\frac{1}{k!}\sum_{\sigma \in S_{k}}\sgn(\sigma) Alt(T_1^{\sigma}\otimes T_2)=\frac{1}{k!}\sum_{\sigma \in S_{k}} \sgn^2(\sigma) Alt(T_1\otimes T_2)=Alt(T_1 \otimes T_2).$$
Аналогично получается второе равенство. Теперь видно, что 
$$Alt(v_1 \wedge \dots \wedge v_p \otimes u_1\wedge \dots \wedge u_q)=Alt(v_1 \otimes \dots \otimes v_p \otimes u_1\wedge \dots \wedge u_q)=$$
$$=Alt( v_1 \otimes \dots \otimes v_p \otimes u_1\otimes \dots \otimes u_q) =v_1 \wedge \dots \wedge v_p  \wedge u_1\wedge \dots \wedge u_q .$$
Это показывает связь нашего определения умножения с ожидаемым определением. Ассоциативность теперь легко проверить на базисных элементах, как и градуированную коммутативность.
\endproof
\ethrm





\thrm[Симметрическая алгебра] Рассмотрим пространство $\Sym(V)=\oplus_k \Sym^k(V)$. Тогда на нём можно ввести структуру ассоциативной коммутативной алгебры задав умножение как $ f*g= S(f\otimes g)$. Более того, указанная алгебра изоморфна алгебре многочленов.
\ethrm



Изначально, внешняя алгебра была нужна для <<исчисления подпространств>> в пространстве $V$. А именно, подпространству $U\leq V$ размерности $k$ можно сопоставить прямую $\Lambda^k U$ в $\Lambda^k V$. Более того, задав на $U$ форму объёма можно выбрать на этой прямой определённую точку. Такие объекты теперь можно перемножать и складывать, хотя в общем случае может получиться и объект, не соответствующий никакому подпространству. 

Сейчас понятие внешней алгебры может служить удобным формализмом для определения интегрирования. А именно, по многообразию размерности $n$ можно проинтегрировать кососимметричное тензорное поле валентности $(n,0)$, то есть заданную в каждой точке многообразия форму объёма на касательном пространстве. В общем же виде кососимметричные тензорные поля типа $(k,0)$ называются дифференциальными формами порядка $k$. На таких дифференциальных формах задана операция взятия дифференциала, делающая из $k$-формы $k+1$-форму. Это ещё одна из канонических <<бескоординатных>> операций. Об этом вам расскажут в курсе анализа.

\subsection{Внешняя алгебра и определитель}

Покажем способ применения внешней степени для доказательства тождеств про определители. Введём обозначение.

\dfn Пусть $A$ -- матрица из $M_{m\times n}(K)$. Тогда если $\Gamma \subseteq \{1,\dots,n\}$. Тогда за $A_{\Gamma}$ обозначим матрицу состоящую из столбцов матрицы $A$ с элементами из $\Gamma$. Аналогично, если $\Gamma \subseteq \{1,\dots,m\}$ то за $A^{\Gamma}$ обозначим подматрицу $A$, из строк, чьи индексы лежат в $\Gamma$.
\edfn

Посмотрим, как устроены координаты внешнего произведения произвольного набора векторов. Пусть $e_1,\dots,e_n$ -- базис $V$, $v_1,\dots,v_k$ --  произвольный набор векторов. Пусть $A$ -- это матрица координат векторов $v_1,\dots,v_k$.
Рассмотрим $v_1 \wedge \dots \wedge v_k$. Это полилинейная кососимметрическая функция от $v_1,\dots,v_k$, а вместе с ней и координаты этого произведения. Ясно, что координаты при $e_{\Gamma}=e_{i_1}\wedge\dots\wedge e_{i_k}$ зависят только от строк матрицы $A$ с номерами из $\Gamma$. Если в этих строках стоит единичная матрица, то ясно, что координата при $e_{\Gamma}$ -- это единица. Вспоминая, что есть определитель, получаем, что в общем виде коэффициент при $e_{\Gamma}$ это определитель $A^{\Gamma}$. Итого 
$$v_1 \wedge \dots \wedge v_k= \sum_{\substack{\Gamma \subseteq \{1,\dots,n\}\\ |\Gamma|=k}} \det A^{\Gamma} e_{\Gamma}.$$

Несложно получить эту формулу непосредственно.

Пусть есть отображение $L \colon U \to V$, где $\dim U= \dim V = n$ и $A$ матрица $L$ в базисах $e_1,\dots e_n$ и $f_1,\dots,f_n$. Тогда $$\Lambda^n L(e_1\wedge \dots \wedge e_n) = \det A \,\,f_1 \wedge \dots \wedge f_n.$$
Из этого замечания уже легко получить мультипликативность определителя. Поступая аналогично можно доказать более общую теорему:



\thrm[Формула Бине-Коши] Рассмотрим две матрицы $A\in M_{m\times n}(K)$ и $B\in M_{n\times m}(K)$. Пусть $m\leq n$. Тогда
$$\det(AB)=\sum_{\substack{\Gamma \subseteq \{1,\dots,n\}\\ |\Gamma|=m}} \det A_{\Gamma} \det B^{\Gamma}.$$
\proof Рассмотрим линейные отображения, заданные матрицами $A\colon K^n \to K^m$ и  $B \colon K^m \to K^n$. Тогда $\Lambda^m (AB)$ есть оператор домножения на $\det AB$. С другой стороны $\Lambda^m(AB)=\Lambda^m(A) \Lambda^m(B)$. Вычислим матрицы этих отображений. Матрица $\Lambda^m(B)$ есть столбец высоты $C_n^m$, чьи элементы проиндексированы $\Gamma \subseteq \{1,\dots,n\}$ размера $m$. Аналогично матрица $\Lambda^m(A)$  есть строчка, чьи элементы проиндексированы аналогично.
Пусть $e_1,\dots,e_m$ -- стандартный базис $K^m$, $e=e_1\wedge \dots \wedge e_m$ единственный базисный элемент $\Lambda^m(K)$, а $f_1,\dots,f_n$ -- стандартный базис $K^n$. Тогда 
$$\Lambda^m(B)(e)=\sum_{\substack{\Gamma \subseteq \ovl{1,n} \\ |\Gamma|=m}} \det B^{\Gamma} f_{\Gamma}.$$
 С другой стороны, 
$$\Lambda^m(A)(f_{\Gamma})=\det(A_{\Gamma})e.$$
Осталось перемножить строчку на столбец.
\endproof
\ethrm

Покажем, как можно использовать формулу Бине-Коши. Вначале докажем общие факты про графы.

\dfn Определим матрицу оператора Лапласа для графа $G$ как $L(G)=D-A(G)$.
Ориентируем произвольным образом рёбра графа $G$. Тогда определим ориентированную матрицу инцидентности, как матрицу, строки которой соответствуют вершинам $G$, столбцы -- рёбрам $G$. В столбце соответствующем ребру стоит $1$ в позиции, конца этого ребра и $-1$ в позиции начала. 
\edfn

\lm Выполнено равенство $L(G)=B^\top B$ при любой ориентации рёбер графа.
\elm

Это сразу говорит нам, что матрица $L(G)$ неотрицательно определена. Однако у матрицы $L(G)$ всегда нетривиальное ядро. Действительно, столбец $(1,\dots,1)^\top$ лежит в ядре $L(G)$. Можно сказать точнее:

\lm Пусть у графа $G$ ровно $k$ компонент связности. Тогда $\dim \Ker L(G)\geq k$. На самом деле, выполнено равенство.   
\elm

\crl Если граф $G$ не связен, то алгебраическое дополнение любого элемента в матрице $L(G)$ равно $0$.
\ecrl

\lm Пусть граф $G$ -- дерево. Тогда алгебраическое дополнение любого элемента матрицы $L(G)$ равно 1.
\elm
\proof Пусть нам дан элемент с индексами $i,j$ в $L(G)$. Если $B$ -- ориентированная матрица инцидентности графа $G$, то этот определитель вычисляется как произведение  $\det B^{\ovl{i}}$ и $\det B^{\ovl{j}}$. Сведём вычисление каждого из этих определителей к ситуации, когда граф $G$ -- путь, а выкинута первая строка. Для того, чтобы разобраться с номером вершины просто подействуем перестановкой Для этого заметим, что оба определителя не меняются при элементарных преобразованиях столбцов $B$.  
\endproof

\crl[Теорема о деревьях] Количество остовных деревьев в графе $G$ есть алгебраическое дополнение любого элемента матрица $L(G)$.
\ecrl
\proof Прежде всего отметим, что если $G$ не связен, то и любой минор $L(G)$ равен $0$. $L(G)=BB^\top$, где $B$ -- ориентированная матрица смежности графа $G$. Найти какое-то алгебраическое дополнение $L$ -- это то же самое, что найти определитель $\hat{B}\hat{B}^\top$, где $\hat{B}$ -- матрица $B$ с одной выкинутой строкой. Определитель $\hat B \hat{B}^\top$ легко считается по формуле Бине-Коши. В этой формуле мы должны выбрать $\hat{B_{\Gamma}}$, то есть $n-1$ строк $\hat{B}$ и найти
$$(\det \hat{B}_{\Gamma})^2=\det(\hat{B}_{\Gamma} \hat{B}_{\Gamma}^\top)$$
Но $\det \hat{B}_{\Gamma} \hat{B}_{\Gamma}^\top$ -- это минор матрицы Лапласа графа, чьи рёбра соответствуют строкам матрицы $B$. Если этот граф  -- не дерево, то он не связен и этот определитель равен $0$. Если это дерево, то оно остовное и этот определитель равен единице. Так как по формуле Бине-Коши для вычисления определителя надо просуммировать такие слагаемые, то мы получаем ровно количество остовных деревьев.
\endproof


\crl[Формула Кэли] Количество различных помеченных деревьев на $n$ вершинах равно $n^{n-2}$.
\ecrl


\chapter{Многочлены}

Настала пора вернуться к незаконченной теме про делимость. В этой связи мы снова вернёмся к предположению, что все кольца коммутативны. На этот раз мы поговорим подробно о кольце многочленов от $n$ переменных. Мы знаем, что кольцо многочленов $K[x]$ над полем $K$ является областью главных идеалов и вывели из этого однозначность разложения на множители в $K[x]$. Есть ли надежда повторить тоже самое для многочленов от двух и более переменных? 

Ответ на этот вопрос даёт следующий пример: рассмотрим идеал, порождённый независимыми переменными $x,y$ в кольце $K[x,y]$. Этот идеал нельзя породить одним элементом.

Однако мы интуитивно представляем то, что разложение многочленов на множители однозначно. Математически такое свойство кольца называлось факториальностью.

\dfn
Область целостности $R$ называется факториальным кольцом, если для любого $a\in R$, что $a\neq 0$  существует представление $a=\eps p_1\dots p_k$, где $\eps \in R^*$, а $p_1,\dots,p_k$ -- простые элементы $R$.
\edfn

\rm Исходя из определение простоты, такое разложение единственно с точностью до ассоциированности. Действительно, если $f=\eps\prod p_i= \delta \prod q_j$, то $p_1 \di \prod q_j$ и благодаря простоте делит скажем $q_1$. Но тогда $p_1h=q_1$, откуда, благодаря неприводимости $q_1$ получаем, что $h$ обратим, то есть, что $p_1 \sim q_1$. Тогда можно сократить на $p_1$ и продолжить по индукции.
\erm


\rm В факториальном кольце есть НОД любых двух элементов (естественно, с точностью до ассоциированности).
\erm


Итак, наша ближайшая цель поговорить о факториальности колец многочленов. Однако, для приложений к теории чисел нам будет необходимо разработать теорию в достаточной общности, чтобы применить её и над $\mb Z$, а не только над полем. Заметим, что $\mb Z$ и поле $K$ являются факториальными кольцами. Мы покажем, что факториальность кольца влечёт факториальность кольца многочленов над ним, что полностью ответит на наши вопросы.

Для этого доказательства нам понадобится конструкция факторкольца, важность которой мы почувствуем позднее.

\dfn Пусть $R$ кольцо, а $I\leq R$ -- идеал. Тогда на факторгруппе $R/I$ можно ввести структуру кольца определив умножение по формуле $\ovl{a}\ovl{b}=\ovl{ab}$. 
\edfn

\dfn Идеал $I$ называется простым, если выполнено, что $ab\in I$, то $a\in I$ или $b\in I$.
\edfn

\rm Элемент $p\in R$ отличный от нуля прост тогда и только тогда, когда идеал $(p)$ прост, тогда и только тогда, когда $R/(p)$ -- область целостности.
\erm


\section{Многочлены над факториальным кольцом}

Наша задача обсудить, что происходит с кольцом многочленов от многих переменных.


\lm[Гаусс] Пусть $R$ -- кольцо. Тогда любой простой элемент $p$ из $R$ остаётся простым в $R[x]$.
\proof
Теоретически удобно воспользоваться следующим соображением: чтобы показать, что элемент прост надо показать, что идеал $(p)$ в $R[x]$ прост, а для этого необходимо и достаточно установить, что $R[x]/(p)$ есть область целостности. Как же описать $R[x]/(p)$? Я утверждаю, что оно изоморфно $(R/p)[x]$. Действительно, из $R[x]$ есть отображение в $(R/p)[x]$, которое берёт все коэффициенты по модулю $p$. Очевидно, в его ядре лежат ровно те многочлены, все коэффициенты которых делятся на $p$, то есть многочлены кратные $p$ в $R[x]$. Но ровно они и образуют идеал $(p)$. Осталось заметить, что кольцо $R/p$ и, вслед за ним, кольцо $R/p[x]$ являются областями целостности.

У этого доказательства есть другая, более элементарная реинкарнация. А именно, формально нам надо доказать, что если произведение двух многочленов $f(x)g(x)$ делится на $p$ (то есть все коэффициенты кратны $p$), то тогда какой-то из них делится на $p$. Пусть это не так. Возьмём тогда у $f$ и у $g$ самые младшие коэффициенты $a_i$ и  $b_j$, которые не делятся на $p$. Тогда посмотрим на коэффициент с номером $i+j$  в произведении. Он имеет вид $c_{i+j}= a_ib_j + \sum_{k \neq i} a_k b_{i+j -k}$. Я утверждаю, что $c_{i+j}$ не делится на $p$. Для этого заметим, что любое слагаемое в сумме делится на $p$, так как либо $k<i$ и тогда $a_i \di p$, либо $k>i$, то есть $i+j-k<j$ и следовательно $b_j \di p$. Противоречие с тем, что $c_{i+j}$ должен делиться на $p$.   
\endproof
\elm

\upr Поясните, почему оба доказательства одинаковы.
\eupr

\dfn Пусть $f(x)$ -- многочлен над факториальным кольцом $R$. Тогда содержанием $f$ называется $\cnt(f)=\Nod (a_i)$, где $a_i$ коэффициенты $f$. 
\edfn

Следующее следствие тоже называют леммой Гаусса.

\crl Если $f(x)=g(x)h(x)$, где $f,g,h \in R[x]$, то $\cnt(f)=\cnt(g)\cnt(h)$
\proof Для начала, упростим задачу, то есть сведём задачу к случаю $\cnt g= \cnt h =1$. Для этого надо рассмотреть многочлены $\frac{g}{\cnt g}$ и $\frac{h}{\cnt h}$. Их произведение есть $\frac{f}{\cnt{g}\cnt{h}}$ имеет содержание $\frac{\cnt f}{\cnt g \cnt h}$ и если показать его единичность, то мы добьёмся требуемого. Итак считаем, что $\cnt g= \cnt h=1$. Если $\cnt f$ не обратим, то $\cnt f \di p$, где $p$ простой элемент из $R$. Но тогда один из $g$ или $h$ делится на $p$ благодаря простоте $p$. 
\endproof
\ecrl


\lm Пусть для многочлена $f(x) \in R[x]$  имеет место разложение $f(x)=g(x)h(x)$, где  $g(x), h(x) \in Q(R)[x]$. Пусть $c \in Q(R)^*$, такая что $cg \in R[x]$ и $\cnt(cg)=1$. Тогда $c^{-1}h \in R[x]$, что означает, что $f(x)=cg(x)c^{-1}h(x)$ -- есть произведение двух многочленов из $R[x]$ пропорциональных исходным.
\proof Для начала отметим, что такая константа $c$ существует. Заменяя $g$ на $cg$ можно считать, что $c=1$. В этом предположении нам надо доказать, что $h\in R[x]$.
Домножим $h$ на $d\in R$, так что $dh \in R[x]$. Тогда $df=g dh$ и значит
$$d\cnt f = \cnt(g) \cnt(dh)=\cnt(dh).$$
Таким образом, коэффициенты $dh$ делятся на $d$, а значит исходный $h$ был из $R[x]$.
\endproof
\elm


\thrm Пусть $R$ -- факториальное кольцо. Тогда кольцо $R[x]$ факториально. Более того, имеет место следующее описание простых элементов кольца $R[x]$ с точностью до ассоциированности:\\
1) $f\in R[x]$ такой, что $cont(f)=1$ и $f$ неприводим в $Q(R)[x]$.\\
2) $f=p \in R$ -- простой в $R$.
\proof 
Для начала покажем, что все указанные ситуации приводят к простым элементам в кольце $R[x]$.
Итак, пусть $f \in R[x]$ неприводим в $Q(R)[x]$ и $cont(f)=1$. Если $gh\di f$, то это же верно над $Q(R)$ и, можно считать например, что $g\di f$ в $Q(R)[x]$. Тогда $g= fh$. Тогда $h\in R[x]$, по лемме. Значит, $g \di f$ в $R[x]$. Второй случай полностью следует из леммы Гаусса.


Теперь покажем, что любой элемент раскладывается в произведение указанных простых. Для этого сначала разложим $f$ в $Q(R)[x]$ в произведение неприводимых $f=\prod g_i$, $g_i \in Q(R)[x]$. Далее сделаем из $g_i$ элементы $\hat{g}_i$ из $R[x]$ с $cont(g_i)=1$. Тогда $f=a\prod \hat{g}_i$, где $a\in Q(R)$. Заметим, что $a\in R$ по лемме. Осталось разложить $a$ на простые из $R$.
\endproof
\ethrm








\section{Признаки неприводимости для многочленов}

Теперь наша задача поговорить про неприводимость многочленов над целыми числами или над $\mb Q$. 
Прежде всего отметим, что обе задачи тесно связаны. А именно, если взять многочлен с рациональными коэффициентами, то домножив его на подходящую рациональную константу мы получим многочлен с целыми коэффициентами и содержанием 1, который по доказанному ранее неприводим тогда и только тогда, когда неприводим исходный. Обратно, неприводимость целочисленных многочленов интересна только в случае, когда содержание этих многочленов равно единице. А в этом случае это эквивалентно рациональной неприводимости. Однако все теоремы я буду формулировать в общем контексте.



\thrm[Редукционный критерий] Пусть $R$ факториальное кольцо, $f \in  R[x]$ многочлен, а $p$ -- простой элемент. Тогда, если старший коэффициент $f$ не делится на $p$ и $\ovl{f}$ неприводим в кольце $R/p[x]$, то он неприводим над $Q(R)$. 
\proof Прежде всего перейдём от многочлена $f$ к $\frac{f}{cont(f)}$ с содержанием равным 1. Достаточно доказать неприводимость  последнего над $Q(R)$, которая эквивалентна его неприводимости над $R$. Итак пусть $cont(f)=1$ и пусть $f=gh$, где $g,h$ --  не константы. Старшие коэффициенты $g$ и $h$ тоже не делятся на $p$. Имеем $\ovl{f}= \ovl{g}\ovl{h}$ и $\deg g = \deg \ovl{g}$ и $\deg h = \deg \ovl{h}$, что даёт нетривиальное разложение $\ovl{f}$ и приводит к противоречию.
\endproof
\ethrm

Вот примеры о том, как пользоваться этим критерием и что не надо забывать про условие со старшим коэффициентом. 


\exm\\
1) Многочлен $x^3+x+1$ неприводим над $\mb F_2=\mb Z/2$, потому что у него нет корней. Следовательно многочлены $3x^3+8x^2+5x+7$ и скажем, $5x^3-4x^2+x+15$ неприводимы над $\mb Q$.\\
2) Рассмотрим многочлен $px^2+x$. Он приводим, но по модулю $p$ -- неприводим.\\
3) Критерий из теоремы сформулирован не в самом сильном виде. А именно, представим себе, например, что по модулю 2 многочлен степени пять разложился в произведение двух неприводимых степени 2 и 3, а по модулю 3 -- в виде произведения степени 4 и 1. Ясно, что он неприводим.\\
4) Не стоит забывать, что если многочлен неприводим над $\mb R$, то он так же неприводим над $\mb Q$. Это, правда, очень слабый критерий, но в комбинации с пунктом 3) может что-то дать.\\
5) Есть, однако, такие многочлены, которые неприводимы, но раскладываются по модулю любого простого. Например, $x^4+1$.\\


Покажем теперь некоторый критерий неприводимости, который применим в случае, если разложение по модулю $p$ получилось неудачное. А именно, представим себе, что $f(x) \equiv c x^n \mod p$. То есть многочлен развалился в произведение максимально возможного числа одинаковых множителей. Оказывается, что в этом случае неприводимость многочлена $f$ зависит от его класса по модулю $p^2$. Точнее:

\thrm[Признак Эйзенштейна] Пусть $R$ -- факториальное кольцо и $f(x)= a_0 + \dots + a_n x^n$. Если $a_n \ndi p$, все $a_i \di p$ при $i<n$, но $a_0\ndi p^2$, то многочлен $f(x)$ неприводим.
\proof
Предположим, что $f=gh$. Тогда $c x^n=\ovl{f}=\ovl{g}\ovl{h}$ в $R/p[x]$ и значит в $Q(R/p)[x]$. Но в поле разложение на неприводимые однозначно. Отсюда $\ovl{g}=ax^k$ и $\ovl{h}=b x^l$. Но тогда их младшие члены делятся на $p$ и, значит, $a_0\di p^2$. Противоречие. 
\endproof
\ethrm 

\upr Распишите это доказательство на языке элементов.
\eupr

Всё, что мы пока обсуждали не говорит ничего о том, как же разложить многочлен на неприводимые множители. Первое, что мы обсудим -- вопрос, почему эта задача в принципе разрешима.

Итак, пусть есть целочисленный многочлен $f(x)$ и мы хотим разложить его на множители. Мы будем искать разложение на целочисленные многочлены. Заметим, что хотя бы один из них обязан иметь степень меньшую, чем $[\frac{n}{2}]$. Вспомним о задаче интерполяции. Если $g$ -- искомый делитель $f$, то $g$ определяется своими значениями в $[\frac{n}{2}]+1$ точке, например в точках $0,1,\dots, [\frac{n}{2}]$. Более того, $f(i) \di g(i)$. Таким образом, набор $g(0),\dots, g([\frac{n}{2}])$ состоит из делителей $f(0),\dots,f([\frac{n}{2}])$. Найти все такие наборы -- конечный перебор. По каждому набору кандидат на делитель $g$ восстанавливается однозначно.

Прежде чем продвинуться дальше в исследовании разложения многочленов от одной переменной на множители стоит немного поговорить о задаче разложения многочленов от нескольких переменных. Сейчас мы увидим ещё один трюк от Кронекера, который позволит свести эту задачу к предыдущей.

\thrm Пусть $R$ -- кольцо. Тогда различным разложениям $f(x_1,\dots,x_n)\in R[x_1,\dots,x_n]$   соответствуют различные разложения $\hat{f}=f(x, x^d, x^{d^2}, \dots, x^{d^{n-1}})$ для $d$ больших $\max_{i=1}^n \{\deg_{x_i} f\}$.
\proof Пусть $f=g_1h_1=g_2h_2$ и пусть $g_1\neq g_2$. Покажем, что $\hat{g_1}\neq \hat{g_2}$. Для этого посмотрим что происходит с мономом $x^{\alpha}$ при указанном преобразовании если $\alpha_i < d$. Этот моном переходит в многочлен $x^{\alpha_1+\alpha_2d+\dots+\alpha_n d^{n-1}}$. Так как по условию все $\alpha_i<d$, то такая степень  $x^{\alpha_1+\alpha_2d+\dots+\alpha_n d^{n-1}}$ может быть получена при подстановке только из монома $x^{\alpha}$. Заметим теперь, что $\deg_{x_i} g_j \leq \deg_{x_i} f <d$. Следовательно мономы многочленов $g_j(x)$ однозначно восстанавливаются по мономам $\hat{g_j}$.
\endproof
\ethrm

Заметим так же, что в теореме указан и метод по которому из многочлена $\hat{g}$ можно восстановить многочлен $\hat h$,
К сожалению, не стоит ожидать взаимооднозначного соответствия между разложениями многочленов $f$ и $\hat{f}$. Например, многочлен $x_2^2$ раскладывается на два множителя одним способом. При $d=3$ его образ есть $x^6$ у которого 3 различных разложения.

Это очень неэффективный алгоритм разложения многочлена на множители. Он был предложен Кронекером ещё в 19-ом веке. В настоящее время известен полиномиальный алгоритм решения этой задачи (см. \href{http://www.math.leidenuniv.nl/%7Ehwl/PUBLICATIONS/1982f/art.pdf}{LLL-алгоритм}). Обсудим, как этот алгоритм работает.

Для проверки неприводимости мы с успехом использовали информацию, полученную из разложения по модулю $n$. Вопрос -- нельзя ли её же использовать и в целочисленной задаче? 

Во-первых, если взять достаточно большой модуль $n$, заметно больший, чем коэффициенты в целочисленном разложении, то разложение $f$ по модулю $n$ с маленькими коэффициентами однозначно будет определять кандидата на целочисленное разложение. Это соображение встречается сразу с двумя проблемами -- первая -- не ясно какие есть ограничения на коэффициенты сомножителей, вторая -- разложений по модулю $n$ может быть много и нет способа эффективно искать их.



Продвинемся в решении второго вопроса. Как же  выбрать достаточно большое число, по модулю которого удобно раскладывать многочлен $f$ на множители? Наибольшим удобством в решении задачи разложения обладают поля. В этом смысле возможно стоило бы искать разложение $f$ по модулю очень большого простого. Однако найти большое простое число довольно тяжело. Смотреть по модулю маленьких простых а потом пытаться склеивать разложение в духе китайской теоремы об остатках может банально не получиться (как в примере 3 -- неясно во что склеить два разных разложения). Оказывается наиболее оптимальный вариант такой: взять небольшое простое число $p$, разложить $f$ над $\mb Z/p$ а затем <<поднять>> это разложение по модулю $p^k$ для достаточно большого $k$. Поясним как делать подъём.

\lm[Гензеля] Пусть $f \in \mb Z[x]$, со старшим коэффициентом 1. Пусть $\ovl{f}=gh$ в кольце $\mb Z/p[x]$, причём $(g,h)=1$ и у $g,h$ тоже единичные старшие коэффициенты. Тогда  для любого $k\geq 1$ существуют единственные по модулю $p^k$ многочлены $\hat{g}, \hat{h} \in \mb Z[x]$, cо старшим коэффициентом $1$, что
$$\ovl{f}=\hat{g} \hat{h} \mod p^k,\quad  \hat{g}\equiv g \mod{p}, \quad \hat{h}\equiv h \mod{p} \quad \text{и} \quad \deg h= \deg \hat{h}, \quad \deg g= \deg \hat{g}.$$
\proof Докажем это индукцией по $k$. Пусть по модулю $p^{k}$ уже построены подходящие многочлены $\hat{h}$ и $\hat{g}$ и мы хотим построить $\ovl{h}$ и $\ovl{g}$. Заметим, что благодаря единственности по модулю $p^k$, такие $\ovl{g}$ и $\ovl{h}$ обязаны совпадать с $\hat{h}$ и $\hat{g}$ по модулю $p^k$. Это означает, что по модулю $p^{k+1}$ 
$$\ovl{h} \equiv \hat{h}+p^ka(x)\pmod{p^{k+1}} \text{\,\, и\quad } \ovl{g} \equiv \hat{g} + p^kb(x)\pmod{p^{k+1}}.$$
Заметим, что многочлены $a(x)$ и $b(x)$  однозначно определяются по модулю $p$,  если известны $\ovl{g}$ и $\ovl{h}$ и по модулю $p$ должны иметь степени меньше чем степени $h(x)$ и $g(x)$ соответственно, для того, чтобы не изменить старший коэффициент. Покажем, что такие $a(x), b(x)$ существуют и единственны по модулю $p$. Заметим, что необходимо проверить лишь условие $f \equiv \ovl{g}\ovl{h} \pmod{p^{k+1}}$. Распишем
$$f\equiv \hat{g}\hat{h} + p^{k}(a(x)g + b(x)h) \pmod{p^{k+1}}.$$
Здесь мы заменили $\hat{h}$ и $\hat{g}$ по модулю $p$ и получили исходные многочлены $g$ и $h$ из $\mb Z/p[x]$. Заметим, что есть единственный такой многочлен $c(x)\in\mb Z/p[x]$, что $f-\hat{g}\hat{h}=p^kc(x) \pmod{p^{k+1}}$. Теперь для выполнения сравнения выше необходимо, чтобы  $$c(x)=a(x)g(x)+b(x)h(x)$$
У такого сравнения есть единственное решение в $\mb Z/p[x]$ при условии $\deg a(x)<\deg h(x)$ и $\deg b(x)< \deg g(x)$. Что и требовалось.
\endproof
\elm

Частным случаем разложения на множители служит разложение вида $f(x)=(x-x_1)g(x)$, соответствующее наличию корня. Именно его мы и разбирали раньше. 

\rm Стоит отметить, что условие на старший коэффициент $f$ не является обременительным, так как его легко изменить при помощи линейной замены переменных над $\mb Q$ и домножения на подходящую константу.
\erm


\section{Дополнительно: оценка на коэффициенты делителя}

Сначала мы поборемся с первым осложнением. Заведём  на пространстве многочленов скалярное произведение $$\lan f(x), g(x)\ran = \sum a_ib_i,$$ где $a_i$ и $b_i$ -- коэффициенты $f$ и $g$. Теперь докажем лемму:

\lm Пусть $f\in \mb C[x]$ имеет вид $f=(x-\alpha)h$. Тогда $|f|=|(\alpha x-1)h|$.
\proof
Обозначим коэффициенты $h$ за $c_0,\dots, c_{n-1}$. Тогда 
$$|f|^2= |-c_0\alpha|^2+|c_0-c_1\alpha|^2+\dots+ |c_{n-1}|^2.$$
С другой стороны имеем
$$|(\alpha x-1)h|^2=|c_0|^2+|\alpha c_0-c_1|^2+\dots + |\alpha c_{n-1}|^2.$$
Распишем отдельно модуль $|\alpha c_i - c_{i+1}|^2$ и $|c_i-\alpha c_{i+1}|^2$:
$$|\alpha c_i - c_{i+1}|^2= |\alpha c_i|^2 + |c_{i+1}|^2 - 2\re \alpha c_i \ovl{c_{i+1}} \text{ и } |c_i-\alpha c_{i+1}|^2= |\alpha c_{i+1}|^2 + |c_{i}|^2 - 2\re \alpha c_i \ovl{c_{i+1}}.$$
Осталось заметить, что все слагаемые вида $|\alpha c_i|^2$, $|c_i|^2$ и $-2\re \alpha c_i \ovl{c_{i+1}}$ встречаются в первой и второй сумме ровно по одному разу.
\endproof
\elm

Сформулируем теорему:

\thrm
Пусть $f=a_0 + a_1x+\dots + a_n x^n \in \mb Z[x]$. Тогда для коэффициентов целочисленного делителя $f$ степени $m$ имеет место оценка
$$|b_j| \leq C_{m-1}^j |f| + C_{m-1}^{j-1}|a_n|.$$
\proof Пусть $g(x)$ -- некоторый многочлен. Имеем разложение $g(x)=b_m \prod_{i=1}^m (x-\alpha_i)$. Тогда рассмотрим многочлен 
$$h(x)=\prod_{|\alpha_i|\geq 1} (x-\alpha_i) \prod_{|\alpha_i|<1}(\alpha_ix-1).$$
Мы знаем по предыдущей лемме, что $|g|=|h|$. Определим величины
$$M(g)=\prod_{|\alpha_i|\geq 1} |\alpha_i|, \,\,\, m(g)=\prod_{|\alpha_i|<1}|\alpha_i|.$$
Тогда имеет место
$$|g|^2=|h|^2\geq |b_m|^2 (M(g)^2+m(g)^2).$$
Для этого заметим, что старший коэффициент $h$ по модулю есть $|b_n|m(g)$, а младший -- $|b_n|M(g)$, что и даёт неравенство. В частности применяя это к многочлену $f$ получаем, что $$M(g) \leq \frac{|A|}{a_m},$$
То есть мы оценили некоторое выражение от корней многочлена $f$. Вернёмся пока к многочлену произвольному многочлену $g$. Имеем следующую оценку на его коэффициент:
$$|b_j| = |b_m| \left|\sum \alpha_{i_1}\dots \alpha_{i_{m-j}}\right| \leq |b_m| \left|\sum \beta_{i_1}\dots \beta_{i_{m-j}}\right|, $$
где $\beta=\max(1,|\alpha|)$.

Покажем лемму

\lm Пусть есть набор вещественных чисел $x_1,\dots,x_m\geq 1$, что $x_1\dots x_m=M$. Тогда для всех $0<j\leq n$ имеем $$\sigma_j(x_1,\dots,x_m) \leq C_{m-1}^j M+ C_{m-1}^{j-1},$$
где $\sigma_i(x_1,\dots,x_n)$ -- элементарный симметрический многочлен.
\proof Покажем, что максимум модуля $\sigma_j$ достигается, если $x_1=\dots=x_{m-1}=1$ и $x_m=M$. Рассмотрим, например, пару $x_1,x_2$. Если $x_1,x_2\neq 1$, то заменим её на пару $1,x_1x_2$. Покажем, что при этом значение $\sigma_j$ увеличилось. Действительно, в новой сумме изменились только слагаемые в которые входили только $x_1$ или только $x_2$. Выпишем их 
$$\sum_{2<i_2 \dots< i_j} x_1 x_{i_2}\dots x_{i_j}  + \sum_{2<i_2\dots} x_2 x_{i_2}\dots x_{i_j}= (x_1+x_2)\sigma_{j-2}(x_3,\dots,x_m)$$
и сделаем в них замену $x_1=1$ и $x_2=x_1x_2$. Получим
$$(1+x_1x_2)\sigma_{j-2}(x_3,\dots,x_m).$$
Осталось заметить, что $0< x_1x_2 -x_1 -x_2 + 1= (x_1-1)(x_2-1)$ в наших предположениях. Аналогично для других пар переменных.
Теперь 
$$\sigma_j(1,\dots,1,M)= C_{m-1}^{j-1}M + C_{m-1}^j.$$
\endproof
\elm

\noindent Заметим теперь, что произведение $\beta_i = M(g)$. Тогда по лемме
$$|b_j|\leq |b_m| (C_{m-1}^{m-j-1}M(g) + C_{m-1}^{m-j})=|b_m| (C_{m-1}^{j}M(g) + C_{m-1}^{j-1}).$$
Теперь пусть $f \di g $ в $\mb Z[x]$. Тогда $a_n \di b_m$ и $M(g)\leq M(f)$ так как корни $g$ являются корнями $f$. Отсюда
$$|b_j|\leq |b_m| (C_{m-1}^{j}M(g) + C_{m-1}^{j-1})\leq |a_n|(C_{m-1}^{j}M(f) + C_{m-1}^{j-1})\leq C_{m-1}^{j}|f| + C_{m-1}^{j-1}|a_n|.$$

\endproof
\ethrm



Это теорема показывает, что размер записи коэффициентов делителя полиномиально зависит от размера записи многочлена $f$.







\section{Результант и дискриминант}

Попробуем решить следующую задачу: как определить по коэффициентам многочлена, что они имеют общий множитель? Если имеет место нетривиальное разложение $f=hk_1$, $g=hk_2$, то степень $k_1$ меньше $\deg f=n$, а степень $k_2\leq \deg g=m$. Заметим, что тогда $fk_2-gk_1=0$. Обратно, если найдены такие $k_1$ и $k_2$, то у $f$ и $g$ есть общий множитель. Рассмотрим отображение $K[x]_{\leq m-1}\times K[x]_{\leq n-1} \to K[x]_{\leq n+m-1}$, заданное по правилу
$$(a(x),b(x)) \to a(x)f(x)+b(x)g(x).$$ 
Когда это отображение вырождено? Тогда и только тогда, когда есть многочлены маленьких степеней, что $a(x)f(x)=-b(x)g(x)$. Это происходит тогда и только тогда, когда у многочленов $f$ и $g$ есть общий множитель. С другой это происходит, если определитель матрицы этого линейного отображения в стандартных базисах обнуляется. Транспонирую эту матрицу и переставляя строки и столбцы приходим к определению:

\dfn Пусть многочлен $f(x)=a_0+\dots+a_nx^n$, а $g(x)=b_0+\dots+b_mx^m$. Тогда результантом многочленов $f$ и $g$ называется $$Res(f,g)=  \det 
\begin{pmatrix}
a_n & a_{n-1} & \cdots & a_0 & 0 & \cdots & 0 \\
0 & a_n & a_{n-1} & \cdots & a_0 & \cdots & 0 \\
\\
0 & \cdots &  a_n & a_{n-1} & a_{n-2} & \cdots &  a_0 \\
b_m & \cdots & b_1 & b_0 & 0 & \cdots & 0 \\
 \\
0 & \cdots & 0 & b_m & \cdots & b_1 & b_0 
\end{pmatrix}.$$
Эта матрица называется матрицей Сильвестра. 
\edfn

Результант двух многочленов может быть определён над любым кольцом. Посмотрим, что из этого можно вытащить. Что значит равенство нулю результанта по $y$ для двух многочленов $f(x,y) $ и $g(x,y)$ из $K[x,y]$? Их результант это многочлен $h(x)$. Рассмотрим точку $x_0$. Допустим, что старшие коэффициенты $f$ и $g$ не обращаются в 0 в точке $x_0$. Тогда равенство нулю результанта $h(x_0)$ -- это равенство нулю результанта многочленов $f(x_0,y)$ и $g(x_0,y)$, что означает, что у последних есть общий корень $y_0$. То есть у системы $f=g=0$ есть корень $(x_0,y_0)$. Таким образом корни результанта -- это  просто $x$-координаты решений системы, или точки, в которых старший коэффициент многочленов обращается в 0.


А что можно вывести из того факта, что у $Res(f,g)=n$ для двух многочленов из $\mb Z[x]$? Разложим $n$ на простые множители. Получим $n=p_1^{\alpha_1} \dots p_k^{\alpha_k}$. Допустим, что $p_i$ не делит старшие коэффициенты $f$ и $g$. Тогда $0=Res(f,g) \mod p_i $ есть $Res(\ovl{f}, \ovl{g})$ и следовательно по модулю $p_i$ у многочленов есть общий корень. Обратно, если у $f$ и $g$ есть общий корень по модулю $p_i$, то $Res(f,g)\di p_i$.

\dfn Дискриминантом многочлена $f=a_0+\dots +a_nx^n$ называется выражение 
$$D(f)=(-1)^{\frac{n(n-1)}{2}} a_n^{-1} Res(f,f').$$
Это полином от коэффициентов $f(x)$.
\edfn

Дискриминант даёт ответ на вопрос, когда многочлен не имеет кратных корней по модулю $p$. Действительно это происходит только тогда, когда $D(f)\ndi p$. Заметим, что это условие может нарушаться только в конечном числе $p$, если $D(f)\neq 0$. Таким образом, либо у многочлена есть кратный корень, либо у него нет кратных корней для почти всех $p$. Это обосновывает, что для применения леммы Гензеля для подъёма всегда можно выбрать подходящее простое, если сам многочлен $f\in \mb Z[x]$ был бесквадратный.


\subsection{Дополнительно: свойства результанта и дискриминанта}

\thrm Пусть многочлен $f(x)=a_0+\dots+a_nx^n$, а $g(x)=b_0+\dots+b_mx^m$ из кольца $K[x]$, где $K$ -- поле. Пусть так же в поле $K$ имеются разложения $f(x)=a_n\prod(x-x_i)$, а $g(x)=b_m\prod (x-y_j)$. Тогда
$$Res(f,g)=a_n^mb_m^n \prod_{i,j} (x_i-y_j),$$
\ethrm

\lm  $$a_n^mb_m^n \prod_{i,j} (x_i-y_j)=(-1)^{mn}b_m^n \prod f(y_j)=a_n^m \prod g(x_i).$$ 
\elm

\upr Кроме того, если $f=gq+r$, где $\deg r=k$, то 
 $$Res(f,g)=(-1)^{(n-k)m}b_m^{n-k} Res(r,g).$$
\eupr

\lm $$D(f)=a_n^{2n-2}\prod_{i< j} (x_i-x_j)^2.$$
\elm

\exm\\
1) $D(x^2+ax+b)=-4b+a^2$.\\
2) $D(x^3+ax+b)=-27b^2-4a^3$.\\


\end{document}
